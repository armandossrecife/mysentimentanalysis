{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armandossrecife/mysentimentanalysis/blob/main/my_automatic_inspection_issues.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A) Testes de Inspeção de Issues do Cassandra"
      ],
      "metadata": {
        "id": "SE16K7_W8lhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies\n",
        "\n",
        "- datasets from Hugging Face\n",
        "- transformers Hugging Face\n",
        "- torch\n",
        "- accelerate\n",
        "- ntlk"
      ],
      "metadata": {
        "id": "J0leFftZR1EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install datasets"
      ],
      "metadata": {
        "id": "0a9qRXxv-LaD"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers[torch]"
      ],
      "metadata": {
        "id": "aCQ174h5SE8Y"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install accelerate -U"
      ],
      "metadata": {
        "id": "hSHkISowSP_g"
      },
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install nltk"
      ],
      "metadata": {
        "id": "A84TFx_-H2GQ"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import dependencies\n",
        "\n",
        "\n",
        "- torch\n",
        "- pandas\n",
        "- numpy\n",
        "- transformers\n",
        "- sklearn\n",
        "- datasets\n",
        "- json\n",
        "- string\n",
        "- nltk"
      ],
      "metadata": {
        "id": "m-gB7jND1XXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from urllib.parse import urlparse"
      ],
      "metadata": {
        "id": "MXsfQgrq1twW"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "b5k6wkKrILS5",
        "outputId": "86ba262f-bc60-4b78-cc59-cba1d6398873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 339,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 339
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "wRifxYLpTqmD",
        "outputId": "73fdfde5-2a2a-4053-c5b6-86b5b51d749f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 340
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_string(text, max_length=100, add_ellipsis=True):\n",
        "  if len(text) <= max_length:\n",
        "    return text\n",
        "\n",
        "  truncated_text = text[:max_length]\n",
        "\n",
        "  if add_ellipsis:\n",
        "    truncated_text += \"...\"\n",
        "\n",
        "  return truncated_text\n",
        "\n",
        "def to_lowercase(text):\n",
        "  return text.lower()\n",
        "\n",
        "def remove_hyperlinks(text):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  filtered_tokens = [token for token in tokens if not urlparse(token).scheme]\n",
        "  return ' '.join(filtered_tokens)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  return text.translate(translator)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = text.split()\n",
        "  filtered_words = [word for word in words if word not in stop_words]\n",
        "  return ' '.join(filtered_words)\n",
        "\n",
        "def preprocess_text(text):\n",
        "  text = to_lowercase(text)\n",
        "  text = remove_hyperlinks(text)\n",
        "  #text = remove_punctuation(text)\n",
        "  text = remove_stopwords(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "E0j8BUK3jT1z"
      },
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset da minha conta Hugging Fase\n",
        "\n",
        "https://huggingface.co/datasets/armandoufpi/cassandraissuesgroundtruth\n"
      ],
      "metadata": {
        "id": "HCjCClbi8Zs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splits = {'train': 'train.jsonl', 'test': 'test.jsonl'}\n",
        "df_treino = pd.read_json(\"hf://datasets/armandoufpi/cassandraissuesgroundtruth/\" + splits[\"train\"])\n",
        "df_teste = pd.read_json(\"hf://datasets/armandoufpi/cassandraissuesgroundtruth/\" + splits[\"test\"])"
      ],
      "metadata": {
        "id": "DcnbM_Tr8cQz"
      },
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_treino"
      ],
      "metadata": {
        "id": "CPLFkCmPdRgL",
        "outputId": "a48b6e6c-cb62-41e3-a438-682924472850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        }
      },
      "execution_count": 383,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           issue_key                                            summary  \\\n",
              "0     CASSANDRA-3489           EncryptionOptions should be instantiated   \n",
              "1    CASSANDRA-16780    Log when writing many tombstones to a partition   \n",
              "2     CASSANDRA-5426                           Redesign repair messages   \n",
              "3     CASSANDRA-5121    system.peers.tokens is empty after node restart   \n",
              "4    CASSANDRA-11944  sstablesInBounds might not actually give all s...   \n",
              "..               ...                                                ...   \n",
              "195  CASSANDRA-18617  Disable the deprecated keyspace/table threshol...   \n",
              "196   CASSANDRA-5244  Compactions don't work while node is bootstrap...   \n",
              "197    CASSANDRA-173                    add getPendingTasks to CFSMBean   \n",
              "198    CASSANDRA-359      CFS readStats_ and diskReadStats_ are missing   \n",
              "199    CASSANDRA-124  NullPointerException in consistency manager af...   \n",
              "\n",
              "      issue_type issue_status issue_priority  \\\n",
              "0            Bug     Resolved            Low   \n",
              "1    Improvement     Resolved         Normal   \n",
              "2    Improvement     Resolved            Low   \n",
              "3            Bug     Resolved            Low   \n",
              "4            Bug     Resolved         Normal   \n",
              "..           ...          ...            ...   \n",
              "195  Improvement     Resolved         Normal   \n",
              "196          Bug     Resolved         Urgent   \n",
              "197  Improvement     Resolved            Low   \n",
              "198          Bug     Resolved         Normal   \n",
              "199          Bug     Resolved         Urgent   \n",
              "\n",
              "                                           description  \\\n",
              "0    As the title says, otherwise you get an NPE wh...   \n",
              "1    Log when writing many tombstones to a partitio...   \n",
              "2    Many people have been reporting 'repair hang' ...   \n",
              "3    Using a 2 nodes fresh cluster (127.0.0.1 & 127...   \n",
              "4    Same problem as with CASSANDRA-11886 - if we t...   \n",
              "..                                                 ...   \n",
              "195  The non-guardrail thresholds 'keyspace_count_w...   \n",
              "196  It seems that there is a race condition in Sto...   \n",
              "197  need to add an atomicint and inc/decr it whene...   \n",
              "198                            There is no description   \n",
              "199  ERROR [CONSISTENCY-MANAGER:2] 2009-04-30 18:22...   \n",
              "\n",
              "                                              comments architectural_impact  \\\n",
              "0    ['There\\'s a bunch of \"if encryption options i...                   NO   \n",
              "1    ['https://github.com/krummas/cassandra/commits...                   NO   \n",
              "2    ['Work in progress is pushed to: https://githu...                  YES   \n",
              "3    ['In StorageService.handleStateNormal, when we...                   NO   \n",
              "4    ['https://github.com/krummas/cassandra/commits...                  YES   \n",
              "..                                                 ...                  ...   \n",
              "195  [\"Part of this change is to add converters tha...                  YES   \n",
              "196  [\"Thanks for the detective work, Jouni.  I'll ...                   NO   \n",
              "197  ['rebased patch as 0001-CASSANDRA-173-added-CF...                   NO   \n",
              "198  [\"shouldn't we also get rid of getReadDiskHits...                   NO   \n",
              "199  [\"Shouldn't ConsistencyManager() constructor c...                   NO   \n",
              "\n",
              "                                         comments_text  label label_text  \n",
              "0    There\\'s a bunch of \"if encryption options is ...      0   negative  \n",
              "1    https://github.com/krummas/cassandra/commits/m...      0   negative  \n",
              "2    https://github.com/yukim/cassandra/commits/542...      1   positive  \n",
              "3    removeEndpoint should be used instead\\n    [ju...      0   negative  \n",
              "4    https://github.com/krummas/cassandra/commits/m...      1   positive  \n",
              "..                                                 ...    ...        ...  \n",
              "195  \\xa0[https://github.com/apache/cassandra/pull/...      1   positive  \n",
              "196  BLOCKED (on object monitor)\\n    at org.apache...      0   negative  \n",
              "197  rebased patch as 0001-CASSANDRA-173-added-CFS-...      0   negative  \n",
              "198  [\"shouldn't we also get rid of getReadDiskHits...      0   negative  \n",
              "199  [\"Shouldn't ConsistencyManager() constructor c...      0   negative  \n",
              "\n",
              "[200 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-296cbd97-c38c-4167-aee5-f2e577a7fb40\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_key</th>\n",
              "      <th>summary</th>\n",
              "      <th>issue_type</th>\n",
              "      <th>issue_status</th>\n",
              "      <th>issue_priority</th>\n",
              "      <th>description</th>\n",
              "      <th>comments</th>\n",
              "      <th>architectural_impact</th>\n",
              "      <th>comments_text</th>\n",
              "      <th>label</th>\n",
              "      <th>label_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CASSANDRA-3489</td>\n",
              "      <td>EncryptionOptions should be instantiated</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>As the title says, otherwise you get an NPE wh...</td>\n",
              "      <td>['There\\'s a bunch of \"if encryption options i...</td>\n",
              "      <td>NO</td>\n",
              "      <td>There\\'s a bunch of \"if encryption options is ...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CASSANDRA-16780</td>\n",
              "      <td>Log when writing many tombstones to a partition</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Log when writing many tombstones to a partitio...</td>\n",
              "      <td>['https://github.com/krummas/cassandra/commits...</td>\n",
              "      <td>NO</td>\n",
              "      <td>https://github.com/krummas/cassandra/commits/m...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CASSANDRA-5426</td>\n",
              "      <td>Redesign repair messages</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>Many people have been reporting 'repair hang' ...</td>\n",
              "      <td>['Work in progress is pushed to: https://githu...</td>\n",
              "      <td>YES</td>\n",
              "      <td>https://github.com/yukim/cassandra/commits/542...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CASSANDRA-5121</td>\n",
              "      <td>system.peers.tokens is empty after node restart</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>Using a 2 nodes fresh cluster (127.0.0.1 &amp; 127...</td>\n",
              "      <td>['In StorageService.handleStateNormal, when we...</td>\n",
              "      <td>NO</td>\n",
              "      <td>removeEndpoint should be used instead\\n    [ju...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CASSANDRA-11944</td>\n",
              "      <td>sstablesInBounds might not actually give all s...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Same problem as with CASSANDRA-11886 - if we t...</td>\n",
              "      <td>['https://github.com/krummas/cassandra/commits...</td>\n",
              "      <td>YES</td>\n",
              "      <td>https://github.com/krummas/cassandra/commits/m...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>CASSANDRA-18617</td>\n",
              "      <td>Disable the deprecated keyspace/table threshol...</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>The non-guardrail thresholds 'keyspace_count_w...</td>\n",
              "      <td>[\"Part of this change is to add converters tha...</td>\n",
              "      <td>YES</td>\n",
              "      <td>\\xa0[https://github.com/apache/cassandra/pull/...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>CASSANDRA-5244</td>\n",
              "      <td>Compactions don't work while node is bootstrap...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Urgent</td>\n",
              "      <td>It seems that there is a race condition in Sto...</td>\n",
              "      <td>[\"Thanks for the detective work, Jouni.  I'll ...</td>\n",
              "      <td>NO</td>\n",
              "      <td>BLOCKED (on object monitor)\\n    at org.apache...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>CASSANDRA-173</td>\n",
              "      <td>add getPendingTasks to CFSMBean</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>need to add an atomicint and inc/decr it whene...</td>\n",
              "      <td>['rebased patch as 0001-CASSANDRA-173-added-CF...</td>\n",
              "      <td>NO</td>\n",
              "      <td>rebased patch as 0001-CASSANDRA-173-added-CFS-...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>CASSANDRA-359</td>\n",
              "      <td>CFS readStats_ and diskReadStats_ are missing</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>There is no description</td>\n",
              "      <td>[\"shouldn't we also get rid of getReadDiskHits...</td>\n",
              "      <td>NO</td>\n",
              "      <td>[\"shouldn't we also get rid of getReadDiskHits...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>CASSANDRA-124</td>\n",
              "      <td>NullPointerException in consistency manager af...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Urgent</td>\n",
              "      <td>ERROR [CONSISTENCY-MANAGER:2] 2009-04-30 18:22...</td>\n",
              "      <td>[\"Shouldn't ConsistencyManager() constructor c...</td>\n",
              "      <td>NO</td>\n",
              "      <td>[\"Shouldn't ConsistencyManager() constructor c...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 11 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-296cbd97-c38c-4167-aee5-f2e577a7fb40')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-296cbd97-c38c-4167-aee5-f2e577a7fb40 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-296cbd97-c38c-4167-aee5-f2e577a7fb40');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0b5f603b-6a25-48c5-9ac2-7c65fd7215a0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0b5f603b-6a25-48c5-9ac2-7c65fd7215a0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0b5f603b-6a25-48c5-9ac2-7c65fd7215a0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4305b99a-7c64-4afc-8d73-229b2decf0f1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_treino')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4305b99a-7c64-4afc-8d73-229b2decf0f1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_treino');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_treino",
              "summary": "{\n  \"name\": \"df_treino\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"issue_key\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"CASSANDRA-2950\",\n          \"CASSANDRA-18803\",\n          \"CASSANDRA-17509\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"Data from truncated CF reappears after server restart\",\n          \"Refactor validation logic in StorageService.rebuild\",\n          \"Add Guardrail to disable GROUP BY functionality\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Improvement\",\n          \"Task\",\n          \"New Feature\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Resolved\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_priority\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Low\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 189,\n        \"samples\": [\n          \"This ticket was created from needs discovered in CASSANDRA-17180. We want to be able to configure a startup check so we figured out that it is necessary to treat all startup checks same - to be able to configure them. This ticket is about making startup checks configurable.\\n\\n\\n\\nOnce this ticket is done, we can continue with the implementation of CASSANDRA-17180 where the implementation of gc grace check will be done.\\n\\n\\n\\nWe have identified that there is one check currently in place which needs to be changed to reflect this configuration implementation and that is FileSystemOwnershipCheck.\\n\\n\\n\\nBecause startup checks were not configurable before via means of a configuration file, they were configurable via system properties. This ticket does not aim to get rid system properties configuration mechanism, system properties will have precedence over settings in configuration file. Then, in the next release, I am aiming to get rid of system properties configuration mechanism.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"[\\\"This is a general issue with all CF's. updating bug.\\\", 'The other permutation of this bug looked like, assuming write with CL.Q:\\\\n* Insert 50 (3 nodes up)\\\\n* truncate CF (3 nodes up)\\\\n* Insert 1 (3 nodes up)\\\\n* Bring node3 down\\\\n* Delete 1  (2 nodes up)\\\\n* Bring up node3 and run repair\\\\n* Take down node1 and node2.\\\\n* Query node3 with CL.ONE: list Standard1;  --- 30 rows returned\\\\n\\\\nNot sure, but this looked suspicious in my logs:\\\\n{code}\\\\n INFO 01:19:45,616 Streaming to /50.57.114.45\\\\n INFO 01:19:45,689 Finished streaming session 698609583499991 from /50.57.107.176\\\\n INFO 01:19:45,690 Finished streaming session 698609609994154 from /50.57.114.45\\\\n INFO 01:19:46,501 Finished streaming repair with /50.57.114.45 for (0,56713727820156410577229101238628035242]: 0 oustanding to complete session\\\\n INFO 01:19:46,531 Compacted to /var/lib/cassandra/data/Keyspace1/Standard1-tmp-g-106-Data.db.  16,646,523 to 16,646,352 (~99% of original) bytes for 30 keys.  Time: 1,509ms.\\\\n INFO 01:19:46,930 Finished streaming repair with /50.57.107.176 for (113427455640312821154458202477256070484,0]: 1 oustanding to complete session\\\\n INFO 01:19:47,619 Finished streaming repair with /50.57.114.45 for (113427455640312821154458202477256070484,0]: 0 oustanding to complete session\\\\n INFO 01:19:48,232 Finished streaming repair with /50.57.107.176 for (56713727820156410577229101238628035242,113427455640312821154458202477256070484]: 1 oustanding to complete session\\\\n INFO 01:19:48,856 Finished streaming repair with /50.57.114.45 for (56713727820156410577229101238628035242,113427455640312821154458202477256070484]: 0 oustanding to complete session\\\\n{code}', \\\"Currently, truncate does:\\\\n* force a flush\\\\n* record the time\\\\n* delete any sstables older than the time\\\\n\\\\nThis isn't quite enough if the machine crashes shortly afterward, however, since there can be mutations present in the commitlog that were previously truncated and are now resurrected by CL replay.\\\\n\\\\nOne thing we could do is record the truncate time for the CF in the system ks and then ignore mutations older than that, however this would require time synchronization between the client and the server to be accurate.\\\\n\\\", 'but we record CL \\\"context\\\" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.\\\\n\\\\nchecked and we do wait for flush to complete in truncate.', 'bq. but we record CL \\\"context\\\" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.\\\\n\\\\nI think there\\\\'s something wrong with that, then:\\\\n\\\\n{noformat}\\\\n INFO 21:25:15,274 Replaying /var/lib/cassandra/commitlog/CommitLog-1312924388053.log\\\\nDEBUG 21:25:15,290 Replaying /var/lib/cassandra/commitlog/CommitLog-1312924388053.log starting at 0\\\\nDEBUG 21:25:15,291 Reading mutation at 0\\\\nDEBUG 21:25:15,295 replaying mutation for system.4c: {ColumnFamily(LocationInfo [47656e65726174696f6e:false:4@1312924388140000,])}\\\\nDEBUG 21:25:15,321 Reading mutation at 89\\\\nDEBUG 21:25:15,322 replaying mutation for system.426f6f747374726170: {ColumnFamily(LocationInfo [42:false:1@1312924388203,])}\\\\nDEBUG 21:25:15,322 Reading mutation at 174\\\\nDEBUG 21:25:15,322 replaying mutation for system.4c: {ColumnFamily(LocationInfo [546f6b656e:false:16@1312924388204,])}\\\\nDEBUG 21:25:15,322 Reading mutation at 270\\\\nDEBUG 21:25:15,324 replaying mutation for Keyspace1.3030: {ColumnFamily(Standard1 [C0:false:34@1312924813259,C1:false:34@1312924813260,C2:false:34@1312924813260,C3:false:34@1312924813260,C4:false:34@1312924813260,])}\\\\n{noformat}\\\\n\\\\nThe last entry there is the first of many errant mutations.', 'Ah, CASSANDRA-2419 keeps on giving...\\\\n\\\\nbq. but we record CL \\\"context\\\" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.\\\\n\\\\nThe obvious problem with this is that the point of truncate is to blow away such sstables...  Patch attached.  Comment explains the core fix:\\\\n\\\\n{noformat}\\\\n// Bonus complication: since we store replay position in sstable metadata,\\\\n// truncating those sstables means we will replay any CL segments from the\\\\n// beginning if we restart before they are discarded for normal reasons\\\\n// post-truncate.  So we need to (a) force a new segment so the currently\\\\n// active one can be discarded, and (b) flush *all* CFs so that unflushed\\\\n// data in others don\\\\'t keep any pre-truncate CL segments alive.\\\\n{noformat}\\\\n\\\\nPatch also fixes the bug in ReplayManagerTruncateTest that made it miss this.\\\\n', 'I think the forceFlush of all the CF is not safe, because if for a given column family the memtable is clean, forceFlush will return immediately, even though there could be a memtable being flush at the same time (or pending flush). So we cannot be sure all the old segment are clean after the waitFutures (I know, it took me some time to figure out some problem with repair for this very reason when the repairs were not properly synchronized).\\\\n\\\\nWhat we would need is to add to the future we wait on the futures of all the flush being processed at that time. Sounds annoying though. ', '+1, though this patch is against trunk, not 0.8.  Also mistakenly bumps the log4j level to debug.', 'v2:\\\\n\\\\n{noformat}\\\\n// Bonus bonus: simply forceFlush of all the CF is not enough, because if\\\\n// for a given column family the memtable is clean, forceFlush will return\\\\n// immediately, even though there could be a memtable being flush at the same\\\\n// time.  So to guarantee that all segments can be cleaned out, we need\\\\n// \\\"waitForActiveFlushes\\\" after the new segment has been created.\\\\n{noformat}', \\\"Attaching a v3 that is rebased against 0.8. I've also slightly change the logic in Truncate to submit all the flushes and then call waitForActiveFlushes, as this is slightly simpler and should work equally well as far as I can tell.\\\\nApart from that, this lgtm.\\\", 'committed', 'Integrated in Cassandra-0.8 #272 (See [https://builds.apache.org/job/Cassandra-0.8/272/])\\\\n    make sure truncate clears out the commitlog\\\\npatch by jbellis; reviewed by slebresne for CASSANDRA-2950\\\\n\\\\njbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156763\\\\nFiles : \\\\n* /cassandra/branches/cassandra-0.8/CHANGES.txt\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java\\\\n* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java\\\\n']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"architectural_impact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"YES\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"\\\\n* /cassandra/branches/cassandra-0.8/CHANGES.txt\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java\\\\n* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java\\\\n \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 383
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_teste"
      ],
      "metadata": {
        "id": "WrWV5F_3dV1E",
        "outputId": "f301028b-a76c-41d7-906e-3d8aa84e2131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 384,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          issue_key                                            summary  \\\n",
              "0   CASSANDRA-11944  sstablesInBounds might not actually give all s...   \n",
              "1   CASSANDRA-12988  make the consistency level for user-level auth...   \n",
              "2   CASSANDRA-15004  Anti-compaction briefly corrupts sstable state...   \n",
              "3   CASSANDRA-15265  Index summary redistribution can start even wh...   \n",
              "4   CASSANDRA-18029                     fix starting Paxos auto repair   \n",
              "5   CASSANDRA-18058                     In-memory index and query path   \n",
              "6   CASSANDRA-18617  Disable the deprecated keyspace/table threshol...   \n",
              "7    CASSANDRA-1919                Add shutdownhook to flush commitlog   \n",
              "8     CASSANDRA-414                                 remove sstableLock   \n",
              "9    CASSANDRA-5426                           Redesign repair messages   \n",
              "10  CASSANDRA-11540           The JVM should exit if jmx fails to bind   \n",
              "11   CASSANDRA-6013   CAS may return false but still commit the insert   \n",
              "12   CASSANDRA-8116    HSHA fails with default rpc_max_threads setting   \n",
              "13  CASSANDRA-10164            Re-apply MV updates on commitlog replay   \n",
              "14  CASSANDRA-11971             More uses of DataOutputBuffer.RECYCLER   \n",
              "15  CASSANDRA-12717         IllegalArgumentException in CompactionTask   \n",
              "16  CASSANDRA-13526  nodetool cleanup on KS with no replicas should...   \n",
              "17   CASSANDRA-2941  Expose number of rpc timeouts for individual h...   \n",
              "18   CASSANDRA-3032                    clean up KSMetadata, CFMetadata   \n",
              "19    CASSANDRA-359      CFS readStats_ and diskReadStats_ are missing   \n",
              "20   CASSANDRA-6170  Modify AbstractCassandraDaemon.initLog4j() to ...   \n",
              "21   CASSANDRA-6706  Duplicate rows returned when in clause has rep...   \n",
              "22   CASSANDRA-6962           examine shortening path length post-5202   \n",
              "23   CASSANDRA-6972  Throw an ERROR when auto_bootstrap: true and b...   \n",
              "24    CASSANDRA-758                      support wrapped range queries   \n",
              "25   CASSANDRA-8627  Support Total/Recent latency histogram metrics...   \n",
              "\n",
              "     issue_type issue_status issue_priority  \\\n",
              "0           Bug     Resolved         Normal   \n",
              "1   Improvement     Resolved            Low   \n",
              "2           Bug     Resolved         Urgent   \n",
              "3           Bug     Resolved         Normal   \n",
              "4           Bug     Resolved         Normal   \n",
              "5   New Feature     Resolved         Normal   \n",
              "6   Improvement     Resolved         Normal   \n",
              "7   Improvement     Resolved            Low   \n",
              "8   Improvement     Resolved         Normal   \n",
              "9   Improvement     Resolved            Low   \n",
              "10          Bug     Resolved         Normal   \n",
              "11          Bug     Resolved         Normal   \n",
              "12          Bug     Resolved            Low   \n",
              "13          Bug     Resolved         Normal   \n",
              "14  Improvement     Resolved            Low   \n",
              "15          Bug     Resolved         Normal   \n",
              "16          Bug     Resolved         Normal   \n",
              "17  Improvement     Resolved            Low   \n",
              "18         Task     Resolved            Low   \n",
              "19          Bug     Resolved         Normal   \n",
              "20  New Feature     Resolved         Normal   \n",
              "21          Bug     Resolved         Normal   \n",
              "22  Improvement     Resolved         Normal   \n",
              "23  Improvement     Resolved            Low   \n",
              "24  New Feature     Resolved            Low   \n",
              "25  Improvement     Resolved         Normal   \n",
              "\n",
              "                                          description  \\\n",
              "0   Same problem as with CASSANDRA-11886 - if we t...   \n",
              "1   Most reads for the auth-related tables execute...   \n",
              "2   Since we use multiple sstable rewriters in ant...   \n",
              "3   When we pause autocompaction for upgradesstabl...   \n",
              "4   This test was not run in CI because of its nam...   \n",
              "5   An in-memory index using the in-memory trie st...   \n",
              "6   The non-guardrail thresholds 'keyspace_count_w...   \n",
              "7   this replaces the periodic_with_flush approach...   \n",
              "8                             There is no description   \n",
              "9   Many people have been reporting 'repair hang' ...   \n",
              "10  If you are already running a cassandra instanc...   \n",
              "11  If a Paxos proposer proposes some value/update...   \n",
              "12  The HSHA server fails with 'Out of heap space'...   \n",
              "13  If a node crashes between the Commit log updat...   \n",
              "14  There are a few more possible use cases for {{...   \n",
              "15  When I was ran LargePartitionsTest.test_11_1G ...   \n",
              "16  From the user list:\\n\\nhttps://lists.apache.or...   \n",
              "17  We have a total number timeouts for each node....   \n",
              "18  There are too many conversion methods between ...   \n",
              "19                            There is no description   \n",
              "20  When customer wants to bump up log level of a ...   \n",
              "21  If a value is repeated within an IN clause the...   \n",
              "22  From CASSANDRA-5202 discussion:\\n\\n{quote}\\nDi...   \n",
              "23  Obviously when this condition exists the node ...   \n",
              "24  we want to support scanning from KeyX to KeyA ...   \n",
              "25  The Metrics histogram is pretty bad at non-nor...   \n",
              "\n",
              "                                             comments architectural_impact  \\\n",
              "0   ['https://github.com/krummas/cassandra/commits...                  YES   \n",
              "1   ['Linked patch allows an operator to set the r...                  YES   \n",
              "2   ['|[3.0|https://github.com/bdeggleston/cassand...                  YES   \n",
              "3   ['Patch adds a flag in `CompactionManager` whi...                  YES   \n",
              "4   ['I fixed here what I could: [https://github.c...                  YES   \n",
              "5   ['The github PR for this ticket is here:\\xa0\\r...                  YES   \n",
              "6   [\"Part of this change is to add converters tha...                  YES   \n",
              "7   [\"The approach I took was to add a shutdownBlo...                  YES   \n",
              "8   ['rebased.\\n\\n02\\n    remove sstableLock.  re-...                  YES   \n",
              "9   ['Work in progress is pushed to: https://githu...                  YES   \n",
              "10  [\"We could exit on IOException (it's {{BindExc...                   NO   \n",
              "11  ['bq. if for a given proposal at least one acc...                   NO   \n",
              "12  ['Committed.', \"Is it guaranteed to OOM?  Can'...                   NO   \n",
              "13  [\"[patch|https://github.com/tjake/cassandra/tr...                   NO   \n",
              "14  ['Patch uses recycled {{DataOutputBuffer}}s in...                   NO   \n",
              "15  ['Patch is here. Could you please review this?...                   NO   \n",
              "16  ['The issue I am seeing on C* cluster with the...                   NO   \n",
              "17  ['expose the number of timeouts per host.\\nexp...                   NO   \n",
              "18  ['Standardizes on to{Avro,Thrift} and from{Avr...                   NO   \n",
              "19  [\"shouldn't we also get rid of getReadDiskHits...                   NO   \n",
              "20  [\"Don't see any reason not to allow a symlink ...                   NO   \n",
              "21  [\"That is kind of the intended behavior. Is it...                   NO   \n",
              "22  [\"It's actually not clear to me that the benef...                   NO   \n",
              "23  ['The catch with doing this is, now everyone h...                   NO   \n",
              "24  ['add wrapped range support + test', '+1 Looks...                   NO   \n",
              "25                             ['Committed, thanks.']                   NO   \n",
              "\n",
              "                                        comments_text  label label_text  \n",
              "0   https://github.com/krummas/cassandra/commits/m...      1   positive  \n",
              "1   [Link|https://app.circleci.com/pipelines/githu...      1   positive  \n",
              "2   not sure what is going on with the dtests thou...      1   positive  \n",
              "3   [3.0|https://circleci.com/workflow-run/8882a8a...      1   positive  \n",
              "4   repaired}} rely on running regular/incremental...      1   positive  \n",
              "5   [https://app.circleci.com/pipelines/github/ade...      1   positive  \n",
              "6   \\xa0[https://github.com/apache/cassandra/pull/...      1   positive  \n",
              "7   Could not create ServerSocket on address /127....      1   positive  \n",
              "8   the cleanup does happen.  If it were the SSTR ...      1   positive  \n",
              "9   https://github.com/yukim/cassandra/commits/542...      1   positive  \n",
              "10  Address already in use\\n        at java.net.Pl...      0   negative  \n",
              "11  attaching v4 with that version (which is equiv...      0   negative  \n",
              "12  Committed.' \"Is it guaranteed to OOM?  Can't w...      0   negative  \n",
              "13  [\"[patch|https://github.com/tjake/cassandra/tr...      0   negative  \n",
              "14  Patch uses recycled {{DataOutputBuffer}}s inst...      0   negative  \n",
              "15  Patch is here. Could you please review this?\\n...      0   negative  \n",
              "16  nodetool cleanup on KS with no replicas should...      0   negative  \n",
              "17  \\n* /cassandra/branches/cassandra-0.8/src/java...      0   negative  \n",
              "18  \\n* /cassandra/trunk/test/unit/org/apache/cass...      0   negative  \n",
              "19  [\"shouldn't we also get rid of getReadDiskHits...      0   negative  \n",
              "20  [\"Don't see any reason not to allow a symlink ...      0   negative  \n",
              "21  [\"That is kind of the intended behavior. Is it...      0   negative  \n",
              "22  feels pretty error prone. What about keeping t...      0   negative  \n",
              "23  false in their seed configs.' 'Yes the right f...      0   negative  \n",
              "24  add wrapped range support + test' '+1 Looks go...      0   negative  \n",
              "25                                 Committed thanks.       0   negative  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f624416b-f2c2-431d-85fb-033ab20fbf9f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_key</th>\n",
              "      <th>summary</th>\n",
              "      <th>issue_type</th>\n",
              "      <th>issue_status</th>\n",
              "      <th>issue_priority</th>\n",
              "      <th>description</th>\n",
              "      <th>comments</th>\n",
              "      <th>architectural_impact</th>\n",
              "      <th>comments_text</th>\n",
              "      <th>label</th>\n",
              "      <th>label_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CASSANDRA-11944</td>\n",
              "      <td>sstablesInBounds might not actually give all s...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Same problem as with CASSANDRA-11886 - if we t...</td>\n",
              "      <td>['https://github.com/krummas/cassandra/commits...</td>\n",
              "      <td>YES</td>\n",
              "      <td>https://github.com/krummas/cassandra/commits/m...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CASSANDRA-12988</td>\n",
              "      <td>make the consistency level for user-level auth...</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>Most reads for the auth-related tables execute...</td>\n",
              "      <td>['Linked patch allows an operator to set the r...</td>\n",
              "      <td>YES</td>\n",
              "      <td>[Link|https://app.circleci.com/pipelines/githu...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CASSANDRA-15004</td>\n",
              "      <td>Anti-compaction briefly corrupts sstable state...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Urgent</td>\n",
              "      <td>Since we use multiple sstable rewriters in ant...</td>\n",
              "      <td>['|[3.0|https://github.com/bdeggleston/cassand...</td>\n",
              "      <td>YES</td>\n",
              "      <td>not sure what is going on with the dtests thou...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CASSANDRA-15265</td>\n",
              "      <td>Index summary redistribution can start even wh...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>When we pause autocompaction for upgradesstabl...</td>\n",
              "      <td>['Patch adds a flag in `CompactionManager` whi...</td>\n",
              "      <td>YES</td>\n",
              "      <td>[3.0|https://circleci.com/workflow-run/8882a8a...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CASSANDRA-18029</td>\n",
              "      <td>fix starting Paxos auto repair</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>This test was not run in CI because of its nam...</td>\n",
              "      <td>['I fixed here what I could: [https://github.c...</td>\n",
              "      <td>YES</td>\n",
              "      <td>repaired}} rely on running regular/incremental...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>CASSANDRA-18058</td>\n",
              "      <td>In-memory index and query path</td>\n",
              "      <td>New Feature</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>An in-memory index using the in-memory trie st...</td>\n",
              "      <td>['The github PR for this ticket is here:\\xa0\\r...</td>\n",
              "      <td>YES</td>\n",
              "      <td>[https://app.circleci.com/pipelines/github/ade...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>CASSANDRA-18617</td>\n",
              "      <td>Disable the deprecated keyspace/table threshol...</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>The non-guardrail thresholds 'keyspace_count_w...</td>\n",
              "      <td>[\"Part of this change is to add converters tha...</td>\n",
              "      <td>YES</td>\n",
              "      <td>\\xa0[https://github.com/apache/cassandra/pull/...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>CASSANDRA-1919</td>\n",
              "      <td>Add shutdownhook to flush commitlog</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>this replaces the periodic_with_flush approach...</td>\n",
              "      <td>[\"The approach I took was to add a shutdownBlo...</td>\n",
              "      <td>YES</td>\n",
              "      <td>Could not create ServerSocket on address /127....</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>CASSANDRA-414</td>\n",
              "      <td>remove sstableLock</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>There is no description</td>\n",
              "      <td>['rebased.\\n\\n02\\n    remove sstableLock.  re-...</td>\n",
              "      <td>YES</td>\n",
              "      <td>the cleanup does happen.  If it were the SSTR ...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>CASSANDRA-5426</td>\n",
              "      <td>Redesign repair messages</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>Many people have been reporting 'repair hang' ...</td>\n",
              "      <td>['Work in progress is pushed to: https://githu...</td>\n",
              "      <td>YES</td>\n",
              "      <td>https://github.com/yukim/cassandra/commits/542...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>CASSANDRA-11540</td>\n",
              "      <td>The JVM should exit if jmx fails to bind</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>If you are already running a cassandra instanc...</td>\n",
              "      <td>[\"We could exit on IOException (it's {{BindExc...</td>\n",
              "      <td>NO</td>\n",
              "      <td>Address already in use\\n        at java.net.Pl...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>CASSANDRA-6013</td>\n",
              "      <td>CAS may return false but still commit the insert</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>If a Paxos proposer proposes some value/update...</td>\n",
              "      <td>['bq. if for a given proposal at least one acc...</td>\n",
              "      <td>NO</td>\n",
              "      <td>attaching v4 with that version (which is equiv...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>CASSANDRA-8116</td>\n",
              "      <td>HSHA fails with default rpc_max_threads setting</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>The HSHA server fails with 'Out of heap space'...</td>\n",
              "      <td>['Committed.', \"Is it guaranteed to OOM?  Can'...</td>\n",
              "      <td>NO</td>\n",
              "      <td>Committed.' \"Is it guaranteed to OOM?  Can't w...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>CASSANDRA-10164</td>\n",
              "      <td>Re-apply MV updates on commitlog replay</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>If a node crashes between the Commit log updat...</td>\n",
              "      <td>[\"[patch|https://github.com/tjake/cassandra/tr...</td>\n",
              "      <td>NO</td>\n",
              "      <td>[\"[patch|https://github.com/tjake/cassandra/tr...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>CASSANDRA-11971</td>\n",
              "      <td>More uses of DataOutputBuffer.RECYCLER</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>There are a few more possible use cases for {{...</td>\n",
              "      <td>['Patch uses recycled {{DataOutputBuffer}}s in...</td>\n",
              "      <td>NO</td>\n",
              "      <td>Patch uses recycled {{DataOutputBuffer}}s inst...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>CASSANDRA-12717</td>\n",
              "      <td>IllegalArgumentException in CompactionTask</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>When I was ran LargePartitionsTest.test_11_1G ...</td>\n",
              "      <td>['Patch is here. Could you please review this?...</td>\n",
              "      <td>NO</td>\n",
              "      <td>Patch is here. Could you please review this?\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>CASSANDRA-13526</td>\n",
              "      <td>nodetool cleanup on KS with no replicas should...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>From the user list:\\n\\nhttps://lists.apache.or...</td>\n",
              "      <td>['The issue I am seeing on C* cluster with the...</td>\n",
              "      <td>NO</td>\n",
              "      <td>nodetool cleanup on KS with no replicas should...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>CASSANDRA-2941</td>\n",
              "      <td>Expose number of rpc timeouts for individual h...</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>We have a total number timeouts for each node....</td>\n",
              "      <td>['expose the number of timeouts per host.\\nexp...</td>\n",
              "      <td>NO</td>\n",
              "      <td>\\n* /cassandra/branches/cassandra-0.8/src/java...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>CASSANDRA-3032</td>\n",
              "      <td>clean up KSMetadata, CFMetadata</td>\n",
              "      <td>Task</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>There are too many conversion methods between ...</td>\n",
              "      <td>['Standardizes on to{Avro,Thrift} and from{Avr...</td>\n",
              "      <td>NO</td>\n",
              "      <td>\\n* /cassandra/trunk/test/unit/org/apache/cass...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>CASSANDRA-359</td>\n",
              "      <td>CFS readStats_ and diskReadStats_ are missing</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>There is no description</td>\n",
              "      <td>[\"shouldn't we also get rid of getReadDiskHits...</td>\n",
              "      <td>NO</td>\n",
              "      <td>[\"shouldn't we also get rid of getReadDiskHits...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>CASSANDRA-6170</td>\n",
              "      <td>Modify AbstractCassandraDaemon.initLog4j() to ...</td>\n",
              "      <td>New Feature</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>When customer wants to bump up log level of a ...</td>\n",
              "      <td>[\"Don't see any reason not to allow a symlink ...</td>\n",
              "      <td>NO</td>\n",
              "      <td>[\"Don't see any reason not to allow a symlink ...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>CASSANDRA-6706</td>\n",
              "      <td>Duplicate rows returned when in clause has rep...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>If a value is repeated within an IN clause the...</td>\n",
              "      <td>[\"That is kind of the intended behavior. Is it...</td>\n",
              "      <td>NO</td>\n",
              "      <td>[\"That is kind of the intended behavior. Is it...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>CASSANDRA-6962</td>\n",
              "      <td>examine shortening path length post-5202</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>From CASSANDRA-5202 discussion:\\n\\n{quote}\\nDi...</td>\n",
              "      <td>[\"It's actually not clear to me that the benef...</td>\n",
              "      <td>NO</td>\n",
              "      <td>feels pretty error prone. What about keeping t...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>CASSANDRA-6972</td>\n",
              "      <td>Throw an ERROR when auto_bootstrap: true and b...</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>Obviously when this condition exists the node ...</td>\n",
              "      <td>['The catch with doing this is, now everyone h...</td>\n",
              "      <td>NO</td>\n",
              "      <td>false in their seed configs.' 'Yes the right f...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>CASSANDRA-758</td>\n",
              "      <td>support wrapped range queries</td>\n",
              "      <td>New Feature</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>we want to support scanning from KeyX to KeyA ...</td>\n",
              "      <td>['add wrapped range support + test', '+1 Looks...</td>\n",
              "      <td>NO</td>\n",
              "      <td>add wrapped range support + test' '+1 Looks go...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>CASSANDRA-8627</td>\n",
              "      <td>Support Total/Recent latency histogram metrics...</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>The Metrics histogram is pretty bad at non-nor...</td>\n",
              "      <td>['Committed, thanks.']</td>\n",
              "      <td>NO</td>\n",
              "      <td>Committed thanks.</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f624416b-f2c2-431d-85fb-033ab20fbf9f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f624416b-f2c2-431d-85fb-033ab20fbf9f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f624416b-f2c2-431d-85fb-033ab20fbf9f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3d4c74c0-f05e-46e8-a2c3-185f9bd0e6ab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3d4c74c0-f05e-46e8-a2c3-185f9bd0e6ab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3d4c74c0-f05e-46e8-a2c3-185f9bd0e6ab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6e065dda-b6b4-4913-9035-fc772f0099a1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_teste')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6e065dda-b6b4-4913-9035-fc772f0099a1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_teste');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_teste",
              "summary": "{\n  \"name\": \"df_teste\",\n  \"rows\": 26,\n  \"fields\": [\n    {\n      \"column\": \"issue_key\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"CASSANDRA-414\",\n          \"CASSANDRA-13526\",\n          \"CASSANDRA-11944\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"remove sstableLock\",\n          \"nodetool cleanup on KS with no replicas should remove old data, not silently complete\",\n          \"sstablesInBounds might not actually give all sstables within the bounds due to having start positions moved in sstables\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Improvement\",\n          \"Task\",\n          \"Bug\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Resolved\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_priority\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Normal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"There is no description\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"['rebased.\\\\n\\\\n02\\\\n    remove sstableLock.  re-order a few ops so that we can never \\\"lose\\\" data temporarily -- always remove old sstable _after_ adding the new ones.  so at worst a few read ops will merge data from an sstable that is obsolete -- this is ok and better than Stop The World locking\\\\n\\\\n01\\\\n    combine addToList and storeLocation; rename to addSSTable\\\\n', \\\"We need to add a comment to CFS.snapshot, indicating that it's a fuzzy one, instead of point-at-time one. Also, the following unit tests failed on me.\\\\n\\\\n[junit] Testcase: testNameSort10(org.apache.cassandra.db.NameSortTest):     Caused an ERROR\\\\n[junit] /home/junrao/local/cassandra_test/build/test/cassandra/data/Keyspace1/Super1-50-Data.db (No such file or directory)\\\\n[junit] java.io.FileNotFoundException: /home/junrao/local/cassandra_test/build/test/cassandra/data/Keyspace1/Super1-50-Data.db (No such file or directory)\\\\n[junit]     at java.io.RandomAccessFile.open(Native Method)\\\\n[junit]     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)\\\\n[junit]     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)\\\\n[junit]     at org.apache.cassandra.io.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)\\\\n[junit]     at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.<init>(SSTableSliceIterator.java:110)\\\\n[junit]     at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:56)\\\\n[junit]     at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:64)\\\\n[junit]     at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1347)\\\\n[junit]     at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1283)\\\\n[junit]     at org.apache.cassandra.db.Table.get(Table.java:564)\\\\n[junit]     at org.apache.cassandra.db.NameSortTest.validateNameSort(NameSortTest.java:113)\\\\n[junit]     at org.apache.cassandra.db.NameSortTest.testNameSort(NameSortTest.java:94)\\\\n[junit]     at org.apache.cassandra.db.NameSortTest.testNameSort10(NameSortTest.java:48)\\\\n[junit]\\\\n[junit]\\\\n[junit] Testcase: testNameSort100(org.apache.cassandra.db.NameSortTest):    Caused an ERROR\\\\n[junit] /home/junrao/local/cassandra_test/build/test/cassandra/data/Keyspace1/Super1-582-Data.db (No such file or directory)\\\\n[junit] java.io.FileNotFoundException: /home/junrao/local/cassandra_test/build/test/cassandra/data/Keyspace1/Super1-582-Data.db (No such file or directory\\\\n[junit]     at java.io.RandomAccessFile.open(Native Method)\\\\n[junit]     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)\\\\n[junit]     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)\\\\n[junit]     at org.apache.cassandra.io.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)\\\\n[junit]     at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.<init>(SSTableSliceIterator.java:110)\\\\n[junit]     at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:56)\\\\n[junit]     at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:64)\\\\n[junit]     at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1347)\\\\n[junit]     at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1283)\\\\n[junit]     at org.apache.cassandra.db.Table.get(Table.java:564)\\\\n[junit]     at org.apache.cassandra.db.NameSortTest.validateNameSort(NameSortTest.java:113)\\\\n[junit]     at org.apache.cassandra.db.NameSortTest.testNameSort(NameSortTest.java:94)\\\\n[junit]     at org.apache.cassandra.db.NameSortTest.testNameSort100(NameSortTest.java:55)\\\\n[junit]\\\", 'when compaction completes it modifies CFS.sstables_ as follows:\\\\n\\\\n    - sstables_.put(new compacted sstable)\\\\n    - remove the source sstables & delete their file on disk\\\\n\\\\nnow, NonBlockingHashMap guarantees that \\\"Iterators and Enumerations return elements reflecting the state of the hash table at some point at or since the creation of the iterator/enumeration.\\\"  BUT this has no effect on other parts of the system, particularly the delete!\\\\n\\\\nso the fundamental problem exhibeted by the unit tests is this:\\\\n\\\\n    - thread A starts iterating over sstables_\\\\n    - compaction thread finishes and does its thing.  now some of the files A needs to see a consistent view of the data are gone: the set of sstables being iterated over was in fact consistent but because we\\\\'re violating encapsulation by doing the delete and remove separately, we can get incorrect results.\\\\n\\\\nI think the simplest solution is to use pseudo-finalizers to do the actual file delete once no references to the owning SSTableReader exist anymore.  These can be done using http://java.sun.com/javase/6/docs/api/java/lang/ref/ReferenceQueue.html and http://java.sun.com/javase/6/docs/api/java/lang/ref/PhantomReference.html.\\\\n', 'Keeping references to SSTableReader may not be enough. The iterator could iterate file A. Then just before an SSTableReader is opened on A, A is removed from sstables and deleted on disk. Now, you can get the same \\\"file not found\\\" exception as above.', 'The iterator will either have a reference to A, or not.  If it does, then A will not be deleted until the iterator is done since the delete is no longer done by compaction but by the referencequeue.  If it does not, it will have a reference to the new sstable instead.', \\\"The question is when a reference is added to the referencequeue. If it's at SSTableReader open time, it may be too late. Between file A is iterated and SSTableReader is opened on A, a compaction can remove A, assuming no one else is reading A at that time.\\\", 'No, it has nothing to do with whether someone is actually reading (has a BufferedRAF open) on A, only whether any reference to the SSTR object itself exists.  Which is no earlier than when the last iterator that might open a BRAF is done.', 'Remember, SSTR objects are not transitory -- we only have one such object for each file on disk ever.  Even the compaction code uses SSTR.get to use a reference to the open object, and does not open new ones (except for the newly compacted files of course, which is in turn the only time open is called on those).', 'I checked the source for NBHM and couldn\\\\'t find any evidence that my literal reading of the iterator contract (\\\"Iterators and Enumerations return elements reflecting the state of the hash table at some point at or since the creation of the iterator/enumeration\\\") was correct.  So I asked the author for clarification here: https://sourceforge.net/forum/message.php?msg_id=7611241.\\\\n\\\\nHe replied, and sure enough, Jun\\\\'s suspicions were correct and even if the compaction thread is careful to add the new SSTR before removing the old ones from sstables_, iterator threads may see the absence of the latter but not the presence of the former.\\\\n\\\\nSo I think that this approach is not going to work.  But I think we can still cut the lock penalty dramatically from what it is now.  I should have some code for that approach Monday.', 'Integrated in Cassandra #191 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/191/])\\\\n    Revert \\\"remove sstableLock.  re-order a few ops so that we can never \\\"lose\\\" data temporarily -- always remove old sstable references _after_ adding the new ones.  so at worst a few read ops will merge data from an sstable that is obsolete -- this is ok and better than Stop The World locking\\\"\\\\nand \\\" combine addToList and storeLocation; rename to addSSTable\\\"\\\\n\\\\nThese were works in progress (and broken); accidentally committed w/ the 418 fix.\\\\n combine addToList and storeLocation; rename to addSSTable\\\\n', '03\\\\n    Replace sstableLock with SSTableTracker, which performs updates to the sstable list atomically\\\\n    without readers ever having to block.  (Readers will always either see the old list, or the new.)\\\\n    We avoid a race on the delete of the old SSTable files on-disk by using a ReferenceQueue:\\\\n    when the last reference is gone, a PhantomReference is added to the queue and can do cleanup.\\\\n    In case Cassandra is killed between compaction and this cleanup, a -Compacted empty file\\\\n    is written to disk; Cassandra removes any files thus tagged on startup.\\\\n\\\\n02\\\\n    convert ssTables_ to a Set, since the filename is encapsulated in the SSTR object now\\\\n\\\\n01\\\\n    CASSANDRA-414 combine addToList and storeLocation; rename to addSSTable\\\\n\\\\nready for review.\\\\n\\\\nthere is a good summary of how PhantomReference and ReferenceQueue work here: http://www.kdgregory.com/index.php?page=java.refobj', '+1', 'committed', \\\"Maybe I don''t understand how PhantomReference works, but the code in SSTableReader dealing with finalizerQueue doesn't look right to me. What gets enqueued in finalizerQueue is SSTR. It doesn't seem like that you can cast it directly to FileDeletingReference. It seems to me that you have to maintain a map btw SSTR and FileDeletingReference. Every time you dequeue an item from finalizerQueue, you can lookup the map to find the corresponding FileDeletingReference.\\\", \\\"Yeah, it's a little subtle.  The article I linked is a good explanation, the javadoc alone isn't sufficient or at least wasn't for me.\\\\n\\\\nA Reference of any type has a get() method that returns the actual referent.  Here that would be the SSTR.  But ReferenceQueue holds the Reference wrapper, not the actual SSTRs.  (When you pass a RQ to the Reference constructor, the Reference will be enqueued on that RQ when its referent is GC'd.  The referent itself already GC'd or in the process of being GC'd so it can't be put on the RQ or you would get back to the Bad Old Days of finalizer resurrection bugs.)\\\\n\\\\nNow, if the referent is no longer live, get() will return null.  Since the point of the RQ design is to do cleanup after the object is dead, we subclass PhantomReference and store a reference to the path, so we don't actually need the SSTR to do the delete.  (In fact for PR in particular get() _always_ returns null but that is not really essential to understanding what is going on here.)\\\", \\\"It's easy enough to check the logs with e.g. stress.py: the cleanup does happen.  If it were the SSTR being enqueued then\\\\n\\\\n                        r = (FileDeletingReference) finalizerQueue.remove();\\\\n\\\\nwould generate ClassCastException and nothing would get cleaned up (until server restart).\\\", 'Integrated in Cassandra #194 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/194/])\\\\n    Replace sstableLock with SSTableTracker, which performs updates to the sstable list atomically\\\\nwithout readers ever having to block.  (Readers will always either see the old list, or the new.)\\\\nWe avoid a race on the delete of the old SSTable files on-disk by using a ReferenceQueue:\\\\nwhen the last reference is gone, a PhantomReference is added to the queue and can do cleanup.\\\\nIn case Cassandra is killed between compaction and this cleanup, a -Compacted empty file\\\\nis written to disk; Cassandra removes any files thus tagged on startup.\\\\n\\\\npatch by jbellis; reviewed by Chris Goffinet for \\\\nconvert ssTables_ to a Set, since the filename is encapsulated in the SSTR object now\\\\npatch by jbellis; reviewed by Chris Goffinet for \\\\ncombine addToList and storeLocation; rename to addSSTable\\\\npatch by jbellis; reviewed by Chris Goffinet for \\\\n']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"architectural_impact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"NO\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"the cleanup does happen.  If it were the SSTR being enqueued then\\\\n\\\\n                        r = (FileDeletingReference) finalizerQueue.remove();\\\\n\\\\nwould generate ClassCastException and nothing would get cleaned up (until server restart).\\\" 'Integrated in Cassandra #194 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/194/])\\\\n    Replace sstableLock with SSTableTracker which performs updates to the sstable list atomically\\\\nwithout readers ever having to block.  (Readers will always either see the old list or the new.)\\\\nWe avoid a race on the delete of the old SSTable files on-disk by using a ReferenceQueue:\\\\nwhen the last reference is gone a PhantomReference is added to the queue and can do cleanup.\\\\nIn case Cassandra is killed between compaction and this cleanup a -Compacted empty file\\\\nis written to disk; Cassandra removes any files thus tagged on startup.\\\\n\\\\npatch by jbellis; reviewed by Chris Goffinet for \\\\nconvert ssTables_ to a Set since the filename is encapsulated in the SSTR object now\\\\npatch by jbellis; reviewed by Chris Goffinet for \\\\ncombine addToList and storeLocation; rename to addSSTable\\\\npatch by jbellis; reviewed by Chris Goffinet for \\\\n \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 384
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carrega o dataset e faz os devidos processamentos (transformações)"
      ],
      "metadata": {
        "id": "U5rFQWyL8_RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.concat([df_treino, df_teste], axis=0)\n",
        "dataset['Textual_Type'] = 'AI_Yes'\n",
        "dataset.loc[dataset['label']==0, 'Textual_Type'] = 'AI_No'\n",
        "dataset['SummaryDescriptionComments']= dataset.apply(lambda row: row['summary'] + ' ' + row['description'] + ' ' + row['comments_text'],axis=1).values\n",
        "dataset['processed_text'] = dataset['SummaryDescriptionComments'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "E7i2QI2OUgml"
      },
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "AOmxjJfXFcib",
        "outputId": "7b7a3959-c642-4fce-fd22-c998c66b19cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        }
      },
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         issue_key                                            summary  \\\n",
              "0   CASSANDRA-3489           EncryptionOptions should be instantiated   \n",
              "1  CASSANDRA-16780    Log when writing many tombstones to a partition   \n",
              "2   CASSANDRA-5426                           Redesign repair messages   \n",
              "3   CASSANDRA-5121    system.peers.tokens is empty after node restart   \n",
              "4  CASSANDRA-11944  sstablesInBounds might not actually give all s...   \n",
              "\n",
              "    issue_type issue_status issue_priority  \\\n",
              "0          Bug     Resolved            Low   \n",
              "1  Improvement     Resolved         Normal   \n",
              "2  Improvement     Resolved            Low   \n",
              "3          Bug     Resolved            Low   \n",
              "4          Bug     Resolved         Normal   \n",
              "\n",
              "                                         description  \\\n",
              "0  As the title says, otherwise you get an NPE wh...   \n",
              "1  Log when writing many tombstones to a partitio...   \n",
              "2  Many people have been reporting 'repair hang' ...   \n",
              "3  Using a 2 nodes fresh cluster (127.0.0.1 & 127...   \n",
              "4  Same problem as with CASSANDRA-11886 - if we t...   \n",
              "\n",
              "                                            comments architectural_impact  \\\n",
              "0  ['There\\'s a bunch of \"if encryption options i...                   NO   \n",
              "1  ['https://github.com/krummas/cassandra/commits...                   NO   \n",
              "2  ['Work in progress is pushed to: https://githu...                  YES   \n",
              "3  ['In StorageService.handleStateNormal, when we...                   NO   \n",
              "4  ['https://github.com/krummas/cassandra/commits...                  YES   \n",
              "\n",
              "                                       comments_text  label label_text  \\\n",
              "0  There\\'s a bunch of \"if encryption options is ...      0   negative   \n",
              "1  https://github.com/krummas/cassandra/commits/m...      0   negative   \n",
              "2  https://github.com/yukim/cassandra/commits/542...      1   positive   \n",
              "3  removeEndpoint should be used instead\\n    [ju...      0   negative   \n",
              "4  https://github.com/krummas/cassandra/commits/m...      1   positive   \n",
              "\n",
              "  Textual_Type                         SummaryDescriptionComments  \\\n",
              "0        AI_No  EncryptionOptions should be instantiated As th...   \n",
              "1        AI_No  Log when writing many tombstones to a partitio...   \n",
              "2       AI_Yes  Redesign repair messages Many people have been...   \n",
              "3        AI_No  system.peers.tokens is empty after node restar...   \n",
              "4       AI_Yes  sstablesInBounds might not actually give all s...   \n",
              "\n",
              "                                      processed_text  \n",
              "0  encryptionoptions instantiated title says , ot...  \n",
              "1  log writing many tombstones partition log writ...  \n",
              "2  redesign repair messages many people reporting...  \n",
              "3  system.peers.tokens empty node restart using 2...  \n",
              "4  sstablesinbounds might actually give sstables ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a61b0157-4781-4732-a35d-e8b519f7d8d3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_key</th>\n",
              "      <th>summary</th>\n",
              "      <th>issue_type</th>\n",
              "      <th>issue_status</th>\n",
              "      <th>issue_priority</th>\n",
              "      <th>description</th>\n",
              "      <th>comments</th>\n",
              "      <th>architectural_impact</th>\n",
              "      <th>comments_text</th>\n",
              "      <th>label</th>\n",
              "      <th>label_text</th>\n",
              "      <th>Textual_Type</th>\n",
              "      <th>SummaryDescriptionComments</th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CASSANDRA-3489</td>\n",
              "      <td>EncryptionOptions should be instantiated</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>As the title says, otherwise you get an NPE wh...</td>\n",
              "      <td>['There\\'s a bunch of \"if encryption options i...</td>\n",
              "      <td>NO</td>\n",
              "      <td>There\\'s a bunch of \"if encryption options is ...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "      <td>AI_No</td>\n",
              "      <td>EncryptionOptions should be instantiated As th...</td>\n",
              "      <td>encryptionoptions instantiated title says , ot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CASSANDRA-16780</td>\n",
              "      <td>Log when writing many tombstones to a partition</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Log when writing many tombstones to a partitio...</td>\n",
              "      <td>['https://github.com/krummas/cassandra/commits...</td>\n",
              "      <td>NO</td>\n",
              "      <td>https://github.com/krummas/cassandra/commits/m...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "      <td>AI_No</td>\n",
              "      <td>Log when writing many tombstones to a partitio...</td>\n",
              "      <td>log writing many tombstones partition log writ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CASSANDRA-5426</td>\n",
              "      <td>Redesign repair messages</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>Many people have been reporting 'repair hang' ...</td>\n",
              "      <td>['Work in progress is pushed to: https://githu...</td>\n",
              "      <td>YES</td>\n",
              "      <td>https://github.com/yukim/cassandra/commits/542...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>AI_Yes</td>\n",
              "      <td>Redesign repair messages Many people have been...</td>\n",
              "      <td>redesign repair messages many people reporting...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CASSANDRA-5121</td>\n",
              "      <td>system.peers.tokens is empty after node restart</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>Using a 2 nodes fresh cluster (127.0.0.1 &amp; 127...</td>\n",
              "      <td>['In StorageService.handleStateNormal, when we...</td>\n",
              "      <td>NO</td>\n",
              "      <td>removeEndpoint should be used instead\\n    [ju...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "      <td>AI_No</td>\n",
              "      <td>system.peers.tokens is empty after node restar...</td>\n",
              "      <td>system.peers.tokens empty node restart using 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CASSANDRA-11944</td>\n",
              "      <td>sstablesInBounds might not actually give all s...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Same problem as with CASSANDRA-11886 - if we t...</td>\n",
              "      <td>['https://github.com/krummas/cassandra/commits...</td>\n",
              "      <td>YES</td>\n",
              "      <td>https://github.com/krummas/cassandra/commits/m...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>AI_Yes</td>\n",
              "      <td>sstablesInBounds might not actually give all s...</td>\n",
              "      <td>sstablesinbounds might actually give sstables ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a61b0157-4781-4732-a35d-e8b519f7d8d3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a61b0157-4781-4732-a35d-e8b519f7d8d3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a61b0157-4781-4732-a35d-e8b519f7d8d3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5b58a6a9-fe85-4c31-8753-ba773f428c0c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b58a6a9-fe85-4c31-8753-ba773f428c0c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5b58a6a9-fe85-4c31-8753-ba773f428c0c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 226,\n  \"fields\": [\n    {\n      \"column\": \"issue_key\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"CASSANDRA-2950\",\n          \"CASSANDRA-18803\",\n          \"CASSANDRA-17509\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"Data from truncated CF reappears after server restart\",\n          \"Refactor validation logic in StorageService.rebuild\",\n          \"Add Guardrail to disable GROUP BY functionality\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Improvement\",\n          \"Task\",\n          \"New Feature\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Resolved\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_priority\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Low\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 190,\n        \"samples\": [\n          \"nodetool status -r not working well on C* 4,\\n\\n Version:\\n\\n{code:java}\\n\\n[root@foo001 ~]# nodetool version\\n\\nReleaseVersion: 4.0-beta3\\n\\n{code}\\n\\nWithout resolving:\\n\\n{code:java}\\n\\n[root@foo001 ~]# nodetool status\\n\\nDatacenter: V4CH\\n\\n================\\n\\nStatus=Up/Down\\n\\n|/ State=Normal/Leaving/Joining/Moving\\n\\n--  Address Load    Tokens  Owns(effective) Host ID                            Rack\\n\\nUN  1.2.3.4 363.68 KiB  128     ?         92ae4c39-edb3-4e67-8623-b49fd8301b66 RAC1\\n\\nUN  1.2.3.5 109.71 KiB  128     ?         d80647a8-32b2-4a8f-8022-f5ae3ce8fbb2 RAC1\\n\\n{code}\\n\\nWith resolving:\\n\\n{code:java}\\n\\n[root@foo001 ~]# nodetool status -r\\n\\nDatacenter: V4CH\\n\\n================\\n\\nStatus=Up/Down\\n\\n|/ State=Normal/Leaving/Joining/Moving\\n\\n--  Address          Load  Tokens  Owns (effective)  Host ID  Rack\\n\\n?N  foo001.tab.com   ?     128     ?                          RAC1\\n\\n?N  foo002.tab.com   ?     128     ?                          RAC1\\n\\n{code}\\n\\n\\n\\nI only changed here IPs and hostnames.\\n\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"[\\\"This is a general issue with all CF's. updating bug.\\\", 'The other permutation of this bug looked like, assuming write with CL.Q:\\\\n* Insert 50 (3 nodes up)\\\\n* truncate CF (3 nodes up)\\\\n* Insert 1 (3 nodes up)\\\\n* Bring node3 down\\\\n* Delete 1  (2 nodes up)\\\\n* Bring up node3 and run repair\\\\n* Take down node1 and node2.\\\\n* Query node3 with CL.ONE: list Standard1;  --- 30 rows returned\\\\n\\\\nNot sure, but this looked suspicious in my logs:\\\\n{code}\\\\n INFO 01:19:45,616 Streaming to /50.57.114.45\\\\n INFO 01:19:45,689 Finished streaming session 698609583499991 from /50.57.107.176\\\\n INFO 01:19:45,690 Finished streaming session 698609609994154 from /50.57.114.45\\\\n INFO 01:19:46,501 Finished streaming repair with /50.57.114.45 for (0,56713727820156410577229101238628035242]: 0 oustanding to complete session\\\\n INFO 01:19:46,531 Compacted to /var/lib/cassandra/data/Keyspace1/Standard1-tmp-g-106-Data.db.  16,646,523 to 16,646,352 (~99% of original) bytes for 30 keys.  Time: 1,509ms.\\\\n INFO 01:19:46,930 Finished streaming repair with /50.57.107.176 for (113427455640312821154458202477256070484,0]: 1 oustanding to complete session\\\\n INFO 01:19:47,619 Finished streaming repair with /50.57.114.45 for (113427455640312821154458202477256070484,0]: 0 oustanding to complete session\\\\n INFO 01:19:48,232 Finished streaming repair with /50.57.107.176 for (56713727820156410577229101238628035242,113427455640312821154458202477256070484]: 1 oustanding to complete session\\\\n INFO 01:19:48,856 Finished streaming repair with /50.57.114.45 for (56713727820156410577229101238628035242,113427455640312821154458202477256070484]: 0 oustanding to complete session\\\\n{code}', \\\"Currently, truncate does:\\\\n* force a flush\\\\n* record the time\\\\n* delete any sstables older than the time\\\\n\\\\nThis isn't quite enough if the machine crashes shortly afterward, however, since there can be mutations present in the commitlog that were previously truncated and are now resurrected by CL replay.\\\\n\\\\nOne thing we could do is record the truncate time for the CF in the system ks and then ignore mutations older than that, however this would require time synchronization between the client and the server to be accurate.\\\\n\\\", 'but we record CL \\\"context\\\" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.\\\\n\\\\nchecked and we do wait for flush to complete in truncate.', 'bq. but we record CL \\\"context\\\" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.\\\\n\\\\nI think there\\\\'s something wrong with that, then:\\\\n\\\\n{noformat}\\\\n INFO 21:25:15,274 Replaying /var/lib/cassandra/commitlog/CommitLog-1312924388053.log\\\\nDEBUG 21:25:15,290 Replaying /var/lib/cassandra/commitlog/CommitLog-1312924388053.log starting at 0\\\\nDEBUG 21:25:15,291 Reading mutation at 0\\\\nDEBUG 21:25:15,295 replaying mutation for system.4c: {ColumnFamily(LocationInfo [47656e65726174696f6e:false:4@1312924388140000,])}\\\\nDEBUG 21:25:15,321 Reading mutation at 89\\\\nDEBUG 21:25:15,322 replaying mutation for system.426f6f747374726170: {ColumnFamily(LocationInfo [42:false:1@1312924388203,])}\\\\nDEBUG 21:25:15,322 Reading mutation at 174\\\\nDEBUG 21:25:15,322 replaying mutation for system.4c: {ColumnFamily(LocationInfo [546f6b656e:false:16@1312924388204,])}\\\\nDEBUG 21:25:15,322 Reading mutation at 270\\\\nDEBUG 21:25:15,324 replaying mutation for Keyspace1.3030: {ColumnFamily(Standard1 [C0:false:34@1312924813259,C1:false:34@1312924813260,C2:false:34@1312924813260,C3:false:34@1312924813260,C4:false:34@1312924813260,])}\\\\n{noformat}\\\\n\\\\nThe last entry there is the first of many errant mutations.', 'Ah, CASSANDRA-2419 keeps on giving...\\\\n\\\\nbq. but we record CL \\\"context\\\" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.\\\\n\\\\nThe obvious problem with this is that the point of truncate is to blow away such sstables...  Patch attached.  Comment explains the core fix:\\\\n\\\\n{noformat}\\\\n// Bonus complication: since we store replay position in sstable metadata,\\\\n// truncating those sstables means we will replay any CL segments from the\\\\n// beginning if we restart before they are discarded for normal reasons\\\\n// post-truncate.  So we need to (a) force a new segment so the currently\\\\n// active one can be discarded, and (b) flush *all* CFs so that unflushed\\\\n// data in others don\\\\'t keep any pre-truncate CL segments alive.\\\\n{noformat}\\\\n\\\\nPatch also fixes the bug in ReplayManagerTruncateTest that made it miss this.\\\\n', 'I think the forceFlush of all the CF is not safe, because if for a given column family the memtable is clean, forceFlush will return immediately, even though there could be a memtable being flush at the same time (or pending flush). So we cannot be sure all the old segment are clean after the waitFutures (I know, it took me some time to figure out some problem with repair for this very reason when the repairs were not properly synchronized).\\\\n\\\\nWhat we would need is to add to the future we wait on the futures of all the flush being processed at that time. Sounds annoying though. ', '+1, though this patch is against trunk, not 0.8.  Also mistakenly bumps the log4j level to debug.', 'v2:\\\\n\\\\n{noformat}\\\\n// Bonus bonus: simply forceFlush of all the CF is not enough, because if\\\\n// for a given column family the memtable is clean, forceFlush will return\\\\n// immediately, even though there could be a memtable being flush at the same\\\\n// time.  So to guarantee that all segments can be cleaned out, we need\\\\n// \\\"waitForActiveFlushes\\\" after the new segment has been created.\\\\n{noformat}', \\\"Attaching a v3 that is rebased against 0.8. I've also slightly change the logic in Truncate to submit all the flushes and then call waitForActiveFlushes, as this is slightly simpler and should work equally well as far as I can tell.\\\\nApart from that, this lgtm.\\\", 'committed', 'Integrated in Cassandra-0.8 #272 (See [https://builds.apache.org/job/Cassandra-0.8/272/])\\\\n    make sure truncate clears out the commitlog\\\\npatch by jbellis; reviewed by slebresne for CASSANDRA-2950\\\\n\\\\njbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156763\\\\nFiles : \\\\n* /cassandra/branches/cassandra-0.8/CHANGES.txt\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java\\\\n* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java\\\\n']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"architectural_impact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"YES\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"\\\\n* /cassandra/branches/cassandra-0.8/CHANGES.txt\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java\\\\n* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java\\\\n \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Textual_Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"AI_Yes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SummaryDescriptionComments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"Data from truncated CF reappears after server restart * Configure 3 node cluster\\n* Ensure the java stress tool creates Keyspace1 with RF=3\\n\\n{code}\\n// Run Stress Tool to generate 10 keys, 1 column\\nstress --operation=INSERT -t 2 --num-keys=50 --columns=20 --consistency-level=QUORUM --average-size-values --replication-factor=3 --create-index=KEYS --nodes=cathy1,cathy2\\n\\n// Verify 50 keys in CLI\\nuse Keyspace1; \\nlist Standard1; \\n\\n// TRUNCATE CF in CLI\\nuse Keyspace1;\\ntruncate counter1;\\nlist counter1;\\n\\n// Run stress tool and verify creation of 1 key with 10 columns\\nstress --operation=INSERT -t 2 --num-keys=1 --columns=10 --consistency-level=QUORUM --average-size-values --replication-factor=3 --create-index=KEYS --nodes=cathy1,cathy2\\n\\n// Verify 1 key in CLI\\nuse Keyspace1; \\nlist Standard1; \\n\\n// Restart all three nodes\\n\\n// You will see 51 keys in CLI\\nuse Keyspace1; \\nlist Standard1; \\n{code}\\n\\n\\n \\\\n* /cassandra/branches/cassandra-0.8/CHANGES.txt\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java\\\\n* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java\\\\n \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"data truncated cf reappears server restart * configure 3 node cluster * ensure java stress tool creates keyspace1 rf=3 { code } // run stress tool generate 10 keys , 1 column stress -- operation=insert -t 2 -- num-keys=50 -- columns=20 -- consistency-level=quorum -- average-size-values -- replication-factor=3 -- create-index=keys -- nodes=cathy1 , cathy2 // verify 50 keys cli use keyspace1 ; list standard1 ; // truncate cf cli use keyspace1 ; truncate counter1 ; list counter1 ; // run stress tool verify creation 1 key 10 columns stress -- operation=insert -t 2 -- num-keys=1 -- columns=10 -- consistency-level=quorum -- average-size-values -- replication-factor=3 -- create-index=keys -- nodes=cathy1 , cathy2 // verify 1 key cli use keyspace1 ; list standard1 ; // restart three nodes // see 51 keys cli use keyspace1 ; list standard1 ; { code } \\\\n * /cassandra/branches/cassandra-0.8/changes.txt\\\\n * /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/systemtable.java\\\\n * /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/recoverymanagertruncatetest.java\\\\n * /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/commitlog.java\\\\n * /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/columnfamilystore.java\\\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 344
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Atributos chaves"
      ],
      "metadata": {
        "id": "dVrHZoqWU5Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[['issue_key', 'summary', 'description', 'comments_text', 'label', 'Textual_Type']]"
      ],
      "metadata": {
        "id": "6h_wcG9KIzo0",
        "outputId": "d39325bf-f6fd-48cc-fa65-ee7e0b3e27d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          issue_key                                            summary  \\\n",
              "0    CASSANDRA-3489           EncryptionOptions should be instantiated   \n",
              "1   CASSANDRA-16780    Log when writing many tombstones to a partition   \n",
              "2    CASSANDRA-5426                           Redesign repair messages   \n",
              "3    CASSANDRA-5121    system.peers.tokens is empty after node restart   \n",
              "4   CASSANDRA-11944  sstablesInBounds might not actually give all s...   \n",
              "..              ...                                                ...   \n",
              "21   CASSANDRA-6706  Duplicate rows returned when in clause has rep...   \n",
              "22   CASSANDRA-6962           examine shortening path length post-5202   \n",
              "23   CASSANDRA-6972  Throw an ERROR when auto_bootstrap: true and b...   \n",
              "24    CASSANDRA-758                      support wrapped range queries   \n",
              "25   CASSANDRA-8627  Support Total/Recent latency histogram metrics...   \n",
              "\n",
              "                                          description  \\\n",
              "0   As the title says, otherwise you get an NPE wh...   \n",
              "1   Log when writing many tombstones to a partitio...   \n",
              "2   Many people have been reporting 'repair hang' ...   \n",
              "3   Using a 2 nodes fresh cluster (127.0.0.1 & 127...   \n",
              "4   Same problem as with CASSANDRA-11886 - if we t...   \n",
              "..                                                ...   \n",
              "21  If a value is repeated within an IN clause the...   \n",
              "22  From CASSANDRA-5202 discussion:\\n\\n{quote}\\nDi...   \n",
              "23  Obviously when this condition exists the node ...   \n",
              "24  we want to support scanning from KeyX to KeyA ...   \n",
              "25  The Metrics histogram is pretty bad at non-nor...   \n",
              "\n",
              "                                        comments_text  label Textual_Type  \n",
              "0   There\\'s a bunch of \"if encryption options is ...      0        AI_No  \n",
              "1   https://github.com/krummas/cassandra/commits/m...      0        AI_No  \n",
              "2   https://github.com/yukim/cassandra/commits/542...      1       AI_Yes  \n",
              "3   removeEndpoint should be used instead\\n    [ju...      0        AI_No  \n",
              "4   https://github.com/krummas/cassandra/commits/m...      1       AI_Yes  \n",
              "..                                                ...    ...          ...  \n",
              "21  [\"That is kind of the intended behavior. Is it...      0        AI_No  \n",
              "22  feels pretty error prone. What about keeping t...      0        AI_No  \n",
              "23  false in their seed configs.' 'Yes the right f...      0        AI_No  \n",
              "24  add wrapped range support + test' '+1 Looks go...      0        AI_No  \n",
              "25                                 Committed thanks.       0        AI_No  \n",
              "\n",
              "[226 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9241346-e4a7-4c1f-b30d-e3a3fd6ab789\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_key</th>\n",
              "      <th>summary</th>\n",
              "      <th>description</th>\n",
              "      <th>comments_text</th>\n",
              "      <th>label</th>\n",
              "      <th>Textual_Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CASSANDRA-3489</td>\n",
              "      <td>EncryptionOptions should be instantiated</td>\n",
              "      <td>As the title says, otherwise you get an NPE wh...</td>\n",
              "      <td>There\\'s a bunch of \"if encryption options is ...</td>\n",
              "      <td>0</td>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CASSANDRA-16780</td>\n",
              "      <td>Log when writing many tombstones to a partition</td>\n",
              "      <td>Log when writing many tombstones to a partitio...</td>\n",
              "      <td>https://github.com/krummas/cassandra/commits/m...</td>\n",
              "      <td>0</td>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CASSANDRA-5426</td>\n",
              "      <td>Redesign repair messages</td>\n",
              "      <td>Many people have been reporting 'repair hang' ...</td>\n",
              "      <td>https://github.com/yukim/cassandra/commits/542...</td>\n",
              "      <td>1</td>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CASSANDRA-5121</td>\n",
              "      <td>system.peers.tokens is empty after node restart</td>\n",
              "      <td>Using a 2 nodes fresh cluster (127.0.0.1 &amp; 127...</td>\n",
              "      <td>removeEndpoint should be used instead\\n    [ju...</td>\n",
              "      <td>0</td>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CASSANDRA-11944</td>\n",
              "      <td>sstablesInBounds might not actually give all s...</td>\n",
              "      <td>Same problem as with CASSANDRA-11886 - if we t...</td>\n",
              "      <td>https://github.com/krummas/cassandra/commits/m...</td>\n",
              "      <td>1</td>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>CASSANDRA-6706</td>\n",
              "      <td>Duplicate rows returned when in clause has rep...</td>\n",
              "      <td>If a value is repeated within an IN clause the...</td>\n",
              "      <td>[\"That is kind of the intended behavior. Is it...</td>\n",
              "      <td>0</td>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>CASSANDRA-6962</td>\n",
              "      <td>examine shortening path length post-5202</td>\n",
              "      <td>From CASSANDRA-5202 discussion:\\n\\n{quote}\\nDi...</td>\n",
              "      <td>feels pretty error prone. What about keeping t...</td>\n",
              "      <td>0</td>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>CASSANDRA-6972</td>\n",
              "      <td>Throw an ERROR when auto_bootstrap: true and b...</td>\n",
              "      <td>Obviously when this condition exists the node ...</td>\n",
              "      <td>false in their seed configs.' 'Yes the right f...</td>\n",
              "      <td>0</td>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>CASSANDRA-758</td>\n",
              "      <td>support wrapped range queries</td>\n",
              "      <td>we want to support scanning from KeyX to KeyA ...</td>\n",
              "      <td>add wrapped range support + test' '+1 Looks go...</td>\n",
              "      <td>0</td>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>CASSANDRA-8627</td>\n",
              "      <td>Support Total/Recent latency histogram metrics...</td>\n",
              "      <td>The Metrics histogram is pretty bad at non-nor...</td>\n",
              "      <td>Committed thanks.</td>\n",
              "      <td>0</td>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>226 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9241346-e4a7-4c1f-b30d-e3a3fd6ab789')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9241346-e4a7-4c1f-b30d-e3a3fd6ab789 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9241346-e4a7-4c1f-b30d-e3a3fd6ab789');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e597419b-1c92-4873-939f-5cab1e0a9e2a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e597419b-1c92-4873-939f-5cab1e0a9e2a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e597419b-1c92-4873-939f-5cab1e0a9e2a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"dataset[['issue_key', 'summary', 'description', 'comments_text', 'label', 'Textual_Type']]\",\n  \"rows\": 226,\n  \"fields\": [\n    {\n      \"column\": \"issue_key\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"CASSANDRA-2950\",\n          \"CASSANDRA-18803\",\n          \"CASSANDRA-17509\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"Data from truncated CF reappears after server restart\",\n          \"Refactor validation logic in StorageService.rebuild\",\n          \"Add Guardrail to disable GROUP BY functionality\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 190,\n        \"samples\": [\n          \"nodetool status -r not working well on C* 4,\\n\\n Version:\\n\\n{code:java}\\n\\n[root@foo001 ~]# nodetool version\\n\\nReleaseVersion: 4.0-beta3\\n\\n{code}\\n\\nWithout resolving:\\n\\n{code:java}\\n\\n[root@foo001 ~]# nodetool status\\n\\nDatacenter: V4CH\\n\\n================\\n\\nStatus=Up/Down\\n\\n|/ State=Normal/Leaving/Joining/Moving\\n\\n--  Address Load    Tokens  Owns(effective) Host ID                            Rack\\n\\nUN  1.2.3.4 363.68 KiB  128     ?         92ae4c39-edb3-4e67-8623-b49fd8301b66 RAC1\\n\\nUN  1.2.3.5 109.71 KiB  128     ?         d80647a8-32b2-4a8f-8022-f5ae3ce8fbb2 RAC1\\n\\n{code}\\n\\nWith resolving:\\n\\n{code:java}\\n\\n[root@foo001 ~]# nodetool status -r\\n\\nDatacenter: V4CH\\n\\n================\\n\\nStatus=Up/Down\\n\\n|/ State=Normal/Leaving/Joining/Moving\\n\\n--  Address          Load  Tokens  Owns (effective)  Host ID  Rack\\n\\n?N  foo001.tab.com   ?     128     ?                          RAC1\\n\\n?N  foo002.tab.com   ?     128     ?                          RAC1\\n\\n{code}\\n\\n\\n\\nI only changed here IPs and hostnames.\\n\\n\",\n          \"If you are already running a cassandra instance, but for some reason try to start another one, this happens:\\n\\n{noformat}\\nINFO  20:57:09 JNA mlockall successful\\nWARN  20:57:09 JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info.\\nERROR 20:57:10 Error starting local jmx server:\\njava.rmi.server.ExportException: Port already in use: 7199; nested exception is:\\n        java.net.BindException: Address already in use\\n        at sun.rmi.transport.tcp.TCPTransport.listen(TCPTransport.java:340) ~[na:1.7.0_76]\\n        at sun.rmi.transport.tcp.TCPTransport.exportObject(TCPTransport.java:248) ~[na:1.7.0_76]\\n        at sun.rmi.transport.tcp.TCPEndpoint.exportObject(TCPEndpoint.java:411) ~[na:1.7.0_76]\\n        at sun.rmi.transport.LiveRef.exportObject(LiveRef.java:147) ~[na:1.7.0_76]\\n        at sun.rmi.server.UnicastServerRef.exportObject(UnicastServerRef.java:207) ~[na:1.7.0_76]\\n        at sun.rmi.registry.RegistryImpl.setup(RegistryImpl.java:122) ~[na:1.7.0_76]\\n        at sun.rmi.registry.RegistryImpl.<init>(RegistryImpl.java:98) ~[na:1.7.0_76]\\n        at java.rmi.registry.LocateRegistry.createRegistry(LocateRegistry.java:239) ~[na:1.7.0_76]\\n        at org.apache.cassandra.service.CassandraDaemon.maybeInitJmx(CassandraDaemon.java:100) [main/:na]\\n        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:222) [main/:na]\\n        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:564) [main/:na]\\n        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:653) [main/:na]\\nCaused by: java.net.BindException: Address already in use\\n        at java.net.PlainSocketImpl.socketBind(Native Method) ~[na:1.7.0_76]\\n        at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376) ~[na:1.7.0_76]\\n        at java.net.ServerSocket.bind(ServerSocket.java:376) ~[na:1.7.0_76]\\n        at java.net.ServerSocket.<init>(ServerSocket.java:237) ~[na:1.7.0_76]\\n        at javax.net.DefaultServerSocketFactory.createServerSocket(ServerSocketFactory.java:231) ~[na:1.7.0_76]\\n        at org.apache.cassandra.utils.RMIServerSocketFactoryImpl.createServerSocket(RMIServerSocketFactoryImpl.java:13) ~[main/:na]\\n        at sun.rmi.transport.tcp.TCPEndpoint.newServerSocket(TCPEndpoint.java:666) ~[na:1.7.0_76]\\n        at sun.rmi.transport.tcp.TCPTransport.listen(TCPTransport.java:329) ~[na:1.7.0_76]\\n        ... 11 common frames omitted\\n{noformat}\\n\\nHowever the startup continues, and ends up replaying commitlogs, which is probably not a good thing.\",\n          \"Title says it all.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"\\\\n* /cassandra/branches/cassandra-0.8/CHANGES.txt\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java\\\\n* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java\\\\n \",\n          \"[~aweisberg] would you mind to take a look? Super easy. On your +1 I ll do all the builds for 6 branches.' '+1 TY' '[3.0|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3035/workflows/7f55fcda-1cf0-43db-8471-ebd54be87c9e]\\\\r\\\\n\\\\r\\\\n[3.11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3034/workflows/e10a709e-97e1-4795-b4ca-7e3aac3253cd]\\\\r\\\\n\\\\r\\\\n[4.0 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/f2e2c30f-6d37-48cc-874a-104f34f67b50]\\\\r\\\\n[4.0 j8|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/af8cc33a-69ca-4164-800f-3b049a8ac6da]\\\\r\\\\n\\\\r\\\\n[4.1 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5297fd1a-f33b-4a17-b56d-d0522b65c95b]\\\\r\\\\n[4.1 j8|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5626cfef-a6a7-453c-9abf-43c30678ec41]\\\\r\\\\n\\\\r\\\\n[5.0 j17|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/70cf2550-e278-426a-a63b-533c25c4eccf]\\\\r\\\\n[5.0 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/2a5dcdc5-17f9-4311-9aeb-60fcea5787d9]\\\\r\\\\n\\\\r\\\\nThese builds are technically not same as what is in the current branches. I just moved one check outside of try-catch to add it logically where it belongs (where other checks are) and I moved logging into try catch (was outside of it). \",\n          \"[PR|https://github.com/apache/cassandra/pull/1543]\\\\r\\\\n[JDK8 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/199/workflows/94525b46-3917-428f-bf96-f8671b1a7ac3]\\\\r\\\\n[JDK11 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/199/workflows/df58d46d-cf06-4f00-869c-0f1a8752f1c6]' 'Patch LGTM I left small comments mostly about \\\"is\\\" vs \\\"get\\\" in MBeans/Interfaces... I see that we use \\\"get\\\" already but this is actually incorrect... so think we should also fix (100% cool not in this patch I accept consistency here over correctness)' 'SelectStatement#prepare(boolean forView) is called from View#getSelectStatement() but once that code path is executed SelectStatement#prepare(ClientState state) is not called. I would say there is a need to propagate ClientState to SelectStatement#prepare(boolean forView)' '[~smiklosovic] I might be wrong but I think that {{View#getSelectStatement()}} is only used by internal queries to get the query used on the {{CREATE VIEW}} query to populate the MV table from the base table contents. I think that query never supports {{{}GROUP BY{}}}. The path for querying MVs seems to work as expected:\\\\r\\\\n{code:java}\\\\r\\\\n@Test\\\\r\\\\npublic void checkView() throws Throwable\\\\r\\\\n{\\\\r\\\\n    setGuardrail(false);\\\\r\\\\n    createTable( \\\"CREATE TABLE %s(pk int ck int v int PRIMARY KEY(pk ck))\\\");\\\\r\\\\n    String viewName = createView(\\\"CREATE MATERIALIZED VIEW %s AS \\\" +\\\\r\\\\n                                 \\\"SELECT * FROM %s WHERE pk IS NOT null and ck IS NOT null \\\" +\\\\r\\\\n                                 \\\"PRIMARY KEY(ck pk)\\\");\\\\r\\\\n    String viewQuery = \\\"SELECT * FROM \\\" + viewName + \\\" WHERE ck=0 GROUP BY pk\\\";\\\\r\\\\n    assertFails(viewQuery \\\"GROUP BY functionality is not allowed\\\");\\\\r\\\\n    testExcludedUsers(() -> viewQuery);\\\\r\\\\n} {code}' 'yes I think you are right thanks for testing that. Please add that test there.' \\\"Added the couple extra tests and tweaked the query so we didn't need to differentiate warning type on query results and force pushed.\\\\r\\\\n\\\\r\\\\nWe good to go here [~adelapena]?\\\" 'latest changes LGTM' 'Looks good to me we only need a rebase fixing the (trivial) conflicts with the recently added guardrails and a final CI round.' 'Had a rebase w/clean run last Friday; went ahead and did one final one this morning and CI kicked off.\\\\r\\\\n\\\\r\\\\n[JDK8 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851]\\\\r\\\\n[JDK11 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/abb10877-f285-49b4-9d44-fb852fb8a584]' \\\"The failures at\\\\xa0[testMetricsCleanupOnDrop|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851/jobs/1977] and [testConnectionsAreRejectedWithInvalidConfig|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851/jobs/1983] in the runs above don't seem related to the changes so I think the results look good. Those tests failures don't appear on [Butler|https://butler.cassandra.apache.org/#/ci/upstream/compare/Cassandra-trunk/trunk] though and I haven't found tickets for them so we should probably create tickets for them.\\\" \\\"bq.  we should probably create tickets for them.\\\\r\\\\nAgree; I ran a clean (i.e. vanilla trunk) circle run earlier today as a reference and am planning on getting together a focused effort to get us to stable green there in the run up to the freeze. I'll make sure failures from here make it to that effort.\\\" \\\"I just saw testMetricsCleanupOnDrop in my runs was there a ticket opened or should I open? (wasn't sure whether there is not some umbrella ticket or anything that I am missing)\\\" \\\"I forgot about that failure and I can't find any ticket for it. The failure can be reproduced on the CircleCI's multiplexer for both 4.1 and trunk although I think we haven't seen it yet on Jenkins. I have created CASSANDRA-17658 for fixing it.\\\"] \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Textual_Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"AI_Yes\",\n          \"AI_No\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 345
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "id": "5JE3Xg24FhzZ",
        "outputId": "153646cf-b28f-443e-fe60-a48e0b39d11b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 226 entries, 0 to 25\n",
            "Data columns (total 14 columns):\n",
            " #   Column                      Non-Null Count  Dtype \n",
            "---  ------                      --------------  ----- \n",
            " 0   issue_key                   226 non-null    object\n",
            " 1   summary                     226 non-null    object\n",
            " 2   issue_type                  226 non-null    object\n",
            " 3   issue_status                226 non-null    object\n",
            " 4   issue_priority              226 non-null    object\n",
            " 5   description                 226 non-null    object\n",
            " 6   comments                    226 non-null    object\n",
            " 7   architectural_impact        226 non-null    object\n",
            " 8   comments_text               226 non-null    object\n",
            " 9   label                       226 non-null    int64 \n",
            " 10  label_text                  226 non-null    object\n",
            " 11  Textual_Type                226 non-null    object\n",
            " 12  SummaryDescriptionComments  226 non-null    object\n",
            " 13  processed_text              226 non-null    object\n",
            "dtypes: int64(1), object(13)\n",
            "memory usage: 26.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minhas_colunas = ['issue_key', 'SummaryDescriptionComments', 'processed_text', 'Textual_Type']\n",
        "dataset2 = dataset[minhas_colunas]\n",
        "dataset2.head()"
      ],
      "metadata": {
        "id": "DsUVNoh8Fo2b",
        "outputId": "c787fb20-068d-42d5-b2ba-848e77072f49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         issue_key                         SummaryDescriptionComments  \\\n",
              "0   CASSANDRA-3489  EncryptionOptions should be instantiated As th...   \n",
              "1  CASSANDRA-16780  Log when writing many tombstones to a partitio...   \n",
              "2   CASSANDRA-5426  Redesign repair messages Many people have been...   \n",
              "3   CASSANDRA-5121  system.peers.tokens is empty after node restar...   \n",
              "4  CASSANDRA-11944  sstablesInBounds might not actually give all s...   \n",
              "\n",
              "                                      processed_text Textual_Type  \n",
              "0  encryptionoptions instantiated title says , ot...        AI_No  \n",
              "1  log writing many tombstones partition log writ...        AI_No  \n",
              "2  redesign repair messages many people reporting...       AI_Yes  \n",
              "3  system.peers.tokens empty node restart using 2...        AI_No  \n",
              "4  sstablesinbounds might actually give sstables ...       AI_Yes  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33d19bed-aee4-4892-8399-9480fa380228\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_key</th>\n",
              "      <th>SummaryDescriptionComments</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>Textual_Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CASSANDRA-3489</td>\n",
              "      <td>EncryptionOptions should be instantiated As th...</td>\n",
              "      <td>encryptionoptions instantiated title says , ot...</td>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CASSANDRA-16780</td>\n",
              "      <td>Log when writing many tombstones to a partitio...</td>\n",
              "      <td>log writing many tombstones partition log writ...</td>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CASSANDRA-5426</td>\n",
              "      <td>Redesign repair messages Many people have been...</td>\n",
              "      <td>redesign repair messages many people reporting...</td>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CASSANDRA-5121</td>\n",
              "      <td>system.peers.tokens is empty after node restar...</td>\n",
              "      <td>system.peers.tokens empty node restart using 2...</td>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CASSANDRA-11944</td>\n",
              "      <td>sstablesInBounds might not actually give all s...</td>\n",
              "      <td>sstablesinbounds might actually give sstables ...</td>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33d19bed-aee4-4892-8399-9480fa380228')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-33d19bed-aee4-4892-8399-9480fa380228 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-33d19bed-aee4-4892-8399-9480fa380228');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f2524c7f-a0a3-4b3e-9975-19d035cf037d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f2524c7f-a0a3-4b3e-9975-19d035cf037d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f2524c7f-a0a3-4b3e-9975-19d035cf037d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset2",
              "summary": "{\n  \"name\": \"dataset2\",\n  \"rows\": 226,\n  \"fields\": [\n    {\n      \"column\": \"issue_key\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"CASSANDRA-2950\",\n          \"CASSANDRA-18803\",\n          \"CASSANDRA-17509\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SummaryDescriptionComments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"Data from truncated CF reappears after server restart * Configure 3 node cluster\\n* Ensure the java stress tool creates Keyspace1 with RF=3\\n\\n{code}\\n// Run Stress Tool to generate 10 keys, 1 column\\nstress --operation=INSERT -t 2 --num-keys=50 --columns=20 --consistency-level=QUORUM --average-size-values --replication-factor=3 --create-index=KEYS --nodes=cathy1,cathy2\\n\\n// Verify 50 keys in CLI\\nuse Keyspace1; \\nlist Standard1; \\n\\n// TRUNCATE CF in CLI\\nuse Keyspace1;\\ntruncate counter1;\\nlist counter1;\\n\\n// Run stress tool and verify creation of 1 key with 10 columns\\nstress --operation=INSERT -t 2 --num-keys=1 --columns=10 --consistency-level=QUORUM --average-size-values --replication-factor=3 --create-index=KEYS --nodes=cathy1,cathy2\\n\\n// Verify 1 key in CLI\\nuse Keyspace1; \\nlist Standard1; \\n\\n// Restart all three nodes\\n\\n// You will see 51 keys in CLI\\nuse Keyspace1; \\nlist Standard1; \\n{code}\\n\\n\\n \\\\n* /cassandra/branches/cassandra-0.8/CHANGES.txt\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java\\\\n* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java\\\\n \",\n          \"Refactor validation logic in StorageService.rebuild This is a follow-up ticket of CASSANDRA-14319 [~aweisberg] would you mind to take a look? Super easy. On your +1 I ll do all the builds for 6 branches.' '+1 TY' '[3.0|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3035/workflows/7f55fcda-1cf0-43db-8471-ebd54be87c9e]\\\\r\\\\n\\\\r\\\\n[3.11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3034/workflows/e10a709e-97e1-4795-b4ca-7e3aac3253cd]\\\\r\\\\n\\\\r\\\\n[4.0 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/f2e2c30f-6d37-48cc-874a-104f34f67b50]\\\\r\\\\n[4.0 j8|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/af8cc33a-69ca-4164-800f-3b049a8ac6da]\\\\r\\\\n\\\\r\\\\n[4.1 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5297fd1a-f33b-4a17-b56d-d0522b65c95b]\\\\r\\\\n[4.1 j8|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5626cfef-a6a7-453c-9abf-43c30678ec41]\\\\r\\\\n\\\\r\\\\n[5.0 j17|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/70cf2550-e278-426a-a63b-533c25c4eccf]\\\\r\\\\n[5.0 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/2a5dcdc5-17f9-4311-9aeb-60fcea5787d9]\\\\r\\\\n\\\\r\\\\nThese builds are technically not same as what is in the current branches. I just moved one check outside of try-catch to add it logically where it belongs (where other checks are) and I moved logging into try catch (was outside of it). \",\n          \"Add Guardrail to disable GROUP BY functionality GROUP BY can be expensive and troublesome on large tables. We should have a guardrail to disable this in clusters where we don't want users to have this functionality. [PR|https://github.com/apache/cassandra/pull/1543]\\\\r\\\\n[JDK8 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/199/workflows/94525b46-3917-428f-bf96-f8671b1a7ac3]\\\\r\\\\n[JDK11 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/199/workflows/df58d46d-cf06-4f00-869c-0f1a8752f1c6]' 'Patch LGTM I left small comments mostly about \\\"is\\\" vs \\\"get\\\" in MBeans/Interfaces... I see that we use \\\"get\\\" already but this is actually incorrect... so think we should also fix (100% cool not in this patch I accept consistency here over correctness)' 'SelectStatement#prepare(boolean forView) is called from View#getSelectStatement() but once that code path is executed SelectStatement#prepare(ClientState state) is not called. I would say there is a need to propagate ClientState to SelectStatement#prepare(boolean forView)' '[~smiklosovic] I might be wrong but I think that {{View#getSelectStatement()}} is only used by internal queries to get the query used on the {{CREATE VIEW}} query to populate the MV table from the base table contents. I think that query never supports {{{}GROUP BY{}}}. The path for querying MVs seems to work as expected:\\\\r\\\\n{code:java}\\\\r\\\\n@Test\\\\r\\\\npublic void checkView() throws Throwable\\\\r\\\\n{\\\\r\\\\n    setGuardrail(false);\\\\r\\\\n    createTable( \\\"CREATE TABLE %s(pk int ck int v int PRIMARY KEY(pk ck))\\\");\\\\r\\\\n    String viewName = createView(\\\"CREATE MATERIALIZED VIEW %s AS \\\" +\\\\r\\\\n                                 \\\"SELECT * FROM %s WHERE pk IS NOT null and ck IS NOT null \\\" +\\\\r\\\\n                                 \\\"PRIMARY KEY(ck pk)\\\");\\\\r\\\\n    String viewQuery = \\\"SELECT * FROM \\\" + viewName + \\\" WHERE ck=0 GROUP BY pk\\\";\\\\r\\\\n    assertFails(viewQuery \\\"GROUP BY functionality is not allowed\\\");\\\\r\\\\n    testExcludedUsers(() -> viewQuery);\\\\r\\\\n} {code}' 'yes I think you are right thanks for testing that. Please add that test there.' \\\"Added the couple extra tests and tweaked the query so we didn't need to differentiate warning type on query results and force pushed.\\\\r\\\\n\\\\r\\\\nWe good to go here [~adelapena]?\\\" 'latest changes LGTM' 'Looks good to me we only need a rebase fixing the (trivial) conflicts with the recently added guardrails and a final CI round.' 'Had a rebase w/clean run last Friday; went ahead and did one final one this morning and CI kicked off.\\\\r\\\\n\\\\r\\\\n[JDK8 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851]\\\\r\\\\n[JDK11 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/abb10877-f285-49b4-9d44-fb852fb8a584]' \\\"The failures at\\\\xa0[testMetricsCleanupOnDrop|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851/jobs/1977] and [testConnectionsAreRejectedWithInvalidConfig|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851/jobs/1983] in the runs above don't seem related to the changes so I think the results look good. Those tests failures don't appear on [Butler|https://butler.cassandra.apache.org/#/ci/upstream/compare/Cassandra-trunk/trunk] though and I haven't found tickets for them so we should probably create tickets for them.\\\" \\\"bq.  we should probably create tickets for them.\\\\r\\\\nAgree; I ran a clean (i.e. vanilla trunk) circle run earlier today as a reference and am planning on getting together a focused effort to get us to stable green there in the run up to the freeze. I'll make sure failures from here make it to that effort.\\\" \\\"I just saw testMetricsCleanupOnDrop in my runs was there a ticket opened or should I open? (wasn't sure whether there is not some umbrella ticket or anything that I am missing)\\\" \\\"I forgot about that failure and I can't find any ticket for it. The failure can be reproduced on the CircleCI's multiplexer for both 4.1 and trunk although I think we haven't seen it yet on Jenkins. I have created CASSANDRA-17658 for fixing it.\\\"] \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"data truncated cf reappears server restart * configure 3 node cluster * ensure java stress tool creates keyspace1 rf=3 { code } // run stress tool generate 10 keys , 1 column stress -- operation=insert -t 2 -- num-keys=50 -- columns=20 -- consistency-level=quorum -- average-size-values -- replication-factor=3 -- create-index=keys -- nodes=cathy1 , cathy2 // verify 50 keys cli use keyspace1 ; list standard1 ; // truncate cf cli use keyspace1 ; truncate counter1 ; list counter1 ; // run stress tool verify creation 1 key 10 columns stress -- operation=insert -t 2 -- num-keys=1 -- columns=10 -- consistency-level=quorum -- average-size-values -- replication-factor=3 -- create-index=keys -- nodes=cathy1 , cathy2 // verify 1 key cli use keyspace1 ; list standard1 ; // restart three nodes // see 51 keys cli use keyspace1 ; list standard1 ; { code } \\\\n * /cassandra/branches/cassandra-0.8/changes.txt\\\\n * /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/systemtable.java\\\\n * /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/recoverymanagertruncatetest.java\\\\n * /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/commitlog.java\\\\n * /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/columnfamilystore.java\\\\n\",\n          \"refactor validation logic storageservice.rebuild follow-up ticket cassandra-14319 [ ~aweisberg ] would mind take look ? super easy . +1 builds 6 branches . ' '+1 ty ' ' [ 3.0|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3035/workflows/7f55fcda-1cf0-43db-8471-ebd54be87c9e ] \\\\r\\\\n\\\\r\\\\n [ 3.11|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3034/workflows/e10a709e-97e1-4795-b4ca-7e3aac3253cd ] \\\\r\\\\n\\\\r\\\\n [ 4.0 j11|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/f2e2c30f-6d37-48cc-874a-104f34f67b50 ] \\\\r\\\\n [ 4.0 j8|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/af8cc33a-69ca-4164-800f-3b049a8ac6da ] \\\\r\\\\n\\\\r\\\\n [ 4.1 j11|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5297fd1a-f33b-4a17-b56d-d0522b65c95b ] \\\\r\\\\n [ 4.1 j8|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5626cfef-a6a7-453c-9abf-43c30678ec41 ] \\\\r\\\\n\\\\r\\\\n [ 5.0 j17|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/70cf2550-e278-426a-a63b-533c25c4eccf ] \\\\r\\\\n [ 5.0 j11|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/2a5dcdc5-17f9-4311-9aeb-60fcea5787d9 ] \\\\r\\\\n\\\\r\\\\nthese builds technically current branches . moved one check outside try-catch add logically belongs ( checks ) moved logging try catch ( outside ) .\",\n          \"add guardrail disable group functionality group expensive troublesome large tables . guardrail disable clusters n't want users functionality . [ pr|https : //github.com/apache/cassandra/pull/1543 ] \\\\r\\\\n [ jdk8 ci|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/199/workflows/94525b46-3917-428f-bf96-f8671b1a7ac3 ] \\\\r\\\\n [ jdk11 ci|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/199/workflows/df58d46d-cf06-4f00-869c-0f1a8752f1c6 ] ' 'patch lgtm left small comments mostly `` '' vs `` get '' mbeans/interfaces ... see use `` get '' already actually incorrect ... think also fix ( 100 % cool patch accept consistency correctness ) ' 'selectstatement # prepare ( boolean forview ) called view # getselectstatement ( ) code path executed selectstatement # prepare ( clientstate state ) called . would say need propagate clientstate selectstatement # prepare ( boolean forview ) ' ' [ ~smiklosovic ] might wrong think { { view # getselectstatement ( ) } } used internal queries get query used { { create view } } query populate mv table base table contents . think query never supports { { { } group { } } } . path querying mvs seems work expected : \\\\r\\\\n { code : java } \\\\r\\\\n @ test\\\\r\\\\npublic void checkview ( ) throws throwable\\\\r\\\\n { \\\\r\\\\n setguardrail ( false ) ; \\\\r\\\\n createtable ( `` create table % ( pk int ck int v int primary key ( pk ck ) ) '' ) ; \\\\r\\\\n string viewname = createview ( `` create materialized view % `` +\\\\r\\\\n `` select * % pk null ck null `` +\\\\r\\\\n `` primary key ( ck pk ) '' ) ; \\\\r\\\\n string viewquery = `` select * `` + viewname + `` ck=0 group pk '' ; \\\\r\\\\n assertfails ( viewquery `` group functionality allowed '' ) ; \\\\r\\\\n testexcludedusers ( ( ) - > viewquery ) ; \\\\r\\\\n } { code } ' 'yes think right thanks testing . please add test . ' `` added couple extra tests tweaked query n't need differentiate warning type query results force pushed.\\\\r\\\\n\\\\r\\\\nwe good go [ ~adelapena ] ? '' 'latest changes lgtm ' 'looks good need rebase fixing ( trivial ) conflicts recently added guardrails final ci round . ' 'had rebase w/clean run last friday ; went ahead one final one morning ci kicked off.\\\\r\\\\n\\\\r\\\\n [ jdk8 ci|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851 ] \\\\r\\\\n [ jdk11 ci|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/abb10877-f285-49b4-9d44-fb852fb8a584 ] ' `` failures at\\\\xa0 [ testmetricscleanupondrop|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851/jobs/1977 ] [ testconnectionsarerejectedwithinvalidconfig|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851/jobs/1983 ] runs n't seem related changes think results look good . tests failures n't appear [ butler|https : //butler.cassandra.apache.org/ # /ci/upstream/compare/cassandra-trunk/trunk ] though n't found tickets probably create tickets . '' `` bq . probably create tickets them.\\\\r\\\\nagree ; ran clean ( i.e . vanilla trunk ) circle run earlier today reference planning getting together focused effort get us stable green run freeze . 'll make sure failures make effort . '' `` saw testmetricscleanupondrop runs ticket opened open ? ( n't sure whether umbrella ticket anything missing ) '' `` forgot failure ca n't find ticket . failure reproduced circleci 's multiplexer 4.1 trunk although think n't seen yet jenkins . created cassandra-17658 fixing . '' ]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Textual_Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"AI_Yes\",\n          \"AI_No\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 347
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset2['processed_text'][0]"
      ],
      "metadata": {
        "id": "wAfhnotvGNB6",
        "outputId": "1edb2367-7f53-4669-e797-98b52134489b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    encryptionoptions instantiated title says , ot...\n",
              "0    sstablesinbounds might actually give sstables ...\n",
              "Name: processed_text, dtype: object"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>encryptionoptions instantiated title says , otherwise get npe options missing yaml . 's included second patch cassandra-3045 one line fix . there\\ 's bunch `` encryption options null ignore '' special cases already you\\ 're going instantiate default instead let\\ 's get rid those.\\n\\nmay also need applied 0.8 unless aforesaid special cases cover everything . ' ' could find special case added first time fixed back 0.8 cassandra-3007 . attached patch removes instantiates default instead . ' `` hmm . thought place otc 's going npe current code base . +1 patch . '' `` ( checked 0.8 otc null check . 're good . ) '' 'committed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sstablesinbounds might actually give sstables within bounds due start positions moved sstables problem cassandra-11886 - try fetch sstablesinbounds canonical_sstables , miss actually overlapping sstables . 3.0+ state sstableset want calling method . looks like issue could cause include many sstables compactions think contain droppable tombstones https : //github.com/krummas/cassandra/commits/marcuse/intervaltreesstableset\\nhttp : //cassci.datastax.com/view/dev/view/krummas/job/krummas-marcuse-intervaltreesstableset-testall/\\nhttp : //cassci.datastax.com/view/dev/view/krummas/job/krummas-marcuse-intervaltreesstableset-dtest/\\n\\npatch remove option pick sstableset want returned live sstables supported . want canonical sstables within bounds provide intervaltree built sstables.\\n\\nalso includes cassandra-11886 part might change depending review ticket ' ' [ ~benedict ] - bandwidth review well along w/cassandra-11886 ? ' 'life busy right sure ... ' 'no doubt - hope context would similar enough 11886 delta would pretty small add top . looking make habit . : ) ' 'habits imply future date life hopefully hectic . currently mid-demolition rebuild home eating free non-free time alike . ' `` 'm little confused patch jira comment - n't see ( branch ) removal option provide sstableset ... '' 'maybe looking wrong commit ? rebased squashed [ here|https : //github.com/krummas/cassandra/commits/marcuse/intervaltreesstableset ] \\n { code } \\n- public collection &lt; sstablereader &gt; getoverlappingsstables ( sstableset sstableset iterable &lt; sstablereader &gt; sstables ) \\n+ public collection &lt; sstablereader &gt; getoverlappingsstables ( iterable &lt; sstablereader &gt; sstables ) \\n { code } ' `` probably - earlier commits seemed ticket . thanks.\\n\\ni 'll proper read later would suggest renaming methods include implicit sstableset.live 's still minimally front-and-centre functionality used perhaps renaming select selectlive sstablesinbounds perhaps inboundslive ( sstables ) ? '' 'pushed new commit method renames branch triggered new cassci builds ' 'ping [ ~benedict ] ' '+1 ' 'committed thanks !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 386
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configura o ambiente para o modelo de IA"
      ],
      "metadata": {
        "id": "8SaPeS5t2cn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'distilbert-base-uncased'\n",
        "device_name = 'cuda'\n",
        "#device_name = 'cpu'\n",
        "max_length = 512\n",
        "cached_model_directory_name = 'distilbert-ehbugs'"
      ],
      "metadata": {
        "id": "jRi1taoo2hfP"
      },
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "1RsJiUb-2vjl"
      },
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGlZKOMC26q_",
        "outputId": "4056a049-0ba8-4f89-ce93-9433513a96c6"
      },
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 351
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe de apoio para manipular o dataset"
      ],
      "metadata": {
        "id": "1Nww3yh19bHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def cap_number(x):\n",
        "    if x > 1:\n",
        "      return 1\n",
        "    elif x < 0:\n",
        "      return 0\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    # preds = pred.predictions.argmax(-1)\n",
        "    outputs = pred.predictions.flatten().tolist()\n",
        "    probas = [cap_number(x) for x in outputs]\n",
        "    preds = np.array(np.array(probas) > 0.5, dtype=int)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "      'accuracy': acc,\n",
        "    }"
      ],
      "metadata": {
        "id": "Ol8kTbJK3Bak"
      },
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo 'Cria a pasta results'\n",
        "!rm -rf results\n",
        "!mkdir results\n",
        "!echo 'Cria a pasta logs'\n",
        "!rm -rf logs\n",
        "!mkdir logs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtO-J8PS3GQ7",
        "outputId": "0fffe34e-7053-443b-d777-556dbbe89cf9"
      },
      "execution_count": 353,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cria a pasta results\n",
            "Cria a pasta logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configura os argumentos para o treinamento"
      ],
      "metadata": {
        "id": "faXeHLbk94i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    learning_rate=5e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=100,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='/content/results',   # output directory\n",
        "    logging_dir='/content/logs',     # directory for storing logs\n",
        "    logging_steps=150,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTMjQ3pk3Ktr",
        "outputId": "e3b94cf4-1ae5-4a97-f772-f8489e0c95a3"
      },
      "execution_count": 354,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = {'AI_Yes', 'AI_No'}\n",
        "label2id = {'AI_No': 0, 'AI_Yes': 1}\n",
        "id2label = {0: 'AI_No', 1: 'AI_Yes'}"
      ],
      "metadata": {
        "id": "t3fOa6Jq6WTZ"
      },
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cria um StratifiedKFold\n",
        "\n",
        "Stratified K-Fold cross-validator.\n",
        "\n",
        "Provides train/test indices to split data in train/test sets.\n",
        "\n",
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ],
      "metadata": {
        "id": "-vFY4JZK99Un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
        "X, y = dataset['processed_text'], dataset['Textual_Type']\n",
        "skf.get_n_splits(X, y)\n",
        "folds = {}"
      ],
      "metadata": {
        "id": "6UT_oikW3Nd_"
      },
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Sph60eOZ6rI9",
        "outputId": "cf87ffbb-3e70-4f1f-de90-4e4b02fe1da6"
      },
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     encryptionoptions instantiated title says , ot...\n",
              "1     log writing many tombstones partition log writ...\n",
              "2     redesign repair messages many people reporting...\n",
              "3     system.peers.tokens empty node restart using 2...\n",
              "4     sstablesinbounds might actually give sstables ...\n",
              "                            ...                        \n",
              "21    duplicate rows returned clause repeated values...\n",
              "22    examine shortening path length post-5202 cassa...\n",
              "23    throw error auto_bootstrap : true bootstrappin...\n",
              "24    support wrapped range queries want support sca...\n",
              "25    support total/recent latency histogram metrics...\n",
              "Name: processed_text, Length: 226, dtype: object"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>encryptionoptions instantiated title says , otherwise get npe options missing yaml . 's included second patch cassandra-3045 one line fix . there\\ 's bunch `` encryption options null ignore '' special cases already you\\ 're going instantiate default instead let\\ 's get rid those.\\n\\nmay also need applied 0.8 unless aforesaid special cases cover everything . ' ' could find special case added first time fixed back 0.8 cassandra-3007 . attached patch removes instantiates default instead . ' `` hmm . thought place otc 's going npe current code base . +1 patch . '' `` ( checked 0.8 otc null check . 're good . ) '' 'committed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>log writing many tombstones partition log writing many tombstones partition like writing large partition https : //github.com/krummas/cassandra/commits/marcuse/16780\\r\\n\\r\\nhttps : //app.circleci.com/pipelines/github/krummas/cassandra ? branch=marcuse % 2f16780 ' ' think add yaml comments explaining . ' 'yep good point pushed fix ' '+1 ' `` n't think technically put 4.0.x unless 's considered bug unfortunately . '' 'committed thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>redesign repair messages many people reporting 'repair hang ' something goes wrong . two major causes hang 1 ) validation failure 2 ) streaming failure . currently , failures happen , failed node would respond back repair initiator . goal ticket redesign message flows around repair repair never hang . https : //github.com/yukim/cassandra/commits/5426-3\\n\\nremoved classes kept backward compatibility.\\n\\nbq . one thing i\\ 'm sure seems get error log doesn\\'t error repair session . maybe otherwise fear people won\\'t notice something went wrong.\\nbq . also fail maybe could send error message ( typically exception message ) easier debugging/reporting.\\n\\nthe latest version notifies user throwing exception filled ( repairsession # exception ) error occurred . sending exception back coordinator useful i\\ 'd rather take different approach use tracing cf ( cassandra-5483 ) .\\n\\nbq . also wonder maybe fail-fast policy errors . instance one node fail it\\ 's validation phase maybe might worth failing right away let user re-trigger repair fixed whatever source error rather still differencing/syncing nodes ( admit solutions possible ) .\\n\\ni changed let repair session fail error occurred think better repair option ( something like -k -- keep-going ) keep repair running report failed session/job end . +1 separate ticket.\\n\\nbq . going bit think add 2 messages interrupt validation sync phase . could useful users need stop repair reason also get error validation one node could use interrupt nodes thus fail fast minimizing amount work done uselessly . anyway guess part done follow ticket.\\n\\n+1 separate ticket . also need add way abort streaming interrupt syncing.\\n\\nbq . repairmessagetype gossip proof could wise add `` future '' type say 4 5 `` case '' .\\nbq . really need repairmessageheader ? making repairmessage repairjobdesc repairmessagetype body rather creating yet another class ? \\n\\nfor messages mimicked way o.a.c.transport.messages does.\\n\\nbq . hashcode methods ( differencer nodepair repairjobdesc ... ) i\\ 'd prefer using guava\\ 's objects.hashcode ( ) ( objects.equal ( ) equals ( ) null ) .\\n\\ndone didn\\'t miss anything.\\n\\nbq . would move gossiper/failure registration ars.addtoactivesessions.\\n\\ndone.\\n\\nbq . i\\ 'd remove validator.rangetovalidate inline desc.range.\\n\\ndone.\\n\\nbq . curiosity mean todo comment validator.add ( ) .\\n\\nthat comment ancient version . removed since longer applicable.\\n\\nbq . merkletree.fullrange maybe it\\ 's time add mt serializer rather restoring manually ugly error prone . aslo partitioner let\\ 's maybe mt uses databasedescriptor.getpartitioner ( ) directly rather restoring manually differencer.run ( ) .\\n\\nyup good time finally cleanup merkletree serialization . done.\\n ' `` streamingrepairtask.initiatestreaming ( ) 's block\\n\\n { code } try\\n { \\n ... \\n streamout.transfersstables ( outsession sstables request.ranges operationtype.aes ) ; \\n // request ranges remote node\\n streamin.requestranges ( request.dst desc.keyspace collections.singleton ( cfstore ) request.ranges operationtype.aes ) ; \\n } \\ncatch ( exception e ) ... { code } \\n\\nis value putting streamin.requestranges ( ) separate try block ( immediately ) fail streamout problem ? could potentially make forward progress ( stream streamin ) even streamout fails ? 'll note 1.2 try/catch yuki 's new work changed regard.\\n\\n\\n\\n '' `` [ ~jasobrown ] actually think try catch block redundant . streaming run thread streamingrepairtask exception handled istreamcallback 's onerror method ( empty current 1.2 ) .\\ni 'm trying overhaul streaming api 2.0 ( cassandra-5286 ) fine grained control streaming . '' 'yuki confirms https : //github.com/yukim/cassandra/commits/5426-3 ready review . ' `` alright v3 lgtm +1.\\n\\ni 've committed though 'll note currently repair tends get stuck due cassandra-5699 ( 've checked ok patch cassandra-5699 ) . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>system.peers.tokens empty node restart using 2 nodes fresh cluster ( 127.0.0.1 &amp; 127.0.0.2 ) running latest 1.2 , ’ querying system.peers get nodes cluster respective token . seems problem either node restart . node starts , querying system.peers seems ok : { code } 127.0.0.1 &gt; select * system.peers ; + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ | data_center | host_id | peer | rack | release_version | rpc_address | schema_version | tokens | +=================+==========================================+===============+===========+=====================+=================+==========================================+===========================================+ | datacenter1 | 4819cbb0-9741-4fe0-8d7d-95941b0247bf | 127.0.0.2 | rack1 | 1.2.0 | 127.0.0.2 | 59adb24e-f3cd-3e02-97f0-5b395827453f | 56713727820156410577229101238628035242 | + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ { code } soon one node restarted ( let ’ say 127.0.0.2 ) , tokens column empty : { code } 127.0.0.1 &gt; select * system.peers ; + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ | data_center | host_id | peer | rack | release_version | rpc_address | schema_version | tokens | +=================+==========================================+===============+===========+=====================+=================+==========================================+=============+ | datacenter1 | 4819cbb0-9741-4fe0-8d7d-95941b0247bf | 127.0.0.2 | rack1 | 1.2.0 | 127.0.0.2 | 59adb24e-f3cd-3e02-97f0-5b395827453f | | + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ { code } { code } log server side : debug responding : rows [ peer ( system , peers ) , org.apache.cassandra.db.marshal.inetaddresstype ] [ data_center ( system , peers ) , org.apache.cassandra.db.marshal.utf8type ] [ host_id ( system , peers ) , org.apache.cassandra.db.marshal.uuidtype ] [ rack ( system , peers ) , org.apache.cassandra.db.marshal.utf8type ] [ release_version ( system , peers ) , org.apache.cassandra.db.marshal.utf8type ] [ rpc_address ( system , peers ) , org.apache.cassandra.db.marshal.inetaddresstype ] [ schema_version ( system , peers ) , org.apache.cassandra.db.marshal.uuidtype ] [ tokens ( system , peers ) , org.apache.cassandra.db.marshal.settype ( org.apache.cassandra.db.marshal.utf8type ) ] | 127.0.0.2 | datacenter1 | 4819cbb0-9741-4fe0-8d7d-95941b0247bf | rack1 | 1.2.0 | 127.0.0.2 | 59adb24e-f3cd-3e02-97f0-5b395827453f | null { code } restarting node ( 127.0.0.1 ) restore back tokens column . removeendpoint used instead\\n [ junit ] \\tat org.apache.cassandra.db.systemtable.updatetokens ( ) \\n [ junit ] \\tat org.apache.cassandra.db.systemtable.updatelocaltokens ( ) \\n [ junit ] \\tat org.apache.cassandra.service.storageservice.handlestatenormal ( ) \\n [ junit ] \\tat org.apache.cassandra.service.storageservice.onchange ( ) \\n [ junit ] \\tat org.apache.cassandra.service.relocatetest.testrelocationsuccess ( ) \\n [ junit ] \\n [ junit ] \\n [ junit ] test org.apache.cassandra.service.relocatetest failed\\n ... \\n { noformat } \\n ' 'you update fixed 17adf8e4f72114d336140fac5157a35e63d1f53a ' 'updated ; test passes thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sstablesinbounds might actually give sstables within bounds due start positions moved sstables problem cassandra-11886 - try fetch sstablesinbounds canonical_sstables , miss actually overlapping sstables . 3.0+ state sstableset want calling method . looks like issue could cause include many sstables compactions think contain droppable tombstones https : //github.com/krummas/cassandra/commits/marcuse/intervaltreesstableset\\nhttp : //cassci.datastax.com/view/dev/view/krummas/job/krummas-marcuse-intervaltreesstableset-testall/\\nhttp : //cassci.datastax.com/view/dev/view/krummas/job/krummas-marcuse-intervaltreesstableset-dtest/\\n\\npatch remove option pick sstableset want returned live sstables supported . want canonical sstables within bounds provide intervaltree built sstables.\\n\\nalso includes cassandra-11886 part might change depending review ticket ' ' [ ~benedict ] - bandwidth review well along w/cassandra-11886 ? ' 'life busy right sure ... ' 'no doubt - hope context would similar enough 11886 delta would pretty small add top . looking make habit . : ) ' 'habits imply future date life hopefully hectic . currently mid-demolition rebuild home eating free non-free time alike . ' `` 'm little confused patch jira comment - n't see ( branch ) removal option provide sstableset ... '' 'maybe looking wrong commit ? rebased squashed [ here|https : //github.com/krummas/cassandra/commits/marcuse/intervaltreesstableset ] \\n { code } \\n- public collection &lt; sstablereader &gt; getoverlappingsstables ( sstableset sstableset iterable &lt; sstablereader &gt; sstables ) \\n+ public collection &lt; sstablereader &gt; getoverlappingsstables ( iterable &lt; sstablereader &gt; sstables ) \\n { code } ' `` probably - earlier commits seemed ticket . thanks.\\n\\ni 'll proper read later would suggest renaming methods include implicit sstableset.live 's still minimally front-and-centre functionality used perhaps renaming select selectlive sstablesinbounds perhaps inboundslive ( sstables ) ? '' 'pushed new commit method renames branch triggered new cassci builds ' 'ping [ ~benedict ] ' '+1 ' 'committed thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>partially heap memtables move contents bytebuffers off-heap records written memtable . ( see comments details ) removed couple used methods added couple refaction.impossible ( ) instead null changed memtable code avoids double cast minor typo.\\n\\nthanks . 'll incorporate cassandra-6694 instead 's okay ? '' `` bq . starts new thread gc work done room static pool n't threads n't timed . cleaner thread started remains forever . mean 're worried flushing many memtables gc work possible would spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>introduce transactional api behaviours corrupt system state penultimate ( probably final 2.1 , agree introduce ) round changes internals managing sstable writing , 've introduced new api called `` transactional '' hope make much easier write correct behaviour . things stand conflate lot behaviours methods like `` close '' - recent changes unpicked , n't go far enough . proposal introduces interface designed support four actions ( top normal function ) : * preparetocommit * commit * abort * cleanup normal operation , finished constructing state change call preparetocommit ; state changes prepared , call commit . point everything fails , abort called . _either_ case , cleanup called last . transactional objects autocloseable , behaviour rollback changes unless commit completed successfully . changes actually less invasive might sound , since recently introduce abort places , well commit like methods . simply formalises behaviour , makes consistent objects interact way . much code change boilerplate , moving object try-declaration , although change still non-trivial . _does_ eliminate _lot_ special casing since 2.1 released . data tracker api changes compaction leftover cleanups finish job making much easier reason , change think worthwhile considering 2.1 , since 've overhauled entire area ( released changes ) , change essentially finishing touches , risk minimal potential gains reasonably significant . 1 ) really horrendous generics ; 2 ) moving preparetocommit transactional making no-args requiring commit preparation arguments provided separate method ; 3 ) leaving as-is . \\n\\ni think 'm leaning towards ( 2 ) though may change mind taken conclusion . n't perfect allow us clearly codify correct behaviours cost needing little use only-temporary builder-like state inside transactional objects preparetocommit parameters also return values ( like sstablerewriter sstablewriter return list readers reader respectively ) .\\n\\nbq . may want convert touched /io tests take advantage exercise various writers transactional\\n\\nyeah . reader tests probably perhaps introduce special sequentialwriter test work kinds implementation test behaviours consistent transactional . appear kind sstablewriter test either . think separate ticket since scope much broader perhaps introduce starter touching functionality file follow-up . '' `` ok 've pushed smallish refactor puts preparetocommit ( )</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>make sstable2json output readable sstable2json writes entire file single json line . also , local timestamp delete given hex bytes instead int . fix attached ' 'also renames `` columns '' field output `` cells '' ' ' feel like missing matching changes sstableimport . make sstableexporttest pass first look ? ' 'v2 ' 'now sstableimporttest . v3 ? ' 'v3 also removes `` old '' format support import ( pre-1.0 stuff ) supercolumn support . ' 'lgtm +1 . ' 'committed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>operational improvements &amp; hardening replica filtering protection cassandra-8272 uses additional space heap ensure correctness 2i filtering queries consistency levels one/local_one . things follow , however , make life bit easier operators generally de-risk usage : ( note : line numbers based { { trunk } } { { 3cfe3c9f0dcf8ca8b25ad111800a21725bf152cb } } . ) * minor optimizations * * { { } } - given size up-front , may able use simple arrays instead lists { { rowstofetch } } { { originalpartitions } } . alternatively ( also ) , may able null references two collections aggressively . ( ex . using { { arraylist # set ( ) } } instead { { get ( ) } } { { queryprotectedpartitions ( ) } } , assuming pass { { tofetch } } argument { { querysourceonkey ( ) } } . ) * { { } } - may able use { { encodingstats.merge ( ) } } remove custom { { stats ( ) } } method . * { { &amp; 228 } } - cache instance { { unaryoperator # identity ( ) } } instead creating one fly . * { { } } - may able scatter/gather rather serially querying every row needs completed . n't clear win perhaps , given targets latency single queries adds complexity . ( certainly decent candidate kick even issue . ) * documentation intelligibility * * places ( changes.txt , tracing output { { replicafilteringprotection } } , etc . ) mention `` replica-side filtering protection '' ( makes seem like coordinator n't filter ) rather `` replica filtering protection '' ( sounds like actually , protect incorrect replica filtering results ) . 's minor fix , would avoid confusion . * method call chain { { dataresolver } } might bit simpler put { { repaireddatatracker } } { { resolvecontext } } . * testing * * want bite bullet get basic tests rfp ( including guardrails might add ) onto in-jvm dtest framework . * guardrails * * stands , n't way enforce upper bound memory usage { { replicafilteringprotection } } caches row responses first round requests . ( remember , later used merged second round results complete data filtering . ) operators likely need way protect , i.e . simply fail queries hit particular threshold rather gc nodes oblivion . ( control limits page sizes n't quite get us , stale results _expand_ number incomplete results must cache . ) fun question , primary axes scope ( per-query , global , etc . ) granularity ( per-partition , per-row , per-cell , actual heap usage , etc. ) . starting disposition right trade-off performance/complexity accuracy something along lines cached rows per query . prior art suggests probably makes sense alongside things like { { tombstone_failure_threshold } } { { cassandra.yaml } } . created cassandra-15948 '' ' [ ~maedhroz ] couldn\\'t resist giving try per-row lazy pre-fetch left [ here|https : //github.com/adelapena/cassandra/commit/accf2a47c341875942b0d8b06c016cc0d66d62cb ] .\\r\\n\\r\\ninstead consuming contents merged partition consumes row-per-row replica contents . way conflicts caches one row per replica instead entire partition per replica . also iif finds rows fetch replica advances first phase merged row iterator bit reaching certain cache size trying find balance cache size number rfp queries . right desired cache size hardcoded 100 could use config property query limit example . also could also let unbounded minimize number rfp queries main advantage approach absence conflicts nothing needs cached . benefit common case conflicts.\\r\\n\\r\\nto illustrate per-row approach behaves let\\ 's see example : \\r\\n { code : python } \\r\\nself._prepare_cluster ( \\r\\n create_table= '' create table ( k int c int v text primary key ( k c ) ) '' \\r\\n create_index= '' create index ( v ) '' \\r\\n both_nodes= [ `` insert ( k c v ) values ( 0 0 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 0 1 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 0 2 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 0 3 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 0 4 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 0 5 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 0 6 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 0 7 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 0 8 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 0 9 \\'old\\ ' ) '' ] \\r\\n only_node1= [ `` insert ( k c v ) values ( 0 4 \\'new\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 0 6 \\'new\\ ' ) '' ] ) \\r\\nself._assert_all ( `` select c v = \\'old\\ ' '' rows= [ [ 0 ] [ 1 ] [ 2 ] [ 3 ] [ 5 ] [ 7 ] [ 8 ] [ 9 ] ] ) \\r\\n { code } \\r\\nwithout per-row approach cached 20 rows ( 10 per replica ) issued one single rfp query . contrast per-row approach behaviour : \\r\\n * target cache size high unbounded cache max 12 rows need 1 rfp queries . less cached row don\\'t cache rows first conflict found fourth row.\\r\\n * target cache size 2 cache max 8 rows need 1 rfp queries . two conflicts fit window cached rows fetched don\\'t need cache rows.\\r\\n * use target cache size 1 cache max 6 rows differently need 2 separate rfp queries.\\r\\n * conflicts one cached row per replica ; current one.\\r\\n\\r\\nnote consuming rows first phase iterator populate cache still produce unlimited growth cache still need guardrail . configurable target cache size mention used try find balance cache size grouping primary keys fetch . ' 'bq . main advantage approach absence conflicts nothing needs cached\\r\\n\\r\\neven though partition-restricted queries without digest mismatches skip { { dataresolver } } entirely still helps case mismatch start large number conflict-free rows correct ? think benefit consider given likely dealing partition-restricted queries.\\r\\n\\r\\nbq . also could also let unbounded minimize number rfp queries\\r\\n\\r\\nthe one thing makes little uneasy extra logic need enforce `` target cache size '' . propose avoid simply leave guardrails we\\ 've already got place avoid catastrophe ( excessive rfp queries ) see means simplify remains ( like clear { { contents } } array list batches ) . i\\ 'll try see looks pull main 3.0 branch works.\\r\\n\\r\\naside think remaining question would verifying safety [ aggressively clearing|https : //github.com/apache/cassandra/pull/659/commits/30b8f4bebd95b3520b637d6d25d6bc16cb4d81a2 ] { { responses } } . ' `` [ ~adelapena ] tried strip lazy rows approach bit [ here|https : //github.com/maedhroz/cassandra/commit/c5abb49626da0141277de92e173fa8ed8062bcf3 ] . understand bit better 'm bit skeptical whether want proceed . already know partition-restricted case without digest mismatch avoids altogether . large number non-conflicting rows start first-phase iterator though seems like price avoiding row caching creating large number { { cachedrowiterator } } objects . maybe right trade-off 'm sure . '' `` [ ~maedhroz ] \\xa0i like changes lazy rows approach . however 'm afraid need snapshot cached rows done [ local copy-and-clear|https : //github.com/adelapena/cassandra/blob/accf2a47c341875942b0d8b06c016cc0d66d62cb/src/java/org/apache/cassandra/service/replicafilteringprotection.java # l522-l524 ] otherwise advances replica introduce new data mess producing multiple test failures . much better previous patch track number contents snapshot save us queue copy 's done [ here|https : //github.com/adelapena/cassandra/blob/35d8e712bbbe03076ba867c11759664e8ff839e4/src/java/org/apache/cassandra/service/replicafilteringprotection.java # l528-l568 ] .\\r\\n\\r\\nalso think making { { currentmergedrows } } / { { unprotectedpartition } } partition iterator correct . 's pointer current first iteration merged partition shared builders rfp . make local reduce speed pointer advanced producing end rfp queries.\\r\\n { quote } large number non-conflicting rows start first-phase iterator though seems like price avoiding row caching creating large number { { cachedrowiterator } } objects . maybe right trade-off 'm sure.\\r\\n { quote } \\r\\nwe find balance max cache size number { { cachedrowiterator } } instances try grow cache bit conflicts : \\r\\n { code : java } \\r\\nwhile ( unprotectedpartition ! = null &amp; &amp; unprotectedpartition.hasnext ( ) \\r\\n &amp; &amp; ( tofetch ! = null || cachedrows.size ( ) &lt; min_cache_size ) ) \\r\\n { code } \\r\\nmin cache/buffer size constant config property function warning threshold something related query limit . would still limit size cache absence conflicts quickly reducing number { { cachedrowiterator } } instances.\\r\\n\\r\\nalso given concerned cache size might want consider tracking max size cache reaches query add new table metric tracks average max cache size . '' ' [ ~adelapena ] quick slack discussion think we\\ 've landed following : \\r\\n\\r\\n1 . ) we\\ 'll stop partition-based lazy first-phase iterator consumption approach ( what\\ 's main patch branch right ) . it\\ 's clear { { min_cache_size } } ( avoid creating tons `` singleton '' { { cachedrowiterator } } instances ) would produce something meaningfully different.\\r\\n\\r\\n2 . ) interest visibility we\\ 'll explore adding histogram quantifies much row caching queries . would put data behind assumptions might help tuning guardrails well . ( i\\ 'll try something shortly ... ) ' `` case need explore per-row approach future 'm leaving [ here|https : //github.com/adelapena/cassandra/commit/90900ec717958270bc38b501b4248dfb7d55958c ] \\xa0the extensive prototype uses two properties control min cache size n't conflicts yet found conflicts . '' ' [ ~maedhroz ] metric cache size really nice . however think would useful track max per-query cache size rather per-partition size . single advance first phase merge iterator still insert indefinite amount partitions cache . example let\\ 's consider scenario rows outdated replica superseded updated replica : \\r\\n { code : java } \\r\\nself._prepare_cluster ( \\r\\n create_table= '' create table ( k int c int v text primary key ( k c ) ) '' \\r\\n create_index= '' create index ( v ) '' \\r\\n both_nodes= [ `` insert ( k c v ) values ( 0 0 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 0 1 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 1 0 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 1 1 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 2 0 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 2 1 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 3 0 \\'old\\ ' ) '' \\r\\n `` insert ( k c v ) values ( 3 1 \\'old\\ ' ) '' ] \\r\\n only_node1= [ `` delete k = 0 '' \\r\\n `` delete k = 1 '' \\r\\n `` delete k = 2 '' \\r\\n `` delete k = 3 '' ] ) \\r\\nself._assert_none ( `` select c v = \\'old\\ ' limit 1 '' ) \\r\\n { code } \\r\\nin test 4 partitions cached single advance merged iterator cache contains 16 rows maximum . however metric records 8 times per-partition cache size 2. think would useful either single metric max cache size 12 two records ( one per replica ) value 6. would make easier detect cases exposed example.\\r\\n\\r\\nalso per-query metric would easier advice operators start worrying consistency among replicas rfp metric starts get higher fetch size independently whether queries single-partition not.\\r\\n\\r\\nas tracking cache size per replica per query think would nice used criteria used guardrail measure thing . would mean either tracking metric per query leaving guardrail changing guardrail per-replica instead per-query . \\r\\n\\r\\nwdyt ? ' `` bq . tracking cache size per replica per query think would nice used criteria used guardrail measure thing . would mean either tracking metric per query leaving guardrail changing guardrail per-replica instead per-query.\\r\\n\\r\\ntracking metric per query rather changing existing guardrail sounds like right move although n't really legitimate multi-partition use-case yet time per-partition metric equivalent per-query one . hand whole point metric help provide guidance operators looking set appropriate warn/fail thresholds . 'll push something today along slightly modified inline</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>anti-compaction briefly corrupts sstable state reads since use multiple sstable rewriters anticompaction , first call preparetocommit remove original sstables tracker view rewriters add sstables . creates brief window reads miss data . sure going dtests though probably need restart ' 'nice catch looks like good fix me.\\r\\n\\r\\n ( +1 ) ' `` blake realised issue patch posted put together alternative patch input [ ~krummas ] .\\r\\n\\r\\n [ 3.0|https : //github.com/belliottsmith/cassandra/tree/15004-3.0 ] [ 3.11|https : //github.com/belliottsmith/cassandra/tree/15004-3.11 ] [ 4.0|https : //github.com/belliottsmith/cassandra/tree/15004-4.0 ] \\r\\n\\r\\nthese patches extract interface { { lifecycletransaction } } no-op relevant calls ( { { preparetocommit } } { { obsoleteoriginals } } ) { { sstablerewriter.preparetocommit } } update tracker - invoked directly rewriter finished preparatory work.\\r\\n\\r\\nit 's bit ugly still finicky probably better/safer invasive surgery point time . '' 'updated unit tests [ 3.0|https : //github.com/krummas/cassandra/tree/15004-3.0 ] [ 3.11|https : //github.com/krummas/cassandra/tree/15004-3.11 ] [ trunk|https : //github.com/krummas/cassandra/tree/15004-trunk ] also adds checks files disk expect ' 'lgtm need comments explaining going comment mentioning { { permitredundanttransitions } } needs removed/updated ' `` thanks . 've pushed branches updated comments . '' '+1 ' 'thanks committed [ 3.0|https : //github.com/apache/cassandra/commit/44785dd2eec5697eec7e496ed3a73d2573f4fe6a ] [ 3.11|https : //github.com/apache/cassandra/commit/9199e591c6148d14f3d12784af8ce5342f118161 ] [ 4.0|https : //github.com/apache/cassandra/commit/df62169d1b6a5bfff2bc678ffbeb0883a3a576b5 ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>consistency level send commit endpoints local_serial consistency level using local_serial , commit send local endpoints . commit needs sent endpoints dcs . [ ~slebresne ] review ' 'committed ( minor code style update ) thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>illegalargumentexception compactiontask ran largepartitionstest.test_11_1g trunk , found test fails due java.lang.illegalargumentexception compaction . exception apparently happens compaction merges large ( &gt; 2gb ) partition . { noformat } debug [ commit-log-allocator ] 2016-09-28 ? : ? - segments reserve ; creating fresh one debug [ commit-log-allocator ] 2016-09-28 ? : ? - segments reserve ; creating fresh one warn [ ] 2016-09-28 ? : ? - writing large partition cql_test_keyspace/table_4:1000000000000000000000 ( 1.004gib ) error [ ] 2016-09-28 ? : ? - fatal exception thread thread [ , main ] java.lang.illegalargumentexception : range : 2234434614 com.google.common.primitives.ints.checkedcast ( ) ~ [ guava-18.0.jar : na ] org.apache.cassandra.db.compaction.compactiontask.runmaythrow ( ) ~ [ main/ : na ] org.apache.cassandra.utils.wrappedrunnable.run ( ) ~ [ main/ : na ] org.apache.cassandra.db.compaction.compactiontask.executeinternal ( ) ~ [ main/ : na ] org.apache.cassandra.db.compaction.abstractcompactiontask.execute ( ) ~ [ main/ : na ] org.apache.cassandra.db.compaction.compactionmanager $ backgroundcompactioncandidate.run ( ) ~ [ main/ : na ] java.util.concurrent.executors $ runnableadapter.call ( ) ~ [ ] java.util.concurrent.futuretask.run ( ) ~ [ ] java.util.concurrent.threadpoolexecutor.runworker ( ) ~ [ ] java.util.concurrent.threadpoolexecutor $ worker.run ( ) [ ] java.lang.thread.run ( ) [ ] debug [ commit-log-allocator ] 2016-09-28 ? : ? - segments reserve ; creating fresh one debug [ commit-log-allocator ] 2016-09-28 ? : ? - segments reserve ; creating fresh one debug [ commit-log-allocator ] 2016-09-28 ? : ? - segments reserve ; creating fresh one debug [ commit-log-allocator ] 2016-09-28 ? : ? - segments reserve ; creating fresh one { noformat } { noformat } java.lang.runtimeexception : java.util.concurrent.executionexception : java.lang.illegalargumentexception : range : 2540348821 org.apache.cassandra.utils.throwables.maybefail ( ) org.apache.cassandra.utils.fbutilities.waitonfutures ( ) org.apache.cassandra.db.compaction.compactionmanager.performmaximal ( ) org.apache.cassandra.db.columnfamilystore.forcemajorcompaction ( ) org.apache.cassandra.db.columnfamilystore.forcemajorcompaction ( ) org.apache.cassandra.cql3.cqltester.compact ( ) org.apache.cassandra.io.sstable.largepartitionstest.lambda $ withpartitionsize $ 2 ( ) org.apache.cassandra.io.sstable.largepartitionstest.measured ( ) org.apache.cassandra.io.sstable.largepartitionstest.withpartitionsize ( ) org.apache.cassandra.io.sstable.largepartitionstest.test_11_1g ( ) sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) sun.reflect.nativemethodaccessorimpl.invoke ( ) sun.reflect.delegatingmethodaccessorimpl.invoke ( ) java.lang.reflect.method.invoke ( ) org.junit.runners.model.frameworkmethod $ 1.runreflectivecall ( ) org.junit.internal.runners.model.reflectivecallable.run ( ) org.junit.runners.model.frameworkmethod.invokeexplosively ( ) org.junit.internal.runners.statements.invokemethod.evaluate ( ) org.junit.internal.runners.statements.runbefores.evaluate ( ) org.junit.internal.runners.statements.runafters.evaluate ( ) org.junit.runners.blockjunit4classrunner.runchild ( ) com.intellij.junit4.junit4testrunnerutil $ ignoreignoredtestjunit4classrunner.runchild ( ) org.junit.runners.blockjunit4classrunner.runchild ( ) org.junit.runners.parentrunner.runchildren ( ) org.junit.runners.parentrunner.access $ 000 ( ) org.junit.runners.parentrunner $ 1.evaluate ( ) org.junit.internal.runners.statements.runbefores.evaluate ( ) org.junit.internal.runners.statements.runafters.evaluate ( ) org.junit.runners.parentrunner.run ( ) org.junit.runner.junitcore.run ( ) com.intellij.junit4.junit4ideatestrunner.startrunnerwithargs ( ) com.intellij.junit4.junit4ideatestrunner.startrunnerwithargs ( ) com.intellij.rt.execution.junit.junitstarter.preparestreamsandstart ( ) com.intellij.rt.execution.junit.junitstarter.main ( ) sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) sun.reflect.nativemethodaccessorimpl.invoke ( ) sun.reflect.delegatingmethodaccessorimpl.invoke ( ) java.lang.reflect.method.invoke ( ) com.intellij.rt.execution.application.appmain.main ( ) caused : java.util.concurrent.executionexception : java.lang.illegalargumentexception : range : 2540348821 java.util.concurrent.futuretask.report ( ) java.util.concurrent.futuretask.get ( ) org.apache.cassandra.utils.fbutilities.waitonfutures ( ) ... 37 caused : java.lang.illegalargumentexception : range : 2540348821 com.google.common.primitives.ints.checkedcast ( ) org.apache.cassandra.db.compaction.compactiontask.runmaythrow ( ) org.apache.cassandra.utils.wrappedrunnable.run ( ) org.apache.cassandra.db.compaction.compactiontask.executeinternal ( ) org.apache.cassandra.db.compaction.abstractcompactiontask.execute ( ) org.apache.cassandra.db.compaction.compactionmanager $ 10.runmaythrow ( ) org.apache.cassandra.utils.wrappedrunnable.run ( ) java.util.concurrent.executors $ runnableadapter.call ( ) java.util.concurrent.futuretask.run ( ) java.util.concurrent.threadpoolexecutor.runworker ( ) java.util.concurrent.threadpoolexecutor $ worker.run ( ) java.lang.thread.run ( ) { noformat } patch . could please review ? \\n\\nfix illegalargumentexception compactiontask\\nhttps : //github.com/matope/cassandra/commit/d6c40dd3d4d95dba8b9c3f88de1015315e45990d ' 'took patch added fix cleanup compaction . ci triggered.\\n\\n||cassandra-3.x| [ branch|https : //github.com/apache/cassandra/compare/cassandra-3.x ... ] | [ testall|http : //cassci.datastax.com/view/dev/view/snazy/job/snazy-12717-3.x-testall/lastsuccessfulbuild/ ] | [ dtest|http : //cassci.datastax.com/view/dev/view/snazy/job/snazy-12717-3.x-dtest/lastsuccessfulbuild/ ] \\n ' 'thanks patch ! ci looks good.\\n\\ncommitted [ 433dd1c0ab77d296dafcc6c2079aa9445a6c1b2a|https : //github.com/apache/cassandra/commit/433dd1c0ab77d296dafcc6c2079aa9445a6c1b2a ] [ cassandra-3.x|https : //github.com/apache/cassandra/tree/cassandra-3.x ] \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>skip columnfamilystore # toppartitions initialization client tool mode { { org.apache.cassandra.db.columnfamilystore } } { { toppartitions } } initialized keyspace system keyspace . however , running cassandra library client mode tool mode , initialization also happens . however , { { toppartitiontracker } } performs queries { { system } } keyspace , might available cases . reason , skip initialization { { toppartitions } } running client mode tool mode . utilities external libraries , produce warning displayed stack trace . warning misleading end users , cause confusion . importantly , initialization { { toppartitions } } required mode . warning similar : { code : java } warn org.apache.cassandra.db.systemkeyspace : could load stored top sizes partitions ... org.apache.cassandra.db.keyspacenotdefinedexception : keyspace system exist org.apache.cassandra.schema.schema.validatetable ( schema.java : xxx ) ~ [ ? : ? ] org.apache.cassandra.cql3.statements.selectstatement $ rawstatement.prepare ( selectstatement.java : xxx ) ~ [ ? : ? ] org.apache.cassandra.cql3.statements.selectstatement $ rawstatement.prepare ( selectstatement.java : xxx ) ~ [ ? : ? ] org.apache.cassandra.cql3.statements.selectstatement $ rawstatement.prepare ( selectstatement.java : xxx ) ~ [ ? : ? ] org.apache.cassandra.cql3.queryprocessor.parseandprepare ( queryprocessor.java : xxx ) ~ [ ? : ? ] ... { code } [ https : //github.com/apache/cassandra/pull/2535 ] ' '+1 thanks patch . ' 'preparing commit\\r\\n||branch||ci||\\r\\n| [ 4.1|https : //github.com/yifan-c/cassandra/tree/r/upstream/pr2535-4.1 ] | [ ci|https : //app.circleci.com/pipelines/github/yifan-c/cassandra ? branch=r % 2fupstream % 2fpr2535-4.1 ] |\\r\\n| [ trunk|https : //github.com/yifan-c/cassandra/tree/r/upstream/pr2519-trunk ] | [ ci|https : //app.circleci.com/pipelines/github/yifan-c/cassandra ? branch=r % 2fupstream % 2fpr2519-trunk ] |\\r\\n\\r\\n -- update -- \\r\\n\\r\\nboth ci runs look green . _known_ test failures due `` failed create test certs '' trunk . fixed another ticket.\\xa0 ' 'committed [ 9c796dfb2|https : //github.com/apache/cassandra/commit/9c796dfb272daa3ce57a2dc5cbeadd9273e1ac72 ] cassandra-4.1 merged trunk .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>nullpointerexception returned select ttl ( value ) , , order paging running query paging returns nullpointerexception : cqlsh : test &gt; select value , ttl ( value ) , last_modified test useruid='userid1 ' direction ( 'out ' , 'in ' ) order last_modified ; servererror : &lt; errormessage code=0000 [ server error ] message= '' java.lang.nullpointerexception '' &gt; 's stack trace system.log : error [ sharedpool-worker-1 ] 2015-09-17 - unexpected exception request java.lang.nullpointerexception : null org.apache.cassandra.db.marshal.longtype.comparelongs ( ) ~ [ ] org.apache.cassandra.db.marshal.timestamptype.compare ( ) ~ [ ] org.apache.cassandra.db.marshal.timestamptype.compare ( ) ~ [ ] org.apache.cassandra.cql3.statements.selectstatement $ singlecolumncomparator.compare ( ) ~ [ ] org.apache.cassandra.cql3.statements.selectstatement $ singlecolumncomparator.compare ( ) ~ [ ] java.util.timsort.countrunandmakeascending ( ) ~ [ ] java.util.timsort.sort ( ) ~ [ ] java.util.arrays.sort ( ) ~ [ ] java.util.arraylist.sort ( ) ~ [ ] java.util.collections.sort ( ) ~ [ ] org.apache.cassandra.cql3.statements.selectstatement.orderresults ( ) ~ [ ] org.apache.cassandra.cql3.statements.selectstatement.process ( ) ~ [ ] org.apache.cassandra.cql3.statements.selectstatement.processresults ( ) ~ [ ] org.apache.cassandra.cql3.statements.selectstatement.execute ( ) ~ [ ] org.apache.cassandra.cql3.statements.selectstatement.execute ( ) ~ [ ] org.apache.cassandra.cql3.statements.selectstatement.execute ( ) ~ [ ] org.apache.cassandra.cql3.queryprocessor.processstatement ( ) ~ [ ] com.datastax.bdp.cassandra.cql3.dsequeryhandler $ statementexecution.execute ( ) ~ [ ] com.datastax.bdp.cassandra.cql3.dsequeryhandler $ operation.executewithtiming ( ) ~ [ ] com.datastax.bdp.cassandra.cql3.dsequeryhandler $ operation.executewithauditlogging ( ) ~ [ ] com.datastax.bdp.cassandra.cql3.dsequeryhandler.process ( ) ~ [ ] org.apache.cassandra.transport.messages.querymessage.execute ( ) ~ [ ] org.apache.cassandra.transport.message $ dispatcher.channelread0 ( ) [ ] org.apache.cassandra.transport.message $ dispatcher.channelread0 ( ) [ ] io.netty.channel.simplechannelinboundhandler.channelread ( ) [ ] io.netty.channel.abstractchannelhandlercontext.invokechannelread ( ) [ ] io.netty.channel.abstractchannelhandlercontext.access $ 700 ( ) [ ] io.netty.channel.abstractchannelhandlercontext $ 8.run ( ) [ ] java.util.concurrent.executors $ runnableadapter.call ( ) [ ] org.apache.cassandra.concurrent.abstracttracingawareexecutorservice $ futuretask.run ( ) [ ] org.apache.cassandra.concurrent.sepworker.run ( ) [ ] java.lang.thread.run ( ) [ ] 's full reproduction : create keyspace test replication = { 'class ' : 'simplestrategy ' , 'replication_factor':3 } durable_writes = true ; use test ; create table test ( useruid varchar , direction varchar , last_modified timestamp , value varchar , primary key ( ( useruid , direction ) , last_modified ) ) ; //insert 4 entries table insert test ( useruid , direction , last_modified , value ) values ( 'userid1 ' , 'out ' , '2013-05-13 ' , ' value1 ' ) ; insert test ( useruid , direction , last_modified , value ) values ( 'userid1 ' , 'out ' , '2013-05-13 ' , ' value2 ' ) ; insert test ( useruid , direction , last_modified , value ) values ( 'userid1 ' , 'none ' , '2013-05-13 ' , ' value3 ' ) ; insert test ( useruid , direction , last_modified , value ) values ( 'userid1 ' , 'in ' , '2013-05-13 ' , ' value4 ' ) ; first query check value table , results : select value , ttl ( value ) , last_modified test ; value | ttl ( value ) | last_modified -- -- -- -- -- + -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- value4 | null | 2013-05-13 value2 | null | 2013-05-13 value1 | null | 2013-05-13 value3 | null | 2013-05-13 ( 4 rows ) run query using clause order clause , fails error : select value , ttl ( value ) , last_modified test useruid='userid1 ' direction ( 'out ' , 'in ' ) order last_modified ; invalidrequest : code=2200 [ invalid query ] message= '' page queries order restriction partition key ; must either remove order sort client side , disable paging query '' run query without ttl ( value ) select part , also shows error : select value , last_modified test useruid='userid1 ' direction ( 'out ' , 'in ' ) order last_modified ; invalidrequest : code=2200 [ invalid query ] message= '' page queries order restriction partition key ; must either remove order sort client side , disable paging query '' message suggests jiras reason message : https : //issues.apache.org/jira/browse/cassandra-7853 select . . . . . . order regression resolution : duplicate cassandra-7514 fix version/s : none https : //issues.apache.org/jira/browse/cassandra-7514 support paging cqlsh resolution : fixed fix version/s : 2.1.1 https : //issues.apache.org/jira/browse/cassandra-6722 cross-partition ordering warning disallowed paging resolution : fixed fix version/s : 2.0.6 turn paging : cqlsh : test &gt; paging ; disabled query paging . re-run query without ttl ( value ) see results : cqlsh : test &gt; select value , last_modified test useruid='userid1 ' direction ( 'out ' , 'in ' ) order last_modified ; value | last_modified -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- value2 | 2013-05-13 value1 | 2013-05-13 value4 | 2013-05-13 ( 3 rows ) however , re-run query ttl ( value ) get nullpointerexception : cqlsh : test &gt; select value , ttl ( value ) , last_modified test useruid='userid1 ' direction ( 'out ' , 'in ' ) order last_modified ; servererror : &lt; errormessage code=0000 [ server error ] message= '' java.lang.nullpointerexception '' &gt; dtest failures run checking 2.0 n't new failures . '' 'thanks 2.0 patch looks good . ' 'committed 2.1 f587397c9c41c1a68b4e46fc16bad8d48c975e4d merged 2.2 3.0 trunk ' 'this probably get changes.txt entry . ' 'sorry forgot . \\ni pushed entry 2.1 86583af4ca0eac34725136adee3143f9b14b75b4 merged 2.2 3.0 trunk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>avoid loops array backed iterators call iter.remove ( ) noticed sampling sometimes compaction spends almost time iter.remove ( ) columnfamilystore.removedeletedstandard . turns cf object using arraybackedsortedcolumns , deletes arraylist . majority columns gcable tombstones ( n^2 ) . data structure changed copy made avoid . [ `` 've edited title 's quite compaction ( n^2 ) certain operations within partition . 's also limited specific method . best solution probably introduce special deletion iterator call remove ( ) simply sets corresponding bit 1 ; exhaust iterator commit deletes one pass . '' '/cc [ ~slebresne ] ' `` actually richard 's issue 1.2 2.0.\\n\\ni 'm sure much issue practice really compaction 2.1 w/ lazilycompactedrow precompactedrow gone . '' `` attempt implement batchremovaliterator 2.0 branch . 've also tested indeed much faster . non-removals thing . '' 'do time review [ ~rlow ] ? ' 'yes review week . ' 'we integrate 2.1 also since behaviour exhibited still compaction . 2.1 use system.arraycopy removed.nextsetbit though performance improved particularly sparse removes . ' `` good point . 've attached new patch containing code using removed.nextsetbit collections.copy . easy change 2.1 . '' `` nice backporting better approach.\\n\\ni 've uploaded tweaked version goal clean variable names ( switch loop ) 's obvious 's happening . also added use nextclearbit tandem nextsetbit 's minor tweak gives better behaviour runs adjacent removes.\\n\\ni n't properly reviewed otherwise might worth introducing cfs.removedroppedcolumns ( ) slicequeryfilter.trim ( ) `` `` thanks writing patch ! comments : \\n\\n- v3 patch batchiterator interface missing.\\n- unnecessary formatting changes import order switching.\\n- remove method throw illegalstateexception called twice element adhere iterator spec.\\n- calling commit twice remove incorrect elements . throw illegalstateexception commit called make idempotent.\\n- could add 'assert test &lt; = src ; ' copy method enforce comment . '' 'hi \\n\\nthanks comments ! \\n\\nv4 uploaded fixes comments . also added batch iterator cfs.removedroppedcolumns ( ) easy enough skipped slicequeryfilter now.\\n ' 'v5 uploaded since noticed still unnecessary formatting changes unit test . working fixing intellij settings speak ... ' '+1 2.0 v5 . 2.1 version ? ' 'patch 2.1 added.\\n ' 'only minor nit bitset initialized size rather cells.length otherwise +1 . ' 'right course . v6 attached . ' ' tested real workload sstables got 2x speedup force compaction ! also output before.\\n\\ncan someone commit patch ? ' `` 'm hesitant add 2.0 commit 2.0.12 ( soon ) gets testing time gets 2.0-line release ( 2.0.13 everything fine ) . '' `` right committed 2.0 2.1. fixed nits - added missing license header made everything conform code style made renames consistency absc iterators ( esp . 2.1 impl ) \\n\\nalso 2.1 made atomicbtreecolumns # getbatchremoveiterator ( ) throw uoe instead adding dummy implementation cf.\\n\\nthe 2.1 version three bugs fixed commit : \\n1 . maybesortcells ( ) n't called returning iterator\\n2 . end commit 'size ' updated 'sortedsize ' not\\n3 . unlike 2.0 version 2.1 version nullifying trimmed cells end array '' 'marking resolved . thanks everyone . ' 'thank !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>refactor validation logic storageservice.rebuild follow-up ticket cassandra-14319 [ ~aweisberg ] would mind take look ? super easy . +1 builds 6 branches . ' '+1 ty ' ' [ 3.0|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3035/workflows/7f55fcda-1cf0-43db-8471-ebd54be87c9e ] \\r\\n\\r\\n [ 3.11|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3034/workflows/e10a709e-97e1-4795-b4ca-7e3aac3253cd ] \\r\\n\\r\\n [ 4.0 j11|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/f2e2c30f-6d37-48cc-874a-104f34f67b50 ] \\r\\n [ 4.0 j8|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/af8cc33a-69ca-4164-800f-3b049a8ac6da ] \\r\\n\\r\\n [ 4.1 j11|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5297fd1a-f33b-4a17-b56d-d0522b65c95b ] \\r\\n [ 4.1 j8|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5626cfef-a6a7-453c-9abf-43c30678ec41 ] \\r\\n\\r\\n [ 5.0 j17|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/70cf2550-e278-426a-a63b-533c25c4eccf ] \\r\\n [ 5.0 j11|https : //app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/2a5dcdc5-17f9-4311-9aeb-60fcea5787d9 ] \\r\\n\\r\\nthese builds technically current branches . moved one check outside try-catch add logically belongs ( checks ) moved logging try catch ( outside ) .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>interrupted recovery requires manual intervention fix originally reported alexander staubo : `` kill server going initial `` row recovery '' phase , risk ending database 's corrupt fail `` negative seek '' exceptions similar . '' prashant replied : '' commit logs deleted successful recovery . still teh commit log u killed server recovering ? u restart server generate new file , compactions name intermediate files .tmp successful dump place usable files , logic required recovery fix coming . `` state u today data loss ass commit logs still exist round process recover since u haave delete intermediate file teh recovery . '' [ `` prashant 's memory seems wrong -- place gettempfilename anticompaction ( bootstrap ) code . everything else uses getnextfilename directly . '' `` writing patches checked closerename fsync ( via filechannel.force ) . audited commitlog similarly.\\n\\nadditionally bootstrap code uses potentially unsafe cfs.getfilename instead gettempfilename . sure 's actually problem . ( note self come back 0.3 ) '' '+1 ' 'integrated cassandra # 59 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/59/ ] ) \\n use gettempfilename / closerename avoid problems w/ half-written sstables.\\npatch jbellis ; reviewed eric evans \\nclean anticompaction code little.\\npatch jbellis ; reveiewed eric evans \\n ' 'committed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>clean ( make sane ) key/row cache loading logspam //start info heap size : 1935147008/1994063872 info jna found . native methods disabled . info loading settings file : /home/hermes/work/c/cass7/conf/cassandra.yaml info diskaccessmode 'auto ' determined mmap , indexaccessmode mmap info creating new commitlog segment /var/lib/cassandra/commitlog/commitlog-1291079883612.log //keycache loading * info read 0 saved key cache * * info read 0 saved key cache * * info read 0 saved key cache * * info read 0 saved key cache * * info read 0 saved key cache * //rowcache loading * info loading row cache locationinfo system * * info completed loading ( 0 ms ; 0 keys ) row cache locationinfo system * * info loading row cache hintscolumnfamily system * * info completed loading ( 0 ms ; 0 keys ) row cache hintscolumnfamily system * * info loading row cache migrations system * * info completed loading ( 0 ms ; 0 keys ) row cache migrations system * * info loading row cache schema system * * info completed loading ( 0 ms ; 0 keys ) row cache schema system * * info loading row cache indexinfo system * * info completed loading ( 0 ms ; 0 keys ) row cache indexinfo system * //the rest info could n't detect schema definitions local storage . info found table data data directories . consider using jmx call org.apache.cassandra.service.storageservice.loadschemafromyaml ( ) . info commitlog files found ; skipping replay info upgrading 0.7. purging hints . old hints snapshotted . info cassandra version : 0.7.0-rc1-snapshot info thrift api version : 19.4.0 info loading persisted ring state info starting server gossip info switching fresh memtable locationinfo commitlogcontext ( file='/var/lib/cassandra/commitlog/commitlog-1291079883612.log ' , position=700 ) info enqueuing flush memtable-locationinfo @ 1249086728 ( 227 bytes , 4 operations ) info writing memtable-locationinfo @ 1249086728 ( 227 bytes , 4 operations ) info completed flushing /var/lib/cassandra/data/system/locationinfo-e-1-data.db ( 473 bytes ) warn generated random token 109302658160365096146210744235544448283. random tokens result unbalanced ring ; see http : //wiki.apache.org/cassandra/operations info switching fresh memtable locationinfo commitlogcontext ( file='/var/lib/cassandra/commitlog/commitlog-1291079883612.log ' , position=996 ) info enqueuing flush memtable-locationinfo @ 1940835386 ( 53 bytes , 2 operations ) info writing memtable-locationinfo @ 1940835386 ( 53 bytes , 2 operations ) info completed flushing /var/lib/cassandra/data/system/locationinfo-e-2-data.db ( 301 bytes ) info load mx4j , mx4j-tools.jar classpath info binding thrift service localhost/127.0.0.1:9160 info using tframedtransport max frame size 15728640 bytes . info listening thrift clients ... logging annoying ( bit schizophrenic ) . either keycache loading logging include much info rowcache loading ( time duration , cf/ks names ) much smaller snippet . best fix would probably line : * info xx : xx : xx , xxx completed loading ( time ; keys ) row/key cache cf ks . * ... would log line per cf per saved key/row cache ( logging error ) . n't know logging `` 0 rows ( key row cache ) successfully loaded '' worth either , could swayed argument . saner logging ' 'backported 0.6 committed ' 'integrated cassandra-0.7 # 70 ( see [ https : //hudson.apache.org/hudson/job/cassandra-0.7/70/ ] ) \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>replace n't clean system.peers new ip use replace_token ( replace_node replace_address ) new node different ip , old node still system.peers existing nodes already correct peers state due token conflict ( ) replacer replacer still dead node peers table . simplest thing finish replacing removing replace_address table since either ( appear ) old node . trivial patch . ' '+1 lgtm ' 'committed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>ttl/writetime function collection column returns invalid value since query individual content collection 1.2 , ttl/writetime function collection column make sense . currently perform function collection get deserialization error like : { code } value '\\x00\\x03\\x00\\x01c\\x00\\x01b\\x00\\x01a ' ( col 'writetime ( l ) ' ) ca n't deserialized bigint : unpack requires string argument length 8 { code } looks like tries deserialize whole list/set/map content bigint writetime int ttl . simple patch attached refuse said function collection columns . ' '+1 ' 'commited thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>throw error auto_bootstrap : true bootstrapping node listed seeds obviously condition exists node bootstrap . obvious logs bootstrapping . throwing error would make obvious therefore faster correct . false seed configs . ' 'yes right fix log explicitly info . added back cassandra-746 got undone point . ' 'committed basically thing ticket . circle life complete .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>remove auto-bootstrap option already optimize auto-bootstrap no-op non-system tables . given , penalty imposed autobootstrap 30s sleep waiting gossip . feels worth avoid confusion option causes , problems n't turn . wait cluster 60s bootstrapping nodes.\\nv2 attached fixes also amends tests wait full ring_delay.\\n ' '+1 ' 'committed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>log warn large batch sizes large batches coordinator cause lot node stress . propose adding warn log entry batch sizes go beyond configurable size . give visibility operators something happen developer side . new yaml setting 5k default . { { # log warn batch size exceeding value . 5k default . } } { { # caution taken increasing size threshold lead node instability . } } { { batch_size_warn_threshold : 5k } } [ jira ] [ commented ] ( cassandra-6487 ) log warn large batch sizes \\n\\n\\n\\xa0\\xa0\\xa0 [ https : //issues.apache.org/jira/browse/cassandra-6487 ? page=com.atlassian.jira.plugin.system.issuetabpanels : comment-tabpanel &amp; focusedcommentid=15059511 # comment-15059511 ] \\n\\npatrick mcfadin commented cassandra-6487 : \\n -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \\n\\n [ ~martin.grotzke ] [ ~pragone ] sorry ! caught comments . \\n\\nvalid points single partition think warrants change way log warn error batches . original intent prevent horrible anti-patterns multi-partition batches . case single partition update impact network payload size . since need coordinator track mutations across batch partitions load much less . \\n\\ni\\ 'll make updated ticket reflect difference . \\n\\nthanks comments raising issue . \\n\\n\\n\\n\\n -- \\nthis message sent atlassian jira\\n ( v6.3.4 # 6332 ) \\n ' 'thanks [ ~pmcfadin ] clarification ! saw created cassandra-10876 also related cassandra-8825 - wasn\\'t aware ticket good see.\\n\\nwould say cost single partition single statement batch exactly `` normal '' single statement ? \\nand would compare single partition batch multiple insert statements multiple insert statements terms server load / throughput - executing multiple single partition statements batch valid approach increase throughput ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>nodetool rebuild dc lets pass invalid datacenters pass invalid datacenter nodetool rebuild , 'll get error like : { code } unable find sufficient sources streaming range ( 3074457345618258602 , -9223372036854775808 ] keyspace system_distributed { code } unfortunately , rabbit hole frustration using caps dc names pass lowercase dc name , typo dc . let 's following : # check dc name 's passed list dcs know # n't find , let 's output reasonable error , list dcs someone could put . # ideally indicate keyspaces set replicate dc n't datacenter1 datacenter2\\r\\nsee 'nodetool help ' 'nodetool help &lt; command &gt; '.\\r\\n $ \\r\\n { code } \\r\\nand for\\r\\n { quote } 3. ideally indicate keyspaces set replicate dc aren't\\r\\n { quote } \\r\\nare referring { { } } datacenter { { rebuild } } command executed one provided option ( e.g . { { no_dc } } example ) ? later invalid dc would keyspaces expected # 3 scenario ? \\r\\n\\r\\n\\xa0\\r\\n\\r\\ni interested working ticket . assign ticket ? \\xa0 '' 'hi [ ~rustyrazorblade ] \\r\\n\\r\\ni took stab implemented # 1 # 2 list attached patch implementation . waiting information # 3 . ' `` 'm sure # 3 something pointed 'm +1 . '' 'the fix looks good . think would good test verify behavior . \\r\\n [ ~vinaykumarcse ] could add test patch ? ' ' [ ~vinaykumarcse ] plan finish ? ok assign ? seems like patch abandoned . ' 'assigning . [ ~vinaykumarcse ] feel free take feel like though . ' ' believe logic moved little bit fail earlier storageservice.rebuild method anything actually executed checks are.\\r\\n\\r\\nhttps : //github.com/apache/cassandra/pull/2309 ' 'trunk [ https : //github.com/apache/cassandra/pull/2309 ] \\r\\nj11 [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2185/workflows/c0b0e974-fdb1-4410-a180-fc8890c9a7e5 ] \\r\\nj8 [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2185/workflows/4fc8308c-3f07-4b41-9ad3-3a20a547c0e9 ] \\r\\n\\r\\n4.1 [ https : //github.com/apache/cassandra/pull/2323 ] \\r\\nj11 [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2194/workflows/e6aee8b6-95e7-446f-879d-a66bb4275255 ] \\r\\nj8 [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2194/workflows/bae89aec-bbe4-4705-bcdf-fc4fdecbfe3a ] \\r\\n\\r\\n4.0 [ https : //github.com/apache/cassandra/pull/2324 ] \\r\\nj11 https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2195/workflows/4e9e290d-9ada-4aa7-a2a9-68d66b1961fe\\r\\nj8 https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2195/workflows/87cb1f9a-b71e-414b-b270-81d934390ab0\\r\\n\\r\\n3.11 [ https : //github.com/apache/cassandra/pull/2325 ] \\r\\nj8 https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2196/workflows/7d692de2-3f73-4247-b4ec-fb8f6031c681\\r\\n\\r\\n3.0 [ https : //github.com/apache/cassandra/pull/2332 ] \\r\\nj8 https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2197/workflows/831b7247-b5aa-4402-b9e9-9748602240f5 ' ' [ ~brandon.williams ] [ ~blerer ] could please review ? thank . ' '+1 . ' `` seems set { { isrebuilding } } false validating input failing { { isrebuilding.compareandset } } part { { try/catch } } introduced . thread able proceed rebuild n't mean another thread n't still proceeding.\\r\\n\\r\\ni think new { { try/catch } } removed { { isrebuilding.compareandset } } existing { { try/catch } } sets { { isrebuilding } } { { false } } actually rebuild fails . '' 'ah good catch take closer look moment . need new ticket though.\\n\\n\\nsent protonmail mobile\\n\\n\\n\\n\\\\ ' `` said moving { { isrebuilding.compareandset } } { { try/catch } } n't make sense . outside { { try/catch } } front validation complete . '' ' [ ~aweisberg ] mean like ? https : //github.com/apache/cassandra/blob/fe004912a829151a4ac3c3ec0d267fc74938953e/src/java/org/apache/cassandra/service/storageservice.java # l1309-l1328\\r\\n\\r\\nhttps : //github.com/apache/cassandra/pull/2637 ' ' prepared patches cassandra-18803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>refactor read executor response resolver , abstract read repair cassandra-10726 stuck right state { { abstractreadexecutor } } { { dataresolver } } make difficult cleanly implement . also looks like additional read repair strategies might added . goal ticket clean structure read path components make cassandra-10726 doable , additional read repair strategies possible . { { dataresolver } } { { repairresults } } field unused . ' `` thanks [ ~iamaleksey ] 'll open jira next week fix '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>make memtable flush thresholds per-cf instead global particularly useful scenario cfs high volume overwrite operations ; increasing memtable size/op count means overwrite memory ever hits disk . disk compaction much work system . , n't want give _all_ cfs high threshold memory better used elsewhere , makes commitlog replay unnecessarily painful . class org.apache.cassandra.config.cfmetadata\\n [ javac ] return new cfmetadata ( ks \\n [ javac ] ^\\n\\n ' 'updated still broken . ' `` apparantly 's broken . '' 'updated pretty print crash usage . uses commons-lang tostringbuilder instead . ' 'committed . ' 'integrated cassandra # 567 ( see [ https : //hudson.apache.org/hudson/job/cassandra/567/ ] ) \\n make memtable flush thresholds per-cf instead global . patch jon hermes reviewed brandonwilliams cassandra-1007\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>( cql3 ) missing validation queries column part pk copy-pasting original mail ( http : //mail-archives.apache.org/mod_mbox/cassandra-user/201209.mbox/ % 3c20120922185826.go6205 @ pslp2 % 3e ) : { noformat } [ cqlsh 2.2.0 | cassandra 1.1.5 | cql spec 3.0.0 | thrift protocol 19.32.0 ] use help help . cqlsh &gt; cqlsh &gt; create keyspace xpl1 strategy_class ='simplestrategy ' strategy_options : replication_factor=1 ; cqlsh &gt; use xpl1 ; cqlsh : xpl1 &gt; create table t1 ( pk varchar primary key , col1 varchar , col2 varchar ) ; cqlsh : xpl1 &gt; create index t1_c1 t1 ( col1 ) ; cqlsh : xpl1 &gt; create index t1_c2 t1 ( col2 ) ; cqlsh : xpl1 &gt; insert t1 ( pk , col1 , col2 ) values ( 'pk1 ' , 'foo1 ' , 'bar1 ' ) ; cqlsh : xpl1 &gt; insert t1 ( pk , col1 , col2 ) values ( 'pk1a ' , 'foo1 ' , 'bar1 ' ) ; cqlsh : xpl1 &gt; insert t1 ( pk , col1 , col2 ) values ( 'pk1b ' , 'foo1 ' , 'bar1 ' ) ; cqlsh : xpl1 &gt; insert t1 ( pk , col1 , col2 ) values ( 'pk1c ' , 'foo1 ' , 'bar1 ' ) ; cqlsh : xpl1 &gt; insert t1 ( pk , col1 , col2 ) values ( 'pk2 ' , 'foo2 ' , 'bar2 ' ) ; cqlsh : xpl1 &gt; insert t1 ( pk , col1 , col2 ) values ( 'pk3 ' , 'foo3 ' , 'bar3 ' ) ; cqlsh : xpl1 &gt; select * t1 col2='bar1 ' ; pk | col1 | col2 -- -- -- + -- -- -- + -- -- -- pk1b | foo1 | bar1 pk1 | foo1 | bar1 pk1a | foo1 | bar1 pk1c | foo1 | bar1 cqlsh : xpl1 &gt; select * t1 col2 ( 'bar1 ' , 'bar2 ' ) ; cqlsh : xpl1 &gt; { noformat } either make last query work refuse query returning nothing wrong . [ `` think refuse query since would require secondary indexes ca n't right . attaching patch simply refuse queries . '' '+1 ' 'forgot mark one resolved somehow .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>mutating sstable component may race entire-sstable-streaming ( zcs ) causing checksum validation failure flaky dtest : [ test_dead_sync_initiator - repair_tests.repair_test.testrepair|https : //ci-cassandra.apache.org/view/all/job/cassandra-devbranch-dtest/143/testreport/junit/dtest.repair_tests.repair_test/testrepair/test_dead_sync_initiator/ ] { code : java|title=stacktrace } unexpected error found node logs ( see stdout full details ) . errors : [ error [ ] 2020-06-03 - [ stream 6f1c3360-a54f-11ea-a808-2f23710fdc90 ] error reading sstable stream table = keyspace1.standard1 org.apache.cassandra.io.sstable.corruptsstableexception : corrupted : /home/cassandra/cassandra/cassandra-dtest/tmp/dtest-te4ty0r9/test/node3/data0/keyspace1/standard1-5f5ab140a54f11eaa8082f23710fdc90/na-2-big-statistics.db org.apache.cassandra.io.sstable.metadata.metadataserializer.maybevalidatechecksum ( ) org.apache.cassandra.io.sstable.metadata.metadataserializer.deserialize ( ) org.apache.cassandra.io.sstable.metadata.metadataserializer.deserialize ( ) org.apache.cassandra.io.sstable.metadata.metadataserializer.mutate ( ) org.apache.cassandra.db.streaming.cassandraentiresstablestreamreader.read ( ) org.apache.cassandra.db.streaming.cassandraincomingfile.read ( ) org.apache.cassandra.streaming.messages.incomingstreammessage $ 1.deserialize ( ) org.apache.cassandra.streaming.messages.incomingstreammessage $ 1.deserialize ( ) org.apache.cassandra.streaming.messages.streammessage.deserialize ( ) org.apache.cassandra.streaming.async.streaminginboundhandler $ streamdeserializingtask.run ( ) io.netty.util.concurrent.fastthreadlocalrunnable.run ( ) java.lang.thread.run ( ) caused : java.io.ioexception : checksums match /home/cassandra/cassandra/cassandra-dtest/tmp/dtest-te4ty0r9/test/node3/data0/keyspace1/standard1-5f5ab140a54f11eaa8082f23710fdc90/na-2-big-statistics.db { code } test , executes `` nodetool repair '' node1 kills node2 repair . end , node3 reports checksum validation failure sstable transferred node1 . { code : java|title=what happened } 1. repair started node1 , performs anti-compaction modifies sstable 's repairat 0 pending repair id session-id . 2. node1 creates { { componentmanifest } } contains file lengths transferred node3 . 3. node1 actually sends files node3 , node2 killed node1 starts broadcast repair-failure-message participants { { coordinatorsession # fail } } 4. node1 receives repair-failure-message fails local repair sessions { { localsessions # failsession } } triggers async background compaction . 5. node1 's background compaction mutate sstable 's repairat 0 pending repair id null via { { pendingrepairmanager # getnextrepairfinishedtask } } , in-progress repair . 6. node1 actually sends sstable node3 sstable 's stats component size different original size recorded manifest . 7. end , node3 reports checksum validation failure tries mutate sstable level `` istransient '' attribute { { cassandraentiresstablestreamreader # read } } . { code } currently , entire-sstable-streaming requires sstable components immutable , \\ { { componentmanifest } } component sizes sent sending actual files . n't problem legacy streaming stats file length n't matter . ideally great make sstable stats metadata immutable , like sstable components , n't worry special case . think 2 ways : # make stats mutation proper compaction create hard link compacting sstable components new descriptor , except stats files copied entirely . mutation applied new stats file . end , old sstable released . ensures sstable components immutable n't make special compaction tasks slower . # change stats metadata format use fixed length encoding repair info https : //app.circleci.com/pipelines/github/dcapwell/cassandra/507/workflows/fc1746fd-6568-4f2f-bc84-c7c40c94426c/jobs/2775/parallel-runs/0 ? filterby=all ' 'thanks review feedback</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>upgrade apache thrift 0.9.1 upgrades apache thrift version 0.9.0 0.9.1 include 190 bug fixes additions since previous release . also upgrade commons-lang 2.6 3.1 dependency thrift . tests passed initial patch trunk : 3f5322f update bundled python thrift cqlsh well ? ' 'hi aleksey . know design decision made use internal version packages available via pypi . would recommend thrift listed dependency setup.py rather using thrift-python-internal-only-0.7.0.zip unless modifications internal version unaware . ' `` modifications afaik . bundle way 0.9.1 instead . 's bundled start using cqlsh immediately without installing anything ( cql-internal-only ) . '' `` +1\\n\\ni 'm attaching update includes updates python lib.\\n\\nalso rowmutation change also required trunk ; including patch . '' `` patch n't apply ( want base tip cassandra-2.0.0 branch 's go go 2.0 2.0.1 ) . '' `` 've cleaned patch based cassandra-2.0 ( 2.0.0 ship sailed ) .\\n\\ni 've also updated references o.a.commons.lang lang3 . issue notimplementedexception remove ; 've replaced usage unsupportedoperationexception ( [ related commons issue|https : //issues.apache.org/jira/browse/lang-769 ] ) . '' `` nope 've bundled wrong - broke cqlsh.\\n\\nunzip old new thrift-internal-only-x.zip compare structure . '' 'fixed ; zip file follows previous one . ' '+1 committed thanks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>ability temporary set minimum maximum compaction threshold need ability temporary set minimum maximum compaction threshold . needed turn compaction bmt . added ability set/get max/min compaction threshold mbean interface\\nupdated nodeprobe new commands . ' `` let 's make compactionmanagermbean instead echoing calls ss mcm . ss starting get cluttered.\\n\\nalso \\n\\n - make variables non-static n't need two versions getter setter methods . ( ok since mcm singleton ) \\n - follow cassandra brace placement convention\\n '' 'created new minorcompactionmanagermbean removed ss wrappings updated nodeprobe\\nmade static variables regular instance variables mcm.\\n\\n ' 'committed . also renamed mcm - &gt; cm . ' 'integrated cassandra # 199 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/199/ ] ) \\n rename minorcompactionmanager - &gt; compactionmanager . patch jbellis \\nadd mbean get/set compaction thresholds . patch sammy yu ; reviewed jbellis \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>add guardrail disable group functionality group expensive troublesome large tables . guardrail disable clusters n't want users functionality . [ pr|https : //github.com/apache/cassandra/pull/1543 ] \\r\\n [ jdk8 ci|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/199/workflows/94525b46-3917-428f-bf96-f8671b1a7ac3 ] \\r\\n [ jdk11 ci|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/199/workflows/df58d46d-cf06-4f00-869c-0f1a8752f1c6 ] ' 'patch lgtm left small comments mostly `` '' vs `` get '' mbeans/interfaces ... see use `` get '' already actually incorrect ... think also fix ( 100 % cool patch accept consistency correctness ) ' 'selectstatement # prepare ( boolean forview ) called view # getselectstatement ( ) code path executed selectstatement # prepare ( clientstate state ) called . would say need propagate clientstate selectstatement # prepare ( boolean forview ) ' ' [ ~smiklosovic ] might wrong think { { view # getselectstatement ( ) } } used internal queries get query used { { create view } } query populate mv table base table contents . think query never supports { { { } group { } } } . path querying mvs seems work expected : \\r\\n { code : java } \\r\\n @ test\\r\\npublic void checkview ( ) throws throwable\\r\\n { \\r\\n setguardrail ( false ) ; \\r\\n createtable ( `` create table % ( pk int ck int v int primary key ( pk ck ) ) '' ) ; \\r\\n string viewname = createview ( `` create materialized view % `` +\\r\\n `` select * % pk null ck null `` +\\r\\n `` primary key ( ck pk ) '' ) ; \\r\\n string viewquery = `` select * `` + viewname + `` ck=0 group pk '' ; \\r\\n assertfails ( viewquery `` group functionality allowed '' ) ; \\r\\n testexcludedusers ( ( ) - &gt; viewquery ) ; \\r\\n } { code } ' 'yes think right thanks testing . please add test . ' `` added couple extra tests tweaked query n't need differentiate warning type query results force pushed.\\r\\n\\r\\nwe good go [ ~adelapena ] ? '' 'latest changes lgtm ' 'looks good need rebase fixing ( trivial ) conflicts recently added guardrails final ci round . ' 'had rebase w/clean run last friday ; went ahead one final one morning ci kicked off.\\r\\n\\r\\n [ jdk8 ci|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851 ] \\r\\n [ jdk11 ci|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/abb10877-f285-49b4-9d44-fb852fb8a584 ] ' `` failures at\\xa0 [ testmetricscleanupondrop|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851/jobs/1977 ] [ testconnectionsarerejectedwithinvalidconfig|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851/jobs/1983 ] runs n't seem related changes think results look good . tests failures n't appear [ butler|https : //butler.cassandra.apache.org/ # /ci/upstream/compare/cassandra-trunk/trunk ] though n't found tickets probably create tickets . '' `` bq . probably create tickets them.\\r\\nagree ; ran clean ( i.e . vanilla trunk ) circle run earlier today reference planning getting together focused effort get us stable green run freeze . 'll make sure failures make effort . '' `` saw testmetricscleanupondrop runs ticket opened open ? ( n't sure whether umbrella ticket anything missing ) '' `` forgot failure ca n't find ticket . failure reproduced circleci 's multiplexer 4.1 trunk although think n't seen yet jenkins . created cassandra-17658 fixing . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>replicalayout follow-up clarify new { { replicalayout } } code , separating replicaplan ( want ) { { replicalayout } } ( know cluster ) , well defined semantics ( comments rare cases semantics weird ) found fixed bugs : * { { commitpaxos } } using live nodes , needed include * writing pending transient replicas * write , hinting full nodes transient replication enabled ( since filtered { { liveonly } } , order include transient replicas { { blockfor } } ) * speculated , { { maybesendadditionalreads } } ( read repair ) would consult node speculated . also applied { { maybesendadditionalwrites } } - issue also true pre-tr . * transient- &gt; full movements mishandled consistency level upgrade * * need treat transitioning node ‘ full ’ writes , safely begin serving full data requests finished , maintain ‘ pending ’ collection else also increase consistency requirements node ’ exist . use _extensively_ also one implementation could cheap implement e.g . { { filterlazily } } { { \\\\ { none\\\\ } match } } \\r\\n * * removed { { select ( ) } } correct logic benefit abstraction . however renamed { { keep ( ) } } method { { endpoints } } re-orders contents { { select ( ) } } make clear distinction semantics.\\r\\n\\r\\nthen follow commits either fix bugs perform follow clean ups.\\r\\n\\r\\nlooking forward feedback . ' 'fyi fix { { maybesendadditionalreads } } incomplete follow monday minor . ' `` [ ~ifesdjeen ] branch linked pr wrong one 's 14705 '' ' [ ~aweisberg ] sorry right branch renamed switched ) \\r\\n\\r\\nthank patch . comments rather minor : \\r\\n\\r\\n * could rename { { liveonly } } { { live } } unless `` '' bears \\r\\n * { { alluncontactedcandidates } } could { { uncontactedcandidates } } \\r\\n * { { fortokenwrite } } { { forwrite } } since we\\ 've removed { { forrangewrite } } similarly { { fortokenwriteliveanddown } } { { fortokenwrite } } \\r\\n * avoid iterating { { replicaplan # forwrite } } checking { { istransient } } short-circuit non-transient keyspace { { cl # hastransient } } .\\r\\n * { { endpointsfor ( range|token ) } } checks range/token matched concatenated ranges . apply checks constructing layouts ? \\r\\n * know patch separate one { { issufficientreplicasforread } } { { assuresufficientreplicas } } share logic potentially makes prone modifications one place . looks like could combine two . methods naming either livereplicas alllive argument . \\r\\n * { { assuresufficientreplicas } } might need word `` live '' it\\r\\n * { { replicacount } } use new { { count } } method\\r\\n * { { replicaplans # maybemerge } } currently checking { { issufficientreplicasforread } } proceeds filtering nodes unlike token reads filter check . since place { { issufficientreplicasforread } } might opportunity consolidation.\\r\\n\\r\\nand we\\ 've discussed offline history : \\r\\n * looks like you\\ 're half step away removing logic { { consistencylevel } } great . theoretically static methods could moved { { replicaplan } } things like `` filterforquery '' could stronger invariants { { forread &lt; endpoints &lt; ? &gt; &gt; } } true { { issufficientreplicasforread/write } } especially applicable write since pass { { endpoints } } separately even though they\\ 're technically bucket { { forwrite } } .\\r\\n * we\\ 've discussed offline it\\ 's probably worth follow-up ticket collecting replicas { { replicalayouts } } general might worth cover tests . they\\ 're tested indirectly however mostly dtests . maybe flushing things especially related pending nodes ( generally hard test ) might good candidates . ' 'thanks.\\xa0 vacillated number naming decisions i\\ 'm happy accept suggestions . \\xa0i\\ 'll respond points : \\r\\n { quote } avoid iterating { { replicaplan # forwrite } } checking { { istransient } } short-circuit non-transient keyspace { { cl # hastransient } } .\\r\\n { quote } \\r\\ni\\ 'm sure - would race condition keyspace . \\xa0very rare admittedly worth avoiding . perhaps could cache value inside endpoints though introduce { { hastransient ( ) } } \\xa0method ? \\xa0 ideally later cache { { replicalayout } } \\xa0 ( perhaps even { { replicaplan } } ) point would short-circuit work queries.\\r\\n { quote } { { endpointsfor ( range|token ) } } checks range/token matched concatenated ranges . apply checks constructing layouts ? \\r\\n { quote } \\r\\nnot quite sure mean ? \\xa0that abstractbounds fully cover ? \\xa0i wonder there\\ 's actually potential race conditions code generate ranges query actually query might fail check . \\xa0but race condition inherent way range ownerships anyway lot point killing queries today reckon . \\xa0this obviously doesn\\'t apply token since they\\xa0are unit range ownerships move.\\r\\n { quote } know patch separate one { { issufficientreplicasforread } } { { assuresufficientreplicas } } share logic potentially makes prone modifications one place . looks like could combine two . methods naming either livereplicas alllive argument.\\r\\n { quote } \\r\\ni agree present would bit painful modify { { assuresufficientreplicas } } \\xa0throws exceptions utilise computations . \\xa0i guess could\\xa0have shared method accepts boolean indicating throw like ? \\xa0my intention would cleaned later jira might address overall management \\'sufficiency\\ ' - dotted resolvers write handlers cl read-repair ... \\r\\n { quote } { { replicacount } } use new { { count } } method\\r\\n { quote } \\r\\ni don\\'t follow ? \\xa0do mean { { replicacollection.count ( ) } } - returns int need two ints ... \\r\\n { quote } { { replicaplans # maybemerge } } currently checking { { issufficientreplicasforread } } proceeds filtering nodes unlike token reads filter check . since place { { issufficientreplicasforread } } might opportunity consolidation.\\r\\n { quote } \\r\\nthese checks little\\xa0different - we\\ 're testing see two\\xa0sufficient plans merged one quick check confirm sufficiency well sufficient . \\xa0we aren\\'t performing later check assert sufficiency aren\\'t use two separate\\xa0plans . \\xa0that said could make the\\xa0structure code similar checking construction - assume point this\\xa0was originally avoid generating garbage necessary.\\r\\n { quote } looks like you\\ 're half step away removing logic { { consistencylevel } } great . theoretically static methods could moved { { replicaplan } } things like `` filterforquery '' could stronger invariants { { forread &lt; endpoints &lt; ? &gt; &gt; } } true { { issufficientreplicasforread/write } } especially applicable write since pass { { endpoints } } separately even though they\\ 're technically bucket { { forwrite } } .\\r\\n { quote } \\r\\nagreed entirely cl ideally return mostly enum . \\xa0since its\\xa0logic tracking sufficiency ( either is/assert checks resolvers write handlers etc . ) address move logic cl . \\xa0i also already follow-up bug fix moves { { filterforquery } } \\xa0into { { replicaplans } } \\r\\n { quote } we\\ 've discussed offline it\\ 's probably worth follow-up ticket collecting replicas { { replicalayouts } } general might worth cover tests . they\\ 're tested indirectly however mostly dtests . maybe flushing things especially related pending nodes ( generally hard test ) might good candidates there.\\r\\n { quote } \\r\\ni try rustle unit\\xa0tests sure though may right follow-up ticket best get committed asap . ' `` bq . 'm sure - would race condition keyspace.\\r\\n\\r\\ntrue n't thought . probably try make replication factor immutable time query attached replica plan point future.\\r\\n\\r\\nbq . quite sure mean ? abstractbounds fully cover ? \\r\\n\\r\\ni mostly meant something like [ this|https : //github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/locator/endpointsforrange.java # l94-l96 ] . maybe 's actually good fail queries collected half replicas one token metadata one ? sure.\\r\\n\\r\\nbq . guess could shared method accepts boolean indicating throw like ? \\r\\n\\r\\nwe could also leave later really . 's rather minor 'm worried right thought 's worth persisting it.\\r\\n\\r\\nbq . replicacount use new count method\\r\\n\\r\\nsorry thinking something different let 's leave is.\\r\\n\\r\\nbq . quick check confirm sufficiency well sufficient . \\r\\n\\r\\nright agreed . maybe used boolean check threw returned { { false } } could still consolidate also huge win . '' `` { quote } maybe 's actually good fail queries collected half replicas one token metadata one ? sure.\\r\\n { quote } \\r\\nyeah sure either since 're already exposed ( generally ) range ownership races 's _probably_\\xa0better hold fix across board imo least surprise users . '' 'thank changes &amp; discussions . \\r\\n\\r\\n+1 lgtm . ' 'these two outdated need click view outdated https : //github.com/apache/cassandra/pull/265 # pullrequestreview-153925151 https : //github.com/apache/cassandra/pull/265 # pullrequestreview-153914279 seem responded to.\\r\\n\\r\\nthe first one really matters think understanding multiple plans issue . ' `` loving github comments experience moment . \\xa0i 've responded comments could find . '' `` could remove one code comment maybe clean refer one replicaplan something todos need either done removed jiraed . 's good policy todos right ? \\r\\n\\r\\n+1 that.\\r\\n\\r\\n '' `` personal view todos much useful left inline context somebody else next comes touch surrounding code\\xa0- whether intended jira or\\xa0another related one . \\xa0they 're also bit specific jira 're guidance consider when\\xa0shaping next solution.\\r\\n\\r\\nthey 're like documentation document inadequacy things next author watch suggestion ( time permits ) \\xa0they\\xa0resolve time . \\xa0\\r\\n\\r\\nthere multiple\\xa0jira filed already code paths might hopefully\\xa0encompass refactoring / fixing todos.\\r\\n { quote } could remove one code comment maybe clean refer one replicaplan\\r\\n { quote } \\r\\nwhich one ? \\xa0the one saw already missing code hence comment being\\xa0out date pr ( missing ) github ui . '' `` [ talking about|https : //github.com/apache/cassandra/pull/265/files # diff-0246c72855070863c2fdbee6d97f494dr63 ] . could n't refer cl parameter replicaplan ? remove todo ? '' `` ah thanks . \\xa0i must gone amiss somehow summary\\xa0github ui . \\xa0it look like use provided replica plan safely 've removed cl parameters . '' `` 've pushed squashed rebased version [ here|https : //github.com/belliottsmith/cassandra/tree/14705-rebase ] however 'm considering last modification - alex 's recent patches mean can\\xa0perhaps get rid replicaplan.shared concept . \\xa0i may try see turns . \\xa0previously needed construct readrepair upfront could construct demand . \\xa0it might useful abstraction anyway \\xa0since least make clear modified . \\xa0but would actually localise scope who\\xa0can see modifications single class . '' '+1 unless want changes part . ' 'this got merged [ 047bcd7ad171d6a4aa89128c5e6c6ed5f012b1c0|https : //github.com/apache/cassandra/commit/047bcd7ad171d6a4aa89128c5e6c6ed5f012b1c0 ] ? \\r\\n\\r\\nis issue ready resolve ? ' 'yes thanks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>scrub could lose increments replicate loss scrub 'repair ' corrupted row , skip . node , row contains sub-count id , lost forever since source truth 's current id . thus renew node id happens avoid ( unlike cleanup ) . [ `` attached patch 0.8.\\n\\nthe patch also add new startup option renew node id startup . could useful someone lose one 's sstable ( bad disk instance ) n't want fully decommission node.\\n\\nthis could arguably splitted another ticket though . '' 'what `` renewing node id ? '' ' `` 's picking new uuid current node use new counter increment.\\n\\nthe problem given node store deltas 's current nodeid ( avoid synchronized read-before-write 'm starting wonder smartest ever ) . anyway scrub skips row may skip deltas . let 's say first increments coming row 'first distinguished replica ' . far still kind good read ( cl &gt; one ) result coming 'version ' 's sub-count smaller one replica us sub-count replica return correct value.\\n\\nhowever soon acknowledge new increments row start inserting new deltas intrinsically date . result definitive undercount.\\n\\nthe goal renewing node id make sure second part never happen ( renew add new deltas ' anymore ) .\\n\\nanyway 've plugged brain patch n't really works never repaired nodes 's inconsistent value.\\n\\nso clue actually fix . '' `` may best short fix make scrub * * skipping row counter column families ( though cassandra-2614 would change 'never ever skipping row ' ) throw runtimeexception . '' `` bq . make scrub skip rows counter column families\\n\\n+1\\n\\nbq . cassandra-2614 would change 'never ever skipping row'\\n\\nonly actually counter column_metadata right ? '' 'attaching patch simply re-throw exception instead skipping row counter column families.\\n\\nbq . actually counter column_metadata right ? \\n\\nright . ' '+1\\n\\ncan add link issue `` dangerous '' comment ? ' 'committed suggested comment update . ' 'integrated cassandra-0.8 # 170 ( see [ https : //builds.apache.org/job/cassandra-0.8/170/ ] ) \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>hinted handoff rows never get deleted list : `` hints delivered , hinted keys deleted hinted cf , application cf . '' prashant verified bug ca n't fixed deletes fully working . note : fix , see w/o compromising immediate-gc hinted cf keys . since purely local data subject read repair find way gc immediately post-delete . ) \\n\\nhow look ? '' 'integrated cassandra # 57 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/57/ ] ) \\n make sendmessage return true ack recipient.\\npatch jun rao ; reviewed jbellis \\n ' 'created cassandra-128 improvments beyond scope 0.3 ' `` looked new patch . comments.\\n\\n1 . move comments sendmessage beginning class.\\n\\n2 . compilation error minorcompactionmanager removed hintedhandoff class.\\n\\n3 . new code deletes column using timestamp column deleted . fundamental question . column non-delete entry deleted entry timestamp one wins ? n't think rely ordering insertion/deletion . insertion deletion end different sstables . compaction filestruct sorted keys . therefore columns different sstables key come arbitrary order . one solution modify cf.addcolumn deletion always wins column timestamp . sure implications though.\\n '' `` 4. 's probably worthwhile make intervalinmins_ hhm configurable . '' 'patch make tombstones higher precedence non- timestamp column ( supercolumn consistent w/ c cf ) ' `` ( incorporated changes comments ( 1 ) ( 2 ) patchset bothering resubmitting unless really want 'em ) '' 'noted comment ( 4 ) cassandra-128 ' 'comments new patch.\\n1 . cfstore.removedeleted ( ) add comments explain resolve conflicts among cf sc c timestamps . time goes likely forget decisions made.\\n\\nother patch looks fine me.\\n ' 'done committed . ' 'integrated cassandra # 63 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/63/ ] ) \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>fix low secondary index performance performing index search + value filtering large index row ( ~100k keys per index value ) chunks ( size 512-1024 keys ) search time 8-12 seconds , low . profiling got picture : 60 % search time calculating md5 hash messagedigester ( cause rundompartitioner ) . 33 % search time ( half md5 hash calculating time ) double calculating md5 comparing two row keys rotating index row startkey ( performing search query next chunk ) . see several performance improvements : 1 ) use good algorithm search startkey sorted collection , faster iteration keys . solution first place simple , need local code changes solve problem ( increase search multiple times ) . 2 ) n't calculate md5 hash startkey every time . 's optimal compute ( search twice faster ) . also need local code changes . 3 ) think something faster md5 hashing ( like tigerrandompartitioner tiger/128 hash ) . need research maybe research done . 4 ) n't use tokens ( md5 hash randompartitioner ) comparing sorting keys index rows . index rows , keys stored compared simple byte comparator . solution requires huge code changes . 'm going start first solution . next improvements done next tickets . \\n * /cassandra/trunk/changes.txt\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/abstractcolumncontainer.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/arraybackedsortedcolumns.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/collationcontroller.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/columnfamilystore.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/isortedcolumns.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/memtable.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/rowiteratorfactory.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/threadsafesortedcolumns.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/treemapbackedsortedcolumns.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/filter/ifilter.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/filter/namesqueryfilter.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/filter/queryfilter.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/filter/slicequeryfilter.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/index/keys/keyssearcher.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/service/rowrepairresolver.java\\n * /cassandra/trunk/test/unit/org/apache/cassandra/db/arraybackedsortedcolumnstest.java\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>handle skipping bad rows lazilycompacted path 's easy handle skipping bad rows compation precompacted ( merged-in-memory ) path done long time . harder lazilycompacted path since already started writing data discover source rows deserialized . adds mark/reset sstablewriter compaction skip back beginning circumstances . corner case bad row extends past good data truncate end good data . ' '+1 ' 'committed ' `` would possible differentiate read errors ( recoverable ) write errors ( non-recoverable ) ? reason ca n't write destination wo n't drop data rest inputs ? '' 'it\\ 's difficult happening `` writer.append ( row ) '' lazy path . open suggestions . ' 'if wrapped compaction read path ( mostly inside iterators ) known ( runtime ? ) exception could differentiate way . imo wait fix file format always discard corrupted data block level . ' `` bq . wrapped compaction read path ( mostly inside iterators ) known ( runtime ? ) exception could differentiate way\\n\\nstarted ( still might ) 'm going revert . feels like silently dropping data floor wrong thing . let 's make separate utility expunge corrupt rows individual sstables if/when need . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>optimize disk seek using min/max column name meta data limit clause used working example sensor data table ( timeseries ) face use case c * optimize read disk . { code } cqlsh : test &gt; create table test ( id int , col int , val text , primary key ( id , col ) ) clustering order ( col desc ) ; cqlsh : test &gt; insert test ( id , col , val ) values ( 1 , 10 , '10 ' ) ; ... &gt; nodetool flush test test ... cqlsh : test &gt; insert test ( id , col , val ) values ( 1 , 20 , '20 ' ) ; ... &gt; nodetool flush test test ... cqlsh : test &gt; insert test ( id , col , val ) values ( 1 , 30 , '30 ' ) ; ... &gt; nodetool flush test test { code } , activate request tracing : { code } cqlsh : test &gt; select * test id=1 limit 1 ; activity | timestamp | source | source_elapsed -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- -+ -- -- -- -- -- -- -- -- execute_cql3_query | | 127.0.0.1 | 0 parsing select * test id=1 limit 1 ; | | 127.0.0.1 | 74 preparing statement | | 127.0.0.1 | 253 executing single-partition query test | | 127.0.0.1 | 930 acquiring sstable references | | 127.0.0.1 | 943 merging memtable tombstones | | 127.0.0.1 | 1032 key cache hit sstable 3 | | 127.0.0.1 | 1160 seeking partition beginning data file | | 127.0.0.1 | 1173 key cache hit sstable 2 | | 127.0.0.1 | 1889 seeking partition beginning data file | | 127.0.0.1 | 1901 key cache hit sstable 1 | | 127.0.0.1 | 2373 seeking partition beginning data file | | 127.0.0.1 | 2384 skipped 0/3 non-slice-intersecting sstables , included 0 due tombstones | | 127.0.0.1 | 2768 merging data memtables 3 sstables | | 127.0.0.1 | 2784 read 2 live 0 tombstoned cells | | 127.0.0.1 | 2976 request complete | | 127.0.0.1 | 3551 { code } clearly see c * hits 3 sstables disk instead one , although min/max column meta data decide sstable contains recent data . funny enough , add clause clustering column select , time c * optimizes read path : { code } cqlsh : test &gt; select * test id=1 col &gt; 25 limit 1 ; activity | timestamp | source | source_elapsed -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- -+ -- -- -- -- -- -- -- -- execute_cql3_query | | 127.0.0.1 | 0 parsing select * test id=1 col &gt; 25 limit 1 ; | | 127.0.0.1 | 60 preparing statement | | 127.0.0.1 | 277 executing single-partition query test | | 127.0.0.1 | 961 acquiring sstable references | | 127.0.0.1 | 971 merging memtable tombstones | | 127.0.0.1 | 1020 key cache hit sstable 3 | | 127.0.0.1 | 1108 seeking partition beginning data file | | 127.0.0.1 | 1117 skipped 2/3 non-slice-intersecting sstables , included 0 due tombstones | | 127.0.0.1 | 1611 merging data memtables 1 sstables | | 127.0.0.1 | 1624 read 1 live 0 tombstoned cells | | 127.0.0.1 | 1700 request complete | | 127.0.0.1 | 2140 { code } validation stats compaction header ca n't simply keep reading till end file . however write toc position component . possible would require changes { { metadataserializer.deserialize ( ) } } signature { { imetadatacomponentserializer.deserialize ( ) } } receive total size work stuff read end . guess go it.\\n\\nbq . personally would prefer modify behaviour mergeiterator keep one simple thing approach charm.\\n\\nthe changes mergeiterator mostly candidate really minimal actual algorithm unchanged . however less invasive approach mind 'm eager hear it.\\n\\nbq . empty row work correctly lower bound . sort needed respect tombstone bounds also included test ( specifically one adds row flushes deletes row flushes checks resurfaces -- believe would break current code ) . \\n\\nthanks 'll add test.\\n\\nbq . use rangetombstonebound deletiontime.live deletion time bound obtained rangetombstone.bound.inclusiveopen right thing directions.\\n\\ni 'm sure mean test fix ? 'm sure 'll work write test though.\\n\\nbq . imergeiterator.lowerbound cryptic rename iteratorwithlowerbound explicit purpose.\\n\\nok\\n\\nbq . choice set rowindexlowerbound partitionleveldeletion ( ) appears arbitrary fragile . reason separately globallowerbound ? fact two separate bounds instead one set precise information available construction time ? \\n\\nthe global lower bound free since available metadata . index lower bound accurate requires seeking index file . calling { { super.partitionleveldeletion ( ) } } also involves initializing iterator accessing data file ( abstractsstableiterator constructor ) . decided use accurate bound really access index anyway partitionleveldeletion ( ) called tombstones . see [ comment|https : //issues.apache.org/jira/browse/cassandra-8180 ? focusedcommentid=14388301 &amp; page=com.atlassian.jira.plugin.system.issuetabpanels : comment-tabpanel # comment-14388301 ] above.\\n\\ni hope resume next days . '' `` bq . example incomplete prefix gap tests ? \\n\\ntombstones . { { delete pk = ? ck1 = ? } } table key { { ( pk ck1 ck2 ) } } generate one.\\n\\nbq . n't understand things like shouldinclude ( ) clusteringindexnamesfilter clusteringindexslicefilter work.\\n\\nif look callsites method see work presence tombstones . one solution use { { min/maxclusteringvalues } } case.\\n\\nbq . \\\\ [ metadataserializer.deserialize ( ) \\\\ ] receive total size work stuff read end.\\n\\nno need set flag { { version } } tell whether information present.\\n\\nbq . 'm sure mean \\\\ [ use rangetombstonebound\\\\ ] test fix ? \\n\\nthis fix . instead empty row lower bound { { rangetombstonebound } } described.\\n\\nbq . global lower bound free since available metadata . index lower bound accurate requires seeking index file.\\n\\nin way use class time { { lowerbound ( ) } } called already done ( { { unfilteredrowmergeiterator.create } } ) possibly unnecessarily ( { { mergeiterator.onetoone } } used ) . would move finding bound { { lowerbound ( ) } } n't think 's even necessary save bound -- retrieve method wo n't called once.\\n '' 'bq . tombstones . { { delete pk = ? ck1 = ? } } table key { { ( pk ck1 ck2 ) } } generate one.\\n\\ni add test thanks.\\n\\nbq . one solution use min/maxclusteringvalues case.\\n\\nso maybe could simply return null lower bound presence tombstones much compromise ? \\n\\nbq . need set flag version tell whether information present.\\n\\ndoesn\\'t indicate different sstable version ( `` la '' `` '' etc ) ? \\n\\nbq . fix . instead empty row lower bound rangetombstonebound described.\\n\\nthanks.\\n\\nbq . would move finding bound lowerbound ( ) don\\'t think it\\ 's even necessary save bound\\n\\nok ' `` pushed [ commit|https : //github.com/stef1927/cassandra/commit/d5cfc6fd56d50eda5d9c510591bae1d66e17ec59 ] n't use lower bound presence tombstones { { deletetest } } passing . ci still pending however . would like point { { iter.partitionleveldeletion ( ) } } currently called sstables { { querymemtableanddiskinternal ( ) } } therefore presence tombstones access sstable anyway.\\n\\nbq . tombstones . delete pk = ? ck1 = ? table key ( pk ck1 ck2 ) generate one.\\n\\nthis case existed already { { deletetest.testdeletewithrangeandtwoclusteringcolumns ( ) } } . fail though clustering comparator compares prefix values first last works fine incomplete prefixes.\\n\\nbq . empty row n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>scrub resulting `` bloom filter claims longer entire row size '' error scrub node upgraded 0.7.1 ( previously 0.6.8 ) 0.7.3. getting error multiple times : { code } warn [ ] 2011-03-08 compactionmanager.java ( line 625 ) row unreadable ; skipping next warn [ ] 2011-03-08 compactionmanager.java ( line 599 ) non-fatal error reading row ( stacktrace follows ) java.io.ioerror : java.io.eofexception : bloom filter claims longer entire row size org.apache.cassandra.io.sstable.sstableidentityiterator. &lt; init &gt; ( ) org.apache.cassandra.db.compactionmanager.doscrub ( ) org.apache.cassandra.db.compactionmanager.access $ 600 ( ) org.apache.cassandra.db.compactionmanager $ 3.call ( ) java.util.concurrent.futuretask $ sync.innerrun ( ) java.util.concurrent.futuretask.run ( ) java.util.concurrent.threadpoolexecutor $ worker.runtask ( ) java.util.concurrent.threadpoolexecutor $ worker.run ( ) java.lang.thread.run ( ) caused : java.io.eofexception : bloom filter claims longer entire row size org.apache.cassandra.io.sstable.indexhelper.defreezebloomfilter ( ) org.apache.cassandra.io.sstable.sstableidentityiterator. &lt; init &gt; ( ) ... 8 warn [ ] 2011-03-08 compactionmanager.java ( line 625 ) row unreadable ; skipping next info [ ] 2011-03-08 compactionmanager.java ( line 637 ) scrub sstablereader ( path='/cassandra/data/reddit/hide-f-671-data.db ' ) complete : 254709 rows new sstable warn [ ] 2011-03-08 compactionmanager.java ( line 639 ) unable recover 1630 skipped . attempt manual recovery pre-scrub snapshot . also run nodetool repair transfer data healthy replica , { code } keys must written ascending order.\\n org.apache.cassandra.io.sstable.sstablewriter.beforeappend ( ) \\n org.apache.cassandra.io.sstable.sstablewriter.append ( ) \\n org.apache.cassandra.db.compactionmanager.doscrub ( ) \\n org.apache.cassandra.db.compactionmanager.access $ 600 ( ) \\n org.apache.cassandra.db.compactionmanager $ 3.call ( ) \\n java.util.concurrent.futuretask $ sync.innerrun ( ) \\n java.util.concurrent.futuretask.run ( ) \\n java.util.concurrent.threadpoolexecutor.runworker ( ) \\n java.util.concurrent.threadpoolexecutor $ worker.run ( ) \\n java.lang.thread.run ( ) \\n { code } \\n\\n ' `` disregard . getting thing unpatched 0.7.3. 'll create separate bug report . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>upgrade murmurhash version 3 murmurhash version 3 finalized june 3. provides enormous speedup increased robustness version 2 , implemented cassandra . information : http : //code.google.com/p/smhasher/ reference implementation : http : //code.google.com/p/smhasher/source/browse/trunk/murmurhash3.cpp ? spec=svn136 &amp; r=136 already done work port ( public domain ) reference implementation java murmurhash class updated bloomfilter class use new implementation : https : //github.com/lindauer/cassandra/commit/cea6068a4a3e5d7d9509335394f9ef3350d37e93 apart faster hash time , new version requires one call hash ( ) rather 2 , since returns 128 bits hash instead 64 . 0.9224838948146583\\n { noformat } \\n\\ni.e . 8 % improvement average.\\n\\n ' 'have repeated benchmark heapbytebuffer input rather directbytebuffer ( using bytebuffer allocate ( ) rather allocatedirect ( ) ) performance improvement seems almost vanish.\\n\\nthe input murmurhash within cassandra seems heapbytebuffer ( based adding println existing murmurhash2 hash64 ( ) method ) inlining probably benefit practice . ' `` vijay would mind picking addressing stu 's feedback brian 's patchset ? '' 'will do\\n\\nsent iphone\\n\\n\\n ' 'thanks vijay . ' 'attached refactor includes fixes per suggestions . added factory make adding newer hashesh easier left legacy alone fairly trivial cleaner want refactor little . let know thanks ! tests passed long test shows significant improvement thanks brian ! ' 'updating patch old one missed new files created . ' 'this patch murmur partitioner also faster bloom filter ? ' 'the latter . cassandra-3772 open try murmur-based partitioner . ' 'vijay please rebase ? ' 'done ! unit tests functional tests works fine . ' '+1 following nit - lazilycompactedrowtest modifications redundant additions patch adds `` clear ( ) '' method filter class fixes problem longbloomfiltertest . ' 'committed cleaned white spaces added license headers new files .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>detecting data resurrection read seen several bugs deleted data gets resurrected . try see detect read path possibly fix . examples brought back data replica lost sstable startup caused one replica lose tombstone data . tombstone past gc grace means could resurrect data . detect invalid states looking replicas . running incremental repair , cassandra keep repaired non-repaired data separate . every-time incremental repair run , move data non-repaired repaired . repaired data across replicas 100 % consistent . example detect mitigate issue cases . say 3 machines , , b c. machines data split b/w repaired non-repaired . 1. machine due bug bring backs data d. data repaired dataset . replicas data tombstone 2. read data comes application involve replicas b. data read involves data repaired state . respond back co-ordinator data b send nothing tombstone past gc grace . cause digest mismatch . 3. patch kick digest mismatch . co-ordinator ask replicas send back data like today patch , replicas respond back data returning coming repaired vs non-repaired . data coming repaired match , know something wrong ! ! time , co-ordinator determine replica resurrected data replica b lost data . still log error logs saying hit invalid state . 4. besides log , take even correct response query . logging invalid state , ask replica b ( also c alive ) send back data including gcable tombstones . machine returns tombstone data , know return data . way avoid returning data deleted . challenges 1. data moved non-repaired repaired , could race . look incremental repairs promoted things replica avoid false positives . 2. third replica live replica tombstone , wont able break tie deciding whether data actually deleted resurrected . 3. read latest data , wont able detect read served non-repaired data . 4. replica lose tombstone last replica compact tombstone , wont able decide data coming back rest replicas lost data . still detect something wrong . 5. wont affect 99.9 % read queries extra work digest mismatch . 6. cl.one reads able detect . repair_tests/incremental_repair_test.py -- -\\n @ @ -918 3 +931 196 @ @ def test_subrange ( self ) : \\n self.assertrepairedandunrepaired ( node1 \\'ks\\ ' ) \\n self.assertrepairedandunrepaired ( node2 \\'ks\\ ' ) \\n self.assertrepairedandunrepaired ( node3 \\'ks\\ ' ) \\n +\\n + @ since ( \\ ' 4.0\\ ' ) \\n + def test_repaired_tracking_with_partition_deletes ( self ) : \\n + `` '' '' \\n + check tracking repaired data status following digest mismatch \\n + repaired data mismatches marked unconfirmed may skip sstables\\n + partition delete encountered.\\n + @ jira_ticket cassandra-14145\\n + `` '' '' \\n + session node1 node2 = self.setup_for_repaired_data_tracking ( ) \\n + stmt = simplestatement ( `` insert ks.tbl ( k c v ) values ( % % % ) '' ) \\n + stmt.consistency_level = consistencylevel.all\\n + range ( 10 ) : \\n + session.execute ( stmt ( ) ) \\n +\\n + node self.cluster.nodelist ( ) : \\n + node.flush ( ) \\n + self.assertnorepairedsstables ( node \\'ks\\ ' ) \\n +\\n + node1.repair ( options= [ \\'ks\\ ) \\n + node2.stop ( wait_other_notice=true ) \\n +\\n + session.execute ( `` delete ks.tbl k = 5 '' ) \\n +\\n + node1.flush ( ) \\n + node2.start ( wait_other_notice=true ) \\n +\\n + # expect unconfirmed inconsistencies partition deletes cause sstables skipped\\n + jolokiaagent ( node1 ) jmx : \\n + self.query_and_check_repaired_mismatches ( jmx session `` select * ks.tbl k = 5 '' \\n + expect_unconfirmed_inconsistencies=true ) \\n + self.query_and_check_repaired_mismatches ( jmx session `` select * ks.tbl k = 5 c = 5 '' \\n + e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>remove databasedescriptor dependency segmentedfile several configurable parameters pulled { { databasedescriptor } } { { segmentedfile } } subclasses . mostly edits comments unused imports restrictive access modifiers rar . 've also fixed resource management problems unit tests two new { { open ( ) } } methods case exceptions . 've rebased resulted couple conflicts especially { { commitlogreader } } code moved around . \\n\\nif 're +1 changes ci results ok 'm also + 1 commit : \\n\\n| [ patch|https : //github.com/stef1927/cassandra/commits/11580 ] | [ testall|http : //cassci.datastax.com/view/dev/view/stef1927/job/stef1927-11580-testall/ ] | [ dtest|http : //cassci.datastax.com/view/dev/view/stef1927/job/stef1927-11580-dtest/ ] |\\n\\nin terms better name { { filehandle } } 'm also short suggestions factory rar basically one owns resources term { { handle } } probably good { { factory } } given created builder tend think { { filehandle } } probably better something term factory . feel free start discussion irc . better name want back holiday commit { { filehandle } } .\\n '' 'change tests looks good thanks ! \\nthe patch needs rebasing code freeze . ' 'committed { { b4133f38d5ef5fc50047eb4a31307ac97c5b72ee } } thanks ! ' ' [ ~yukim ] introduced new assertion sstable makes tiny bit harder instantiate sstable tools previously optimisation strategy required ( least assertion ) . think makes sense add sort default no-op optimiser would work tools ? assuming always overridden { { databasedescriptor } } ' `` [ ~ifesdjeen ] may able set default { { diskoptimizationstrategy } } ( 'ssd ' cassandra.yaml ) { { databasedescriptor } } think configs set properly open sstable . right need { { databasedescriptor.toolinitialization } } anyway reads config { { cassandra.yaml } } file set that.\\n\\nlater want move sstable related config dd put sstable config something control configuring opening sstable programatically.\\n '' `` unfortunately use { { databasedescriptor.toolinitialization } } 'd require config file present ( n't since 's client-only tool ) . 'll wait patch moving sstable initialization code sstable improve . thank ! '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>indexsummarymanagertest.testcompactionrace times periodically issue amount time test takes highly variable biased towards creating condition test retry compaction attempting . solution decrease bias https : //github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/columnfamilystore.java # l2522 check every millisecond instead every 100 milliseconds . 're author test 's flaky w/regards passing since introduction . thoughts long-testing ? '' `` actually [ ~tjake ] author ; committed along fix ( es ) . pov * * 1 ) move long test ; 2 ) make robust\\n\\ni 'm pretty sure performing actual major compaction unnecessary largely problem comes ( indeterminate sleeping wait cessation active tasks ) . need instances reader change try marking files redistribution probably direct tight loop code sections spinning direct contention . increases likelihood failure scenario also ensures n't encounter stop promptly ( spinning fixed interval calling day n't failed ) . '' `` bq . actually jake luciani author ; committed along fix ( es ) \\nnow mention 'm pretty sure 've already gone jira comments another ticket . '' 're-committed [ ~aweisberg ] fix 4362e71 ' 'this still happening pretty much hard failing . ' `` ran six builds cassci n't fail them.\\nhttp : //cassci.datastax.com/view/dev/view/aweisberg/job/aweisberg-c-9271-2-testall/\\n\\nhttps : //github.com/apache/cassandra/compare/trunk ... aweisberg : c-9271-2\\nhttps : //github.com/apache/cassandra/compare/trunk ... aweisberg : c-9271-2.diff '' ' stopped 106 loop runs test - lgtm : ) ' `` may fix behaviour unfortunately -1 using thread.yield ( ) . depends priority thread generally good practice . could lead spinning burning full cpu flat compactions acquiesce . n't like making application behaviour even slightly less good order fix problematic test.\\n\\nas stated possible modify test simply mark unmark compacting directly rather actually perform major compaction . '' `` ok 'll back thread.yield ( ) . '' ' propose remove test replace single line . goal test ensure could mark compacting incorrect instance sstable . test checks . ' 'ok . +1 replacing simpler assertion invariant maintained tracker .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>make serializingcache memory pluggable serializing cache uses native malloc free making fm pluggable , users choice gcc malloc , tcmalloc jemalloc needed . initial tests shows less fragmentation jemalloc issue ( tcmalloc jemalloc ) kind single threaded ( at-least crash test otherwise ) . add static allocator memory.\\n\\nmore refactoring cleaner ? move allocation outside memory replace constructor ( long reference long bytes ) ; add allocate ( long bytes ) allocaterefcounted ( long bytes ) factory allocator . refcountedmemory would need wrap memory instead subclassing.\\n\\ni also suggest adding commented-out example cassandra.yaml cassandra-env.sh illustrate enable brave enough try . ( go 1.3 plenty time test . ) '' `` hi jonathan \\n\\n { quote } \\nwe n't need jni\\nld_preload makes things segfault ld_library_path works fine\\n\\nright ? \\n { quote } \\nwe dont need additional jni use jna load library : ) .\\n\\n { quote } \\nmore refactoring cleaner ? \\n { quote } \\nthe problem free called allocator hence attached patch doesnt refactor : ) \\n\\nrest done thanks ! '' 'good point free.\\n\\nnits : \\n\\n- cassandra.yaml comments actual memory_allocator option\\n- rename iallocator follow convention\\n- instance capitalized\\n\\nrest lgtm ship ! ' `` committed nit 's fixed thanks ! '' ' little bit late better ever never ... \\n\\n '' jemalloc issue ( tcmalloc jemalloc ) kind single threaded ( at-least crash test otherwise ) . `` \\n\\njemalloc must configured : \\n { code } \\n -- disable-lazy-lock\\n disable code wraps pthread_create ( ) detect application\\n switches single-threaded multi-threaded mode avoid\\n mutex locking/unlocking operations single-threaded mode . in\\n practice feature usually little impact performance unless\\n thread-specific caching disabled.\\n { code } \\n\\njemalloc deafult wraps pthread api tries detect application\\n switches single-threaded multi-threaded mode . trick work inside jvm course.\\n\\n ' 'thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>examine shortening path length post-5202 cassandra-5202 discussion : { quote } give ? could clean redundancy little moving id directory name ? e.g. , ks/cf-uuid/version-generation-component.db 'm worried path length , limited windows . edit : give specific example , ks foo table bar /var/lib/cassandra/flush/foo/bar-2fbb89709a6911e3b7dc4d7d4e3ca4b4/foo-bar-ka-1-data.db 'm proposing /var/lib/cassandra/flush/foo/bar-2fbb89709a6911e3b7dc4d7d4e3ca4b4/ka-1-data.db { quote } feels pretty error prone . keeping keyspace/table name ( sake making easy mix sstables mistake ) limit say 10 characters ( file name ) truncating name necessary ? '' 'bq . limit say 10 characters ( file name ) truncating name necessary ? \\n\\nwe truncate name fit within os path limit adaptively calculation.\\n\\nhow completely omit keyspace name keep columnfamily name adaptively adjust ( truncate ) name ? ' 'this turns bit complex first thought secondary index cfs flushing directory . : ( \\nany ideas ? ' `` 'm inclined say leave alone 2.1. windows users start yelling 3.0 address . lots stuff work meantime . '' `` patch attached unit test.\\ni ended following naming convention/directory structure : \\n\\n { code } \\n/var/lib/cassandra/data/ks/cf-a85fc210cb1011e3a15f9d25721dbb44\\n /.idx\\n ka-1-data.db\\n ... \\n /snapshots\\n /my_snapshot\\n /.idx\\n ka-1-data.db\\n ... \\n ka-1-data.db\\n ... \\n /backups\\n /.idx\\n ka-1-data.db\\n ... \\n ka-1-data.db\\n ... \\n ka-1-data.db\\n ... \\n ks-cf-jb-123-data.db ( older version co exist ) \\n ks-cf.idx-jb-2-data.db\\n { code } \\n\\nhighlight : \\n\\n * keyspace/columnfamily name omitted filename.\\n * 'temporary ' file file name would 'tmp-ka-1-data.db'.\\n * secondary index sstable directory whose name starts '. ' . distinguish snapshots/backup directories.\\n * older version sstable file co-exist along new directory structure . '' `` still fan removing keyspace/table name filename . imo go jonathan 's suggestion leave alone people actually start yelling cause n't remember single user reporting path lengths blocker 'd rather change directory layout ( might break user scripts ) practical evidence 's problem .. '' 'wdyt [ ~joshuamckenzie ] ? likely problem windows users ? ' 'cassandra-4110 limitations schema.java provide us protection there\\ 's really nothing stop users nesting cassandra data 250 characters deep path things blow regardless length limit to.\\n\\non snapshots we\\ 'll using 204 chars worst-case ( 48 ks 48 cf * 2 9 `` snapshots '' 3 slashes ) doesn\\'t leave us lot breathing room path data_file_directories . maybe lowering name_length schema.java would appropriate given cassandra-7136 ? lot users rolling 40+ char ks cf names general much less windows ? ' 'we lot people hit issues first time lowered max name lengths schemas ' `` looks like lowered across board per-platform basis . see file-path limitation linux surprise 's part ecosystem people used windows . '' 'bq . looks like lowered across board per-platform basis.\\n\\nyeah nobody * really * needs keyspace names long ; worth explaining linux snapshot broke moved windows avoid it.\\n\\nbq . snapshots we\\ 'll using 204 chars worst-case ( 48 ks 48 cf * 2 9 `` snapshots '' 3 slashes ) doesn\\'t leave us lot breathing room path data_file_directories.\\n\\nso ... review 3.0 ? it\\ 's ton code . ' `` 3.0 seems like right time frame changes going . n't huge undertaking . '' 'officially tagging [ ~joshuamckenzie ] reviewer ' `` [ ~yukim ] - could get rebase trunk instead 2.1-beta2 ? 's variety file access issues beta2 ( windows ) making testing bit headache . 'll continue testing linux want verify platforms though n't expect platform-specific differences . '' ' [ ~joshuamckenzie ] attaching patch trunk . ' ' w/len 48 cf ks long username ( 30 chars ) windows path length still allows bit 50 chars secondary index name - plenty.\\nfile names look good tmp names snapshots secondary indexes check linux windows.\\n\\n+1 ' 'committed . thanks review . ' `` end removing ks cf name sstable filename right ? \\n\\nbecause breaks least cqlsstablewriter ( precisely abstractsstablesimplewriter ) n't force sstable directory name ks cf breaks code [ here|https : //github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/abstractsstablesimplewriter.java # l66-l88 ] ( precisely { { desc.cfname.equals ( columnfamily ) } } test fails { { desc.cfname } } wrong ) .\\n\\ni suppose could fix code removing test question would n't rally break code say 'm rather uncomfortable fact filename nothing identifying table anymore . feels way easy mistakenly copy sstable different tables directory ending overwriting stuffs n't . also feels like way find ks table name without relying sstable file 's rather fragile imo . maybe record metadata something . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>repair exception getpositionsforranges returns empty iterator cassandra-5250 broke repair , re-adds code cassandra-5249 causing breakage ? possible add test exposes problem ? ' `` fix cassandra-5250 fixed lcs ( tests intersecting sstables empty leveledscanner ) re-broke stcs\\n\\ni 'll try write unit test '' 'adds unit test would found bug ' 'lgtm committed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>cep-11 : memtable api implementation pluggable memtable api described [ cep-11|https : //cwiki.apache.org/confluence/display/cassandra/cep-11 % 3a+pluggable+memtable+implementations ] . initial version already available [ branch|https : //github.com/datastax/cassandra/tree/memtable-api ] , needs updated changes trunk . two additional features suggested cep reviewers also implemented : * sharding support : extending memtable owner interface supply suitable shard boundaries split owned token space agreement disk boundaries . * shared read api sstables : defining common interface reading partitions memtables sstables ; include filters avoid unnecessary copying . 32 { } } } ) .\\r\\n - table schema specify memtable configuration use allowed modify properties.\\r\\n\\r\\nlet\\ 's continue discussion . ' `` sgtm . agree 's point overcomplicating achieves every goal raised . '' `` [ ~blambov ] quite done review pass wanted clarify something pieces memtable api jira used cassandra-17240 . ( ex . { { shardboundaries } } friends ) assuming resolve issue 4.1 freeze reason want items included * * want able plug cassandra-17240 4.1 builds even though n't land in-tree 5.0 ? \\r\\n\\r\\nat end day cassandra-17240 going happen long bits 're committing up-front n't destabilizing n't really matter . due diligence make sure patch n't larger . thoughts ? '' `` done first pass review . 've left ton nits questions inline pr . aside mentioned previous comment thing 'm really worried terms impact 4.1 [ this|https : //github.com/apache/cassandra/pull/1295/files # r848687665 ] . overall things look pretty good copious inline documentation made things easy reason about.\\r\\n\\r\\ni 've gone back forth mentally depth hierarchy { { memtable } } specifically whether composition would made sense ( i.e . strategies allocation commitlog stuff etc. ) . perhaps fact either 'll control implementations pretty tightly wo n't many external ones makes unimportant . ( 'll also ignore moment divergence might create public/non-public forks lol ) '' ' initially left sharding bits ticket 17240 moved comments discussion thread indicated interest form support . ' '+1 ' 'committed [ e4e19e33faf9ac7cf27a9779c8083a7f5c5b865a|https : //github.com/apache/cassandra/commit/e4e19e33faf9ac7cf27a9779c8083a7f5c5b865a ] . ' 'thank [ ~maedhroz ] [ ~adelapena ] detailed review quick responses . ' 'dtest changes committed [ bd5e29c7ca8e0d6987ba9d180d97766cb30eb0fa|https : //github.com/apache/cassandra-dtest/commit/bd5e29c7ca8e0d6987ba9d180d97766cb30eb0fa ] .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>indexes : cf mbeans automatic indexes never unregistered deleted . add , delete , add index get stacktrace effect : { noformat } java.lang.runtimeexception : javax.management.instancealreadyexistsexception : org.apache.cassandra.db : type=indexcolumnfamilies , keyspace=keyspace1 , columnfamily=standard1.616765 org.apache.cassandra.db.columnfamilystore. &lt; init &gt; ( ) org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore ( ) org.apache.cassandra.db.columnfamilystore.addindex ( ) org.apache.cassandra.db.columnfamilystore.reload ( ) org.apache.cassandra.db.migration.updatecolumnfamily.applymodels ( ) org.apache.cassandra.db.migration.migration.apply ( ) org.apache.cassandra.thrift.cassandraserver $ 2.call ( ) java.util.concurrent.futuretask $ sync.innerrun ( ) java.util.concurrent.futuretask.run ( ) java.util.concurrent.threadpoolexecutor $ worker.runtask ( ) java.util.concurrent.threadpoolexecutor $ worker.run ( ) java.lang.thread.run ( ) caused : javax.management.instancealreadyexistsexception : org.apache.cassandra.db : type=indexcolumnfamilies , keyspace=keyspace1 , columnfamily=standard1.616765 com.sun.jmx.mbeanserver.repository.addmbean ( ) com.sun.jmx.interceptor.defaultmbeanserverinterceptor.internal_addobject ( ) com.sun.jmx.interceptor.defaultmbeanserverinterceptor.registerdynamicmbean ( ) com.sun.jmx.interceptor.defaultmbeanserverinterceptor.registerobject ( ) com.sun.jmx.interceptor.defaultmbeanserverinterceptor.registermbean ( ) com.sun.jmx.mbeanserver.jmxmbeanserver.registermbean ( ) org.apache.cassandra.db.columnfamilystore. &lt; init &gt; ( ) ... 11 { noformat } cfs.reload ( ) manages index deletion , never unregisters mbeans creates initialization . someone already wrote method forgot call it.\\nwhoops . ' 'committed . ' 'updated changes ' 'integrated cassandra-0.7 # 70 ( see [ https : //hudson.apache.org/hudson/job/cassandra-0.7/70/ ] ) \\n ' 'integrated cassandra # 625 ( see [ https : //hudson.apache.org/hudson/job/cassandra/625/ ] ) \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>error deleting columnfamily compacted . following dtest command produces error : { code } export cassandra_version=git : cassandra-1.1 ; nosetests -- nocapture -- nologcapture concurrent_schema_changes_test.py : testconcurrentschemachanges.load_test { code } error : { code } error occured compaction java.util.concurrent.executionexception : java.io.ioerror : java.io.filenotfoundexception : /tmp/dtest-6ecmgy/test/node1/data/keyspace1/standard1/keyspace1-standard1-hc-47-data.db ( file directory ) java.util.concurrent.futuretask $ sync.innerget ( ) java.util.concurrent.futuretask.get ( ) org.apache.cassandra.db.compaction.compactionmanager.performmaximal ( ) org.apache.cassandra.db.columnfamilystore.forcemajorcompaction ( ) org.apache.cassandra.service.storageservice.forcetablecompaction ( ) sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) sun.reflect.nativemethodaccessorimpl.invoke ( ) sun.reflect.delegatingmethodaccessorimpl.invoke ( ) java.lang.reflect.method.invoke ( ) com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2 ( ) com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2 ( ) com.sun.jmx.mbeanserver.mbeanintrospector.invokem ( ) com.sun.jmx.mbeanserver.perinterface.invoke ( ) com.sun.jmx.mbeanserver.mbeansupport.invoke ( ) com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke ( ) com.sun.jmx.mbeanserver.jmxmbeanserver.invoke ( ) javax.management.remote.rmi.rmiconnectionimpl.dooperation ( ) javax.management.remote.rmi.rmiconnectionimpl.access $ 200 ( ) javax.management.remote.rmi.rmiconnectionimpl $ privilegedoperation.run ( ) javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation ( ) javax.management.remote.rmi.rmiconnectionimpl.invoke ( ) sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) sun.reflect.nativemethodaccessorimpl.invoke ( ) sun.reflect.delegatingmethodaccessorimpl.invoke ( ) java.lang.reflect.method.invoke ( ) sun.rmi.server.unicastserverref.dispatch ( ) sun.rmi.transport.transport $ 1.run ( ) java.security.accesscontroller.doprivileged ( native method ) sun.rmi.transport.transport.servicecall ( ) sun.rmi.transport.tcp.tcptransport.handlemessages ( ) sun.rmi.transport.tcp.tcptransport $ connectionhandler.run0 ( ) sun.rmi.transport.tcp.tcptransport $ connectionhandler.run ( ) java.util.concurrent.threadpoolexecutor.runworker ( ) java.util.concurrent.threadpoolexecutor $ worker.run ( ) java.lang.thread.run ( ) caused : java.io.ioerror : java.io.filenotfoundexception : /tmp/dtest-6ecmgy/test/node1/data/keyspace1/standard1/keyspace1-standard1-hc-47-data.db ( file directory ) org.apache.cassandra.io.sstable.sstablescanner. &lt; init &gt; ( ) org.apache.cassandra.io.sstable.sstablereader.getdirectscanner ( ) org.apache.cassandra.io.sstable.sstablereader.getdirectscanner ( ) org.apache.cassandra.db.compaction.abstractcompactionstrategy.getscanners ( ) org.apache.cassandra.db.compaction.abstractcompactionstrategy.getscanners ( ) org.apache.cassandra.db.compaction.compactiontask.execute ( ) org.apache.cassandra.db.compaction.compactionmanager $ 6.runmaythrow ( ) org.apache.cassandra.utils.wrappedrunnable.run ( ) java.util.concurrent.executors $ runnableadapter.call ( ) java.util.concurrent.futuretask $ sync.innerrun ( ) java.util.concurrent.futuretask.run ( ) ... 3 caused : java.io.filenotfoundexception : /tmp/dtest-6ecmgy/test/node1/data/keyspace1/standard1/keyspace1-standard1-hc-47-data.db ( file directory ) java.io.randomaccessfile.open ( native method ) java.io.randomaccessfile. &lt; init &gt; ( ) org.apache.cassandra.io.util.randomaccessreader. &lt; init &gt; ( ) org.apache.cassandra.io.util.randomaccessreader.open ( ) org.apache.cassandra.io.util.randomaccessreader.open ( ) org.apache.cassandra.io.sstable.sstablereader.opendatareader ( ) org.apache.cassandra.io.sstable.sstablescanner. &lt; init &gt; ( ) ... 13 { code } reference , dtest function causes failure . error happens line near bottom drops columnfamily : { code } def load_test ( self ) : `` '' '' apply schema changes cluster load. `` '' '' debug ( `` load_test ( ) '' ) cluster = self.cluster cluster.populate ( 1 ) .start ( ) node1 = cluster.nodelist ( ) [ 0 ] wait ( 2 ) cursor = self.cql_connection ( node1 ) .cursor ( ) def stress ( args= [ ] ) : debug ( `` stressing '' ) node1.stress ( args ) debug ( `` done stressing '' ) def compact ( ) : debug ( `` compacting ... '' ) node1.nodetool ( 'compact ' ) debug ( `` done compacting . '' ) # put data cluster stress ( [ ' -- num-keys=1000000 ' ] ) # start compacting ... tcompact = thread ( target=compact ) tcompact.start ( ) wait ( 1 ) # cluster lot load . make schema changes . cursor.execute ( `` use keyspace1 '' ) wait ( 1 ) cursor.execute ( `` drop columnfamily standard1 '' ) wait ( 3 ) cursor.execute ( `` create columnfamily standard1 ( key text primary key ) '' ) tcompact.join ( ) { code } , error happens cassandra-1.1 , cassandra-1.0 . one seems caused problem cassandra-4230 . ' `` maybe 'm skeptical -- 4230 complaining file existing n't one says file n't exist : ) '' 'patch adds try stop running compactions given keyspace columnfamily running drop command . tried test description ran without failures . ' 'that takes us back bad old days pre-cassandra-3116 though . able fix w/o resorting big lock . ' 'for ks cf drop seems necessary try wait running compactions finish otherwise would end errors like one description also operations - create update - affected . ' 'the idea 3116 : \\n\\n- drop delete sstables actively compacted\\n- post-compaction check cf dropped delete sstables ' `` n't know one better tho compaction fails reason scenario would n't mean sstables left behind staying somebody manually deletes ( restart would drop ) ? would add complexity schema merge handle case well local side ... '' `` way would 'mark cf delete ' return user right way ( making cf invisible users ) sending drop request others would apply thing ( try stop compactions running wait done ) drop . '' `` bq . would n't mean sstables left behind staying somebody manually deletes ( restart would drop ) ? \\n\\nwe already clean partially-written sstables compaction failure n't see could n't use similar logic . '' `` problem see need snapshot start dropping deleting cf files 's probably better make drop option 'deferred ' running compactions stopped persistent view files would operate upon . '' 'datatracker already makes sstable changes atomic though . time snapshot get consistent view . ' 'tyler still reproduce recent schema fixes 1.1 branch ? ' 'yes error happened . fresh pull branch branch cassandra-1.1 . ' `` interesting ca n't reproduce . please run logging patch attached ( enabled debug logging ) attach debug log c * node task check happening inside datatracker case ? ... `` 'this applying patches cassandra-1.1 branch setting logging debug . ' 'somehow server.log debug info . looking . ' 'debug enabled ; looks like ccm overwrites log level . logging patch applied run . ' ' see debug information added right ioerror log described task ... ' `` experimentation problem happening log level set info n't happen debug . got ta love ones ! modified logging patch logging.info ( ) rather logging.debug ( ) problem still happens least see debug messages . hope enough go . '' `` hah know causing - 's drop problem situation triggered re-create columnfamily right drop + ( sstables actually deleted background task ) + reads sstables directory back system tries compact simultaneously deleted background . warn people * avoid * making modifications active cfs otherwise could lead strange situations like one . '' 'would fixed cassandra-3794 since old new cf different ids ? ' 'not really generates uuid ksname + cfname able make across machines independent state . ' 'should add call abort in-progress compactions drop time ( help cleanup happen faster ) call `` close we\\ 're going get ? '' ' 'this patch : ) ' '- stopcompactionfor take cfs parameters instead string\\n- don\\'t see reason wait indefinitely ; fact make sure wait compaction finishes odds much better tell client `` done '' won\\'t able send `` create '' quickly enough hit bug\\n- need call stopcompactionfor every replica compactionserver -- move defstable.dropcolumnfamily ? \\n ' 'bq . stopcompactionfor take cfs parameters instead string\\n\\ni don\\'t really follow want list cfmetadata instead string ? string better suited compactioninfo.getcolumnfamily ( ) returns string ( cf name ) .\\n\\nbq . don\\'t see reason wait indefinitely ; fact make sure wait compaction finishes odds much better tell client `` done '' won\\'t able send `` create '' quickly enough hit bug\\n\\nwe don\\'t really try wait indefinitely 30 seconds ( worst case ) compactions don\\'t finish move delete . want wait compactions finish ? \\n\\nbq . need call stopcompactionfor every replica compactionserver – move defstable.dropcolumnfamily ? \\n\\ni agree i\\ 'm going move dropcolumnfamily call gets called replicas . ' `` bq . string better suited compactioninfo.getcolumnfamily ( ) returns string \\n\\nfeel free fix . : ) \\n\\nbq . compactions n't finish move delete\\n\\nit throws ioexception.\\n\\nremember check ability abort compaction every row ; 're compacting wide row could easily take 30s w/ throttling . '' `` bq . throws ioexception . remember check ability abort compaction every row ; 're compacting wide row could easily take 30s w/ throttling.\\n\\noh yes sorry . think remove exception move drop want compactions finish ? \\n\\n '' `` v3 attached . removes compactioninfo fields redundant w/ introduction cfm removes wait stop method ( n't help clean sstables involved faster point slowing drop ) . '' 'committed nit compactioninfo.getcolumnfamily ( ) return cfname instead ksname v3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>ae arraybackedsortedcolumns { noformat } error [ ] 2013-08-07 cassandradaemon.java ( line 192 ) exception thread thread [ , main ] java.lang.assertionerror : added column sort last column org.apache.cassandra.db.arraybackedsortedcolumns.addcolumn ( ) org.apache.cassandra.db.abstractcolumncontainer.addcolumn ( ) org.apache.cassandra.db.abstractcolumncontainer.addcolumn ( ) org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns ( ) org.apache.cassandra.db.filter.queryfilter.collatecolumns ( ) org.apache.cassandra.db.filter.queryfilter.collateondiskatom ( ) org.apache.cassandra.db.collationcontroller.collectalldata ( ) org.apache.cassandra.db.collationcontroller.gettoplevelcolumns ( ) org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns ( ) org.apache.cassandra.db.columnfamilystore.getcolumnfamily ( ) org.apache.cassandra.db.columnfamilystore.getcolumnfamily ( ) org.apache.cassandra.db.table.getrow ( ) org.apache.cassandra.db.slicefromreadcommand.getrow ( ) org.apache.cassandra.service.storageproxy $ localreadrunnable.runmaythrow ( ) org.apache.cassandra.service.storageproxy $ droppablerunnable.run ( ) java.util.concurrent.threadpoolexecutor.runworker ( ) java.util.concurrent.threadpoolexecutor $ worker.run ( ) java.lang.thread.run ( ) { noformat } test_column_index_stress wide_rows_test reproduce within ~20 runs bisect strongly points regression cassandra-5762 make buildbound return collection avoid extra copy ? \\n\\nno unfortunately ca n't . well kinda 'd create extra iterator getkeybound ( wo n't able .get ( 0 ) ) two extra iterators makefilter ( wo n't able .get ( ) - need iterate startbounds endbounds simultaneously build columnslices ) . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>secondary indexing map keys work properly mixing contains contains_key table map column index map key selecting data using contains key contains return expected data . problem reproduced using following unit test : { code } @ test public void testmapkeycontainsandvaluecontains ( ) throws throwable { createtable ( `` create table % ( account text , id int , categories map &lt; text , text &gt; , primary key ( account , id ) ) '' ) ; createindex ( `` create index % ( keys ( categories ) ) '' ) ; execute ( `` insert % ( account , id , categories ) values ( ? , ? , ? ) '' , `` test '' , 5 , map ( `` lmn '' , `` foo '' ) ) ; assertrows ( execute ( `` select * % account = ? id = ? categories contains key ? categories contains ? allow filtering '' , `` test '' , 5 , `` lmn '' , `` foo '' ) , row ( `` test '' , 5 , map ( `` lmn '' , `` foo '' ) ) ) ; } { code } http : //www.datastax.com/dev/blog/cql-in-2-1 ) \\n\\nin example index keys select attempting query values index . empty result sort correct probably warning message prevent confusion ( `` index exist '' something like ) .\\n\\n/cc [ ~slebresne ] ' 'the empty result correct matter look . two things happen response query : \\nyou either get good result back get error message telling wrong.\\ni agree querying key value map make lot sense error message perfectly acceptable . ' `` n't totally agree . correct currently allow one index per cql column one index keys values given map 's test . test index keys.\\n\\nregarding { { select } } provided indexed clause ( example ) 's allowed non-indexed clause ( require { { allow filtering } } 's used example ) . 'm sure n't work ( 's worth testing current 2.1 branch though maybe fixed since 2.1.0 ) .\\n\\nbq . agree querying key value map make lot sense\\n\\nout curiosity would n't make sense ? '' `` { quote } ( 's worth testing current 2.1 branch though maybe fixed since 2.1.0 ) . { quote } \\ni tested latest 2.1\\n\\n { quote } curiosity would n't make sense ? { quote } \\nas map one value associated given key using query means want check key exists value one think . select using contains key able information also know key missing value expect.\\nthat think make lot sense error message fine user.\\nnow user also true also give better sense robustness query handled properly ; - ) \\n '' `` bq . map one value associated given key using query means want check key exists value one think be.\\n\\nthat 's query means . asking maps contains given key given value imply said given value must associated said given key.\\nbesides even query means query still make sense . might terribly useful make sense 'd still think throwing error would user friendly . '' 'the problem cause fact indexsearcher trying use key index search contains value.\\nthis patch fix problem also fix selectstatement check indexed column ( select contains map key indexed reject query ) .\\nthe patch also replace index option strings constants avoid typos issues . ' 'hmm makes fixes cassandra-8155 different way . personally prefer putting logic `` index support operator '' index code instead operator code ( 8155 patch ) . however would good include test cases constants instead string options.\\n\\ndo want take look patch see agree ? ' ' like { { supportsoperator } } approach cassandra-8155 think validation preparation phase select statement execution time . seems user friendly me.\\n\\nso personally would kind merge two patches follow : \\n * use constants instead string options tests patch\\n * use { { supportsoperator } } approach cassandra-8155 instead { { isvalidindexfor } } { { secondaryindex } } found much nicer\\n * keep validation selectstatement preparation phase ( like approach patch use one ) . ' `` bq . personally would kind merge two patches follow\\n\\nthat sounds good . take care making patch 'll resolve 8155 duplicate ? '' 'no problem . ' 'patch resulting merge v1 patch one cassandra-8155 ' 'overall looks good.\\n\\nthe remaining use { { relation.allowsindexqueryon ( ) } } { { selectstatement.processrelationentity ( ) } } . could remove { { allowsindexqueryon ( ) } } ( { { collectiontype.ismap ( ) } } ) something like { { processrelationentity ( ) } } : \\n\\n { code } \\nsecondaryindex index = indexmanager.getindexforcolumn ( def.name.bytes ) ; \\nif ( index ! = null &amp; &amp; index.supportsoperator ( relation.operator ( ) ) ) \\n return new boolean [ ] { true def.kind == columndefinition.kind.clustering_column } ; \\n { code } \\n\\n ( looks like need able convert { { relation.type } } enum values { { indexexpression.operator } } enum values though . ) ' ' { { relation.type } } { { indexexpression.operator } } really similar . merge ? yes package think put resulting class ? ' `` bq . relation.type indexexpression.operator really similar . merge ? \\n\\ni think 's good idea 're willing ( perhaps second patch clarity ) .\\n\\nbq . package think put resulting class ? \\n\\ni 'm tempted use relation.type think 's bad idea reason would make o.a.c.cql3.operator class . '' 'additional patch replaces { { relation.type } } { { indexexpression.operator } } { { operator } } fix last remaining issue . ' 'thanks ! committed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>remove unrepaired sstables garbage collection only_purge_repaired_tombstones true avoid assertionerror nodetool garbagecollect manually running garbage collection compaction across table unrepaired sstables only_purge_repaired_tombstones set true assertion error thrown . unrepaired sstables n't removed transaction filtered filtersstables ( ) . ||3.11||trunk|| | [ branch|https : //github.com/vincewhite/cassandra/commit/e13c822736edd3df3403c02e8ef90816f158cde2 ] | [ branch|https : //github.com/vincewhite/cassandra/commit/cc8828576404e72504d9b334be85f84c90e77aa7 ] | stacktrace : { noformat } -- stacktrace -- java.lang.assertionerror org.apache.cassandra.db.compaction.compactionmanager.parallelallsstableoperation ( ) org.apache.cassandra.db.compaction.compactionmanager.performgarbagecollection ( ) org.apache.cassandra.db.columnfamilystore.garbagecollect ( ) org.apache.cassandra.service.storageservice.garbagecollect ( ) sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) sun.reflect.nativemethodaccessorimpl.invoke ( ) sun.reflect.delegatingmethodaccessorimpl.invoke ( ) java.lang.reflect.method.invoke ( ) sun.reflect.misc.trampoline.invoke ( ) sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) sun.reflect.nativemethodaccessorimpl.invoke ( ) sun.reflect.delegatingmethodaccessorimpl.invoke ( ) java.lang.reflect.method.invoke ( ) sun.reflect.misc.methodutil.invoke ( ) com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2 ( ) com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2 ( ) com.sun.jmx.mbeanserver.mbeanintrospector.invokem ( ) com.sun.jmx.mbeanserver.perinterface.invoke ( ) com.sun.jmx.mbeanserver.mbeansupport.invoke ( ) com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke ( ) com.sun.jmx.mbeanserver.jmxmbeanserver.invoke ( ) javax.management.remote.rmi.rmiconnectionimpl.dooperation ( ) javax.management.remote.rmi.rmiconnectionimpl.access $ 300 ( ) javax.management.remote.rmi.rmiconnectionimpl $ privilegedoperation.run ( ) javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation ( ) javax.management.remote.rmi.rmiconnectionimpl.invoke ( ) sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) sun.reflect.nativemethodaccessorimpl.invoke ( ) sun.reflect.delegatingmethodaccessorimpl.invoke ( ) java.lang.reflect.method.invoke ( ) sun.rmi.server.unicastserverref.dispatch ( ) sun.rmi.transport.transport $ 1.run ( ) sun.rmi.transport.transport $ 1.run ( ) java.security.accesscontroller.doprivileged ( native method ) sun.rmi.transport.transport.servicecall ( ) sun.rmi.transport.tcp.tcptransport.handlemessages ( ) sun.rmi.transport.tcp.tcptransport $ connectionhandler.run0 ( ) sun.rmi.transport.tcp.tcptransport $ connectionhandler.lambda $ run $ 0 ( ) java.security.accesscontroller.doprivileged ( native method ) sun.rmi.transport.tcp.tcptransport $ connectionhandler.run ( ) java.util.concurrent.threadpoolexecutor.runworker ( ) java.util.concurrent.threadpoolexecutor $ worker.run ( ) java.lang.thread.run ( ) { noformat } patch lgtm could probably test ensure never include unrepaired sstables onlypurgerepairedtombstones true . ' 'so patched implementation pretty much follows { { performsstablerewrite ( ) } } looks like correct way handle . could modify { { gccompactiontest } } bit make effects testable see [ ebd7de7|https : //github.com/spodkowinski/cassandra/commit/ebd7de758b48a6f924d60eeecbc615c355c87257 ] . ' 'prs\\r\\n\\r\\ntrunk https : //github.com/apache/cassandra/pull/2423\\r\\n4.1 https : //github.com/apache/cassandra/pull/2424\\r\\n4.0 https : //github.com/apache/cassandra/pull/2425\\r\\n3.11 https : //github.com/apache/cassandra/pull/2426\\r\\n\\r\\nbuilds\\r\\n\\r\\ntrunk \\r\\nj11 pre-commit https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2482/workflows/b8690c93-121b-4ed6-aed7-6e742285ce13\\r\\nj8 pre-commit https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2482/workflows/20e45719-36cb-4010-95b5-89c5519e91d3 \\r\\n4.1\\r\\nj11 pre-commit https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2484/workflows/bfa530d2-1ab9-4ce3-9f08-59d41f0bbcac\\r\\nj8 pre-commit https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2497/workflows/2eb2a057-8b1a-44ad-af12-72c34459b551\\r\\n4.0 \\r\\nj11 pre-commit https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2485/workflows/cb68a9f9-8e52-4d45-843d-24e626ca0402\\r\\nj8 pre-commit https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2485/workflows/fbf627a1-ed93-42c9-99c8-524e22536891\\r\\n3.11 https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2486/workflows/c32e4dc3-e680-4ceb-8218-3bd71c5a5bff ' ' [ ~jjirsa ] would mind take look please ? quite straightforward . ' ' [ ~blambov ] would mind take look ? contacted jeff working cassandra actively moment . ' 'thank [ ~blambov ] review would mind take look ? https : //github.com/apache/cassandra/pull/2423 ' 'branimir +1ed pr . builds here\\r\\n\\r\\n3.11 [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2631/workflows/812d7b99-1da3-4cc1-b15e-ac5cb07c8f4e ] \\r\\n4.0 j11 [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2632/workflows/85890046-207a-4508-8c76-fb3e06b76c48 ] \\r\\n4.0 j8 [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2632/workflows/ae1a70e2-65ff-46e9-a5d1-b885fa372aef ] \\r\\n4.1 j11 [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2633/workflows/9041d37f-a5a0-4346-81ec-0bdcd2189b36 ] \\r\\n4.1 j8 [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2633/workflows/d55d764d-1661-4b2c-9b2b-964f30775600 ] \\r\\ntrunk j8 [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2637/workflows/24a86898-bcb3-47d1-9631-a894aac0f12a ] \\r\\ntrunk j8 [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2637/workflows/e84fc541-abe8-43ca-9b4b-d38109fe374b ] \\r\\n\\r\\n [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2638/workflows/c84de8f7-8e9f-4f41-82d4-7ad3c5892859 ] \\r\\n [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/2638/workflows/38a73d2f-a21d-4bd3-abfc-12f79425ffb3 ] \\r\\n\\r\\ni going merge .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>sstablereader.clonewithnewstart drop much page cache compressed files description patch available [ here|https : //github.com/belliottsmith/cassandra/tree/8746 ] \\n\\nbasically move droppagecache ( ) call inside segmentedfile ( better encapsulation anyway ) ; compressed versions override lookup start position relevant segment drop data prior ' '+1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>index summary redistribution start even compactions paused pause autocompaction upgradesstables/scrub/cleanup etc pause compaction strategies make sure grab sstables , index summary redistribution pause cause us fail operation . [ 3.0|https : //circleci.com/workflow-run/8882a8a6-8593-4d3e-8ec1-05bcab855a44 ] [ 3.11|https : //circleci.com/workflow-run/6b057c7e-1b4a-4f11-9af8-eb3ec2dd8cc9 ] [ trunk|https : //circleci.com/workflow-run/457f8304-c477-45e7-b195-06cf67c22450 ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>remove backpressurestrategy odd : { { info [ main ] 2019-10-25 - back-pressure disabled strategy org.apache.cassandra.net.ratebasedbackpressure\\ { high_ratio=0.9 , factor=5 , flow=fast } . } } saw , n't sure back pressure actually disabled , really using { { ratebasedbackpressure . } } change output either : { { back-pressure disabled } } { { } } { { back-pressure enabled strategy org.apache.cassandra.net.ratebasedbackpressure\\ { high_ratio=0.9 , factor=5 , flow=fast } . } } { { } } maybe [ change wording here|https : //github.com/belliottsmith/cassandra/commit/2ba8b4d162c20142c3d4c7a225432337b7bdbc36 # diff-4805e34bd9553ede03778be66ddc06c7r262 ] `` removed '' instead `` deprecated '' ? ' 'thanks committed suggestion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>reject bootstrapping endpoints already ring different gossip data ring silently broken improperly bootstrapping endpoint existing entry gossip table . case node attempts bootstrap ip address existing ring member , old token metadata dropped without warning , resulting range shifts cluster . n't bad non-vnode cases , general , tokens explicitly assigned , bootstrap token would result range shifts . vnode cases , convention let nodes come selecting tokens , bootstrap override existing tokens endpoint . issues open adding explicit rebootstrap feature vnode cases , given changes operator habits vnode rings , seems bit easy make happen . even undesirable fact 's basically silent . proposal checking exact case : bootstraps endpoints existing ring entries different hostids and/or tokens rejected error message describing happened override safety check . looks like override supported using existing `` nodetool removenode -force '' . work patch . https : //github.com/netflix/priam/issues/313\\n '' ' would assume 1.2.x branch also affected issue . plans backport enhancement 1.2 branch ? ' `` assumption correct 1.2 receiving updates longer . 's probably trivial apply choose route . '' 'just caught misread date seen elsewhere . thank replying .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>fix leak detected errors unit tests several errors running unit tests trunk : { code } [ junit ] error leak detected : reference ( org.apache.cassandra.utils.concurrent.ref $ state @ 317c884a ) class org.apache.cassandra.io.util.safememory $ memorytidy @ 943674927 : memory @ [ 7f1bcc0078e0 .. 7f1bcc007908 ) released reference garbage collected [ junit ] error leak detected : reference ( org.apache.cassandra.utils.concurrent.ref $ state @ 317c884a ) class org.apache.cassandra.io.util.safememory $ memorytidy @ 943674927 : memory @ [ 7f1bcc0078e0 .. 7f1bcc007908 ) released reference garbage collected [ junit ] error allocate trace org.apache.cassandra.utils.concurrent.ref $ state @ 317c884a : [ junit ] thread [ , main ] [ junit ] java.lang.thread.getstacktrace ( ) [ junit ] org.apache.cassandra.utils.concurrent.ref $ debug. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.utils.concurrent.ref $ state. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.utils.concurrent.ref. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.io.util.safememory. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.io.util.safememorywriter. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.io.sstable.indexsummarybuilder. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.io.sstable.format.big.bigtablewriter $ indexwriter. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.io.sstable.format.big.bigtablewriter. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.io.sstable.format.big.bigformat $ writerfactory.open ( ) [ junit ] org.apache.cassandra.io.sstable.format.sstablewriter.create ( ) [ junit ] org.apache.cassandra.db.compaction.writers.defaultcompactionwriter. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.db.compaction.compactiontask.getcompactionawarewriter ( ) [ junit ] org.apache.cassandra.db.compaction.compactiontask.runmaythrow ( ) [ junit ] org.apache.cassandra.utils.wrappedrunnable.run ( ) [ junit ] org.apache.cassandra.db.compaction.compactiontask.executeinternal ( ) [ junit ] org.apache.cassandra.db.compaction.abstractcompactiontask.execute ( ) [ junit ] org.apache.cassandra.db.compaction.compactionmanager $ backgroundcompactiontask.run ( ) [ junit ] java.util.concurrent.executors $ runnableadapter.call ( ) [ junit ] java.util.concurrent.futuretask.run ( ) [ junit ] java.util.concurrent.threadpoolexecutor.runworker ( ) [ junit ] java.util.concurrent.threadpoolexecutor $ worker.run ( ) [ junit ] java.lang.thread.run ( ) [ junit ] [ junit ] error allocate trace org.apache.cassandra.utils.concurrent.ref $ state @ 317c884a : [ junit ] thread [ , main ] [ junit ] java.lang.thread.getstacktrace ( ) [ junit ] org.apache.cassandra.utils.concurrent.ref $ debug. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.utils.concurrent.ref $ state. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.utils.concurrent.ref. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.io.util.safememory. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.io.util.safememorywriter. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.io.sstable.indexsummarybuilder. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.io.sstable.format.big.bigtablewriter $ indexwriter. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.io.sstable.format.big.bigtablewriter. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.io.sstable.format.big.bigformat $ writerfactory.open ( ) [ junit ] org.apache.cassandra.io.sstable.format.sstablewriter.create ( ) [ junit ] org.apache.cassandra.db.compaction.writers.defaultcompactionwriter. &lt; init &gt; ( ) [ junit ] org.apache.cassandra.db.compaction.compactiontask.getcompactionawarewriter ( ) [ junit ] org.apache.cassandra.db.compaction.compactiontask.runmaythrow ( ) [ junit ] org.apache.cassandra.utils.wrappedrunnable.run ( ) [ junit ] org.apache.cassandra.db.compaction.compactiontask.executeinternal ( ) [ junit ] org.apache.cassandra.db.compaction.abstractcompactiontask.execute ( ) [ junit ] org.apache.cassandra.db.compaction.compactionmanager $ backgroundcompactiontask.run ( ) [ junit ] java.util.concurrent.executors $ runnableadapter.call ( ) [ junit ] java.util.concurrent.futuretask.run ( ) [ junit ] java.util.concurrent.threadpoolexecutor.runworker ( ) [ junit ] java.util.concurrent.threadpoolexecutor $ worker.run ( ) [ junit ] java.lang.thread.run ( ) [ junit ] { code } leaks fixed cassandra-9117 patch attached covers 2 remaining unit tests ( reftests ) . ' 'hi [ ~thobbs ] ] would mind reviewing ? ' '+1 committed { { 1f65a12 } } . thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>serverwide caps memtable thresholds storing global operation throughput thresholds , could eliminate `` many small memtables '' problem caused many cfs . global threshold would set config file , allow different classes servers different values configured . operations occurring memtable would add global counters , addition memtable-local counters . global threshold violated , memtable system using largest fraction 's local threshold would flushed . local thresholds would continue act always . result would larger sstables , safer operation multiple cfs per node tuning . last nitpick flush_largest_memtables_at memtable_total_space_in_mb settings could made consistent . absolute minimum refer one another config file 'm wondering might unify 3 4 different reasons flushing monitoring/logging somehow.\\n\\nalso start making plan deprecate per-cf settings convert fractions mentioned . '' `` committed.\\n\\ni think want keep flush_largest_memtables_at ; since measures directly gc 's good complement.\\n\\nagreed deprecating per-cf settings let 's see shakes w/ testing first . '' 'integrated cassandra # 838 ( see [ https : //hudson.apache.org/hudson/job/cassandra/838/ ] ) \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>remove cql2 entirely cassandra 3.0 cql2 officially longer worked since 1.2. cqlsh longer supports cql2 cassandra 2.0. 's probably time deprecate cql2 2.0 remove entirely 2.2 - nothing cql2 ca n't done via cql3 two versions advance warning plenty time still using cql2 switch cql3 . added deprecation warning 2.0 5fe46e145adfe54a1fc4521fd274833e3bce4ac2 . ' 'if going remove breaking change 3.0. still people production using cql2 . 2.x issues warning give 3.x migrate away . ' 'that\\ 's basically 2.2 `` next major release one . '' ' `` takes calling 3.0 make happy 's doable . : ) '' 'your concern happiness heart warming . heh\\n\\nwhatever isn\\'t `` took away functionality inside 2.x '' fine . ' `` fwiw sounds like reasonable idea . also encouraging model deprecating features small set people might using one start using 're unsupported/unmaintained . '' 'pushed commit https : //github.com/iamaleksey/cassandra/commits/5918.\\n\\nnot sure thirft definitions ( besides obviously bumping major ) . throw ire cql2 methods ( commit ) get rid altogether erasing last traces cql2 existence ( news changes ) . ' `` 'd say 's marginally user-friendly throw ire rely thrift server respond whatever no-such-method exception . '' 'left alone ( throwing exceptions instead removing entirely ) . regenerated thrift ( tiny change ) . pushed -f branch . ' 'ship ! ' 'committed thanks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>cassandra2.1~beta1 stall boot trying new release several perf . improvements interested . upgrading cassandra 2.0.5 beta version , cassandra stalled init column families . might misconfigure something , seems suck loop . added couple debug statements , , second thought , think leave experts ... 's looping following : { code : title=src/java/org/apache/cassandra/utils/memory/pool.java # needscleaning } 0 &gt; = -858993472 &amp; &amp; true &amp; &amp; true { code } { code : title=log } info [ heapslabpoolcleaner ] 2014-02-21 - java.lang.thread.getstacktrace ( unknown source ) , org.apache.cassandra.db.keyspace $ 1.apply ( ) , org.apache.cassandra.db.keyspace $ 1.apply ( ) , com.google.common.collect.iterators $ 8.transform ( ) , com.google.common.collect.transformediterator.next ( ) , org.apache.cassandra.db.columnfamilystore.all ( ) , org.apache.cassandra.db.columnfamilystore $ flushlargestcolumnfamily.run ( ) , org.apache.cassandra.utils.memory.poolcleanerthread.run ( ) { code } may totally unrelated normal behavior . let know info provide . [ `` obvious questions : \\n- see errors ? \\n- 0 -858993472 correspond used ( ) nextclean part method respectively correct ? limit cleanthreshold ? say ? \\n- consistent every time start ? \\n\\nthis definitely normal almost certainly bug n't ever stop cassandra starting . wonder strange interaction going problem may easier track figure another problem.\\n\\ncould attach output jstacking process ? \\n\\nthe easiest possibility explain somehow memtable_cleanup_threshold negative . n't actually check startup oversight . fact value nextclean exactly \\\\-0.4 * 2gb suspicious - 8gb heap would default 2gb limit default cleanup_threshold 0.4. possible accidentally added '\\\\- ' prefix line config file ? unlikely know would explain instantly : - ) \\n\\n '' 'aha ! good call ! \\n\\n { code : title=src/java/org/apache/cassandra/utils/memory/pool.java # needscleaning } \\nused ( 0 ) &gt; = nextclean ( -858993472 ) &amp; &amp; updatenextclean ( true ) &amp; &amp; cleanerthread ( true ) -- limit ( -2147483648 ) cleanthreshold ( 0.400000 ) \\n { code } \\n\\nit seems working : \\n { code : title=src/java/org/apache/cassandra/config/databasedescriptor.java:1385 } \\n ( long ) conf.memtable_total_space_in_mb &lt; &lt; 20\\n { code } \\n\\nand failed exception ( doh ! ) ... unrelated ticket . ' 'uploaded simple patch correct overflow prevent provision bad cleanup thresholds ' 'committed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>clean messagingservice protocol limitations weaknesses existing protocol : - information asymmetry : node know version node b expects , vice versa ( see cassandra-4101 ) - delayed information : node often know version node b expects , first contacting node b -- forcing throw first message away retry next one - protocol handle cross-dc forwarding broadcast_address ! = socket address ( see bottom cassandra-4099 ) - version partly global , partly per-connection , partly per-message , resulting interesting hacks ( cassandra-3166 ) difficulty layering sophisticated outputstreams socket ( cassandra-3127 , cassandra-4139 ) v ; \\n } \\n { noformat } \\n\\n+1 otherwise . ' 'bq . ms.setversion needs minor fix prevent npe\\n\\ndone committed.\\n\\nbq . could negotiate ssl way compression\\n\\nsure open separate ticket ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>enable cdc unittest follow cassandra-14066 2 cdc unittests skipped normal test run , { { $ ant test-cdc } } run cdc test . fix enables normal { { $ ant test } } build/test/cassandra/cdc_raw:0/commitlog-7-1517000670440_cdc.idx\\r\\n [ junit ] \\tat org.apache.cassandra.db.commitlog.commitlogsegmentmanagercdctest.testcompletedflag ( ) \\r\\n { noformat } \\r\\n\\r\\ni 'm going try circle.yml trunk see . wondering test requires disk space pass ? `` `` think may disk space related . suddenly lost access larger volume circleci supposed mounted container 's longer appearing . '' `` nope disk space . also passes run manually container . reliably fails every time 's run automatically . \\r\\n\\r\\nthis seems generally flaky laptop circleci . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>move migration tasks non-periodic queue , assure flush executor shutdown non-periodic executor example failure : http : //cassci.datastax.com/job/cassandra-3.8_dtest_upgrade/1/testreport/upgrade_tests.cql_tests/testcqlnodes3rf3_upgrade_current_3_x_to_indev_3_x/whole_list_conditional_test failed cassci build cassandra-3.8_dtest_upgrade # 1 relevant error logs { code } unexpected error node1 log , error : error [ ] 2016-07-20 - exception thread thread [ , main ] java.util.concurrent.rejectedexecutionexception : threadpoolexecutor shut org.apache.cassandra.concurrent.debuggablethreadpoolexecutor $ 1.rejectedexecution ( ) ~ [ ] java.util.concurrent.threadpoolexecutor.reject ( ) ~ [ ] java.util.concurrent.threadpoolexecutor.execute ( ) ~ [ ] org.apache.cassandra.concurrent.debuggablethreadpoolexecutor.execute ( ) ~ [ ] java.util.concurrent.abstractexecutorservice.submit ( ) ~ [ ] org.apache.cassandra.db.columnfamilystore.switchmemtable ( ) ~ [ ] org.apache.cassandra.db.columnfamilystore.switchmemtableifcurrent ( ) ~ [ ] org.apache.cassandra.db.columnfamilystore.forceflush ( ) ~ [ ] org.apache.cassandra.schema.schemakeyspace.lambda $ flush $ 1 ( ) ~ [ ] org.apache.cassandra.schema.schemakeyspace $ $ lambda $ 200/1129213153.accept ( unknown source ) ~ [ na : na ] java.lang.iterable.foreach ( ) ~ [ ] org.apache.cassandra.schema.schemakeyspace.flush ( ) ~ [ ] org.apache.cassandra.schema.schemakeyspace.mergeschema ( ) ~ [ ] org.apache.cassandra.schema.schemakeyspace.mergeschemaandannounceversion ( ) ~ [ ] org.apache.cassandra.service.migrationtask $ 1.response ( ) ~ [ ] org.apache.cassandra.net.responseverbhandler.doverb ( ) ~ [ ] org.apache.cassandra.net.messagedeliverytask.run ( ) ~ [ ] java.util.concurrent.executors $ runnableadapter.call ( ) ~ [ ] java.util.concurrent.futuretask.run ( ) ~ [ ] java.util.concurrent.threadpoolexecutor.runworker ( ) ~ [ ] java.util.concurrent.threadpoolexecutor $ worker.run ( ) [ ] java.lang.thread.run ( ) [ ] { code } mixed 3.0.8 , 3.8-tentative cluster \\n\\n| [ 2.2|https : //github.com/ifesdjeen/cassandra/tree/12251-upgrade-2.2 ] | [ dtest|https : //cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-2.2-dtest/ ] | [ testall|https : //cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-2.2-testall/ ] |\\n| [ 3.0|https : //github.com/ifesdjeen/cassandra/tree/12251-upgrade-3.0 ] | [ dtest|https : //cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-3.0-dtest/ ] | [ testall|https : //cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-3.0-testall/ ] |\\n| [ trunk|https : //github.com/ifesdjeen/cassandra/tree/12251-upgrade-trunk ] | [ dtest|https : //cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-dtest/ ] | [ testall|https : //cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-testall/ ] |\\n\\n| [ upgrade tests|https : //cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-upgrade/ ] |\\n\\ni re-ran upgrade tests use patched branches . results look clean. `` '+1 - lgtm . merge forward cleanly 2.2 . ' 'committed [ 465bb5d45ccef337382592127e214a0ca16a3d88|https : //github.com/apache/cassandra/commit/465bb5d45ccef337382592127e214a0ca16a3d88 ] 2.2 merged 3.0 3.0 trunk . thanks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>set cache capacity via nodeprobe cassandra-688 added ability set capacity via jmx . adding nodeprobe useful changing node 's capacity fly . [ `` 03\\n better division aggregated key cache capacity among sstable caches\\n\\n02\\n add cache info cfstats ; add setcachecapacity\\n\\n01\\n use 0-capacity cache instead null indicate caching ; means n't need worry creating &amp; destroying caches jmx ( synchronizing ) \\n '' '+1 ' 'rebased &amp; committed ' `` integrated cassandra # 335 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/335/ ] ) \\n better division aggregated capacity among sstable caches\\npatch jbellis ; reviewed goffinet \\nadd cache info cfstats ; add setcachecapacity\\npatch jbellis ; reviewed goffinet \\nuse 0-capacity cache instead null indicate caching ; means n't need worry creating &amp; destroying cache objects jmx\\npatch jbellis ; reviewed goffinet \\n '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>fix starting paxos auto repair test run ci name ( ending test ) went undetected . fails locally well ( least ) . repaired } } rely running regular/incremental/paxos repair clearing { { system.paxos } } important job auto repair means coordinated paxos repairs finish bit quicker.\\r\\n ' `` given explanation think ticket rc blocker given progress fix two flaky tests . ( still try get 's one two tickets currently waiting on… ) '' `` ticket n't fixing tests starting paxos auto repair done think ready commit ? go rc create tickets flaky tests . '' `` even ticket included running tests erroneously running flaky would entirely weird refuse merge view flaky tests . tests _should running_ failing either way . failing test metric gamed 's encourage good practices . '' `` n't disagree fix naming test runs . n't think block rc though unless time fix . '' `` agree n't block rc . '' 'what got go extra line cassandradeamon renamed test class fixed tests even flaky block rc create new tickets make stable . assign run builds trunk well . ' ' 4.1 j11 pre-commit [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/1573/workflows/84394d1a-13a7-4de3-955c-a0d7cfed2681 ] \\r\\n4.1 j8 pre-commit [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/1561/workflows/c8fae944-d876-4c2a-a8fd-05bb0b8a693b ] \\r\\ntrunk j11 pre-commit [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/1572/workflows/e5b6b663-68c7-46dc-82df-206d907da949 ] \\r\\ntrunk j8 pre-commit [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/1572/workflows/c324f4ef-c9d1-4456-9f0d-9d4b4e2524c6 ] \\r\\n\\r\\ntrunk pr https : //github.com/apache/cassandra/pull/2007\\r\\n4.1 pr https : //github.com/apache/cassandra/pull/1994\\r\\n ' ' moving `` needs commiter '' afaik needs go 4.1 trunk . nothing 4.0 like . two tests flaky expected . ' ' think [ ~benedict ] may +1 already ? ' 'yep lgtm +1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>get_range_slice npe call get_range_slice arguments slicerange structure , seems npe . think nothing range specified column slice start end . error - error threadpoolexecutor java.lang.runtimeexception : java.lang.nullpointerexception org.apache.cassandra.service.rangesliceverbhandler.doverb ( ) org.apache.cassandra.net.messagedeliverytask.run ( ) java.util.concurrent.threadpoolexecutor $ worker.runtask ( ) java.util.concurrent.threadpoolexecutor $ worker.run ( ) java.lang.thread.run ( ) caused : java.lang.nullpointerexception org.apache.cassandra.db.row.addcolumnfamily ( ) org.apache.cassandra.db.columnfamilystore.getrangeslice ( ) org.apache.cassandra.service.rangesliceverbhandler.doverb ( ) ... 4 error - fatal exception thread thread [ , main ] java.lang.runtimeexception : java.lang.nullpointerexception org.apache.cassandra.service.rangesliceverbhandler.doverb ( ) org.apache.cassandra.net.messagedeliverytask.run ( ) java.util.concurrent.threadpoolexecutor $ worker.runtask ( ) java.util.concurrent.threadpoolexecutor $ worker.run ( ) java.lang.thread.run ( ) caused : java.lang.nullpointerexception org.apache.cassandra.db.row.addcolumnfamily ( ) org.apache.cassandra.db.columnfamilystore.getrangeslice ( ) org.apache.cassandra.service.rangesliceverbhandler.doverb ( ) ... 4 11/25/2009 thread \\n2009-11-25_17:00:45.31534 \\n2009-11-25_17:00:45.31534 java.io.eofexception \\n2009-11-25_17:00:45.31534 java.io.datainputstream.readfully ( ) \\n2009-11-25_17:00:45.31534 java.io.datainputstream.readutf ( ) \\n2009-11-25_17:00:45.31534 java.io.datainputstream.readutf ( ) \\n2009-11-25_17:00:45.31534 org.apache.cassandra.db.columnfamilyserializer.readcomparator ( ) \\n2009-11-25_17:00:45.31534 org.apache.cassandra.db.columnfamilyserializer.deserialize ( ) \\n2009-11-25_17:00:45.31534 org.apache.cassandra.db.rowserializer.deserialize ( ) \\n2009-11-25_17:00:45.31534 org.apache.cassandra.db.readresponseserializer.deserialize ( ) \\n2009-11-25_17:00:45.31534 org.apache.cassandra.db.readresponseserializer.deserialize ( ) \\n2009-11-25_17:00:45.31534 org.apache.cassandra.service.readresponseresolver.isdatapresent ( ) \\n2009-11-25_17:00:45.31534 org.apache.cassandra.service.quorumresponsehandler.response ( ) \\n2009-11-25_17:00:45.31534 org.apache.cassandra.net.responseverbhandler.doverb ( ) \\n2009-11-25_17:00:45.31534 org.apache.cassandra.net.messagedeliverytask.run ( ) \\n2009-11-25_17:00:45.31534 java.util.concurrent.threadpoolexecutor.runworker ( ) \\n2009-11-25_17:00:45.31534 java.util.concurrent.threadpoolexecutor $ worker.run ( ) \\n2009-11-25_17:00:45.31534 java.lang.thread.run ( ) ' 'the comparator problem changing row internals affects commit log . simplest wipe commitlogs otherwise go back old version nodeprobe flush clear patching\\n\\nworking fix npe ' 'updated patch 03 fix latest npe ' ' think ! thank much ! ' 'committed ' 'integrated cassandra # 269 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/269/ ] ) \\n allow serializing null cf ; add get_range_slice test exercising this\\npatch jbellis ; tested dan di spaltro \\nmake row contain single final cf reference\\npatch jbellis ; tested dan di spaltro \\nr/m unused row code move table variable callers rather serializing redundantly\\npatch jbellis ; tested dan di spaltro \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>crash caused insufficient disk space flush times seen cassandra nodes crash running memory . starts following exception : { noformat } error [ ] 2013-05-31 cassandradaemon.java ( line 164 ) exception thread thread [ , main ] java.lang.runtimeexception : insufficient disk space write 8042730 bytes org.apache.cassandra.io.util.diskawarerunnable.runmaythrow ( ) org.apache.cassandra.utils.wrappedrunnable.run ( ) java.util.concurrent.threadpoolexecutor.runworker ( ) java.util.concurrent.threadpoolexecutor $ worker.run ( ) java.lang.thread.run ( ) { noformat } , seems memtablepostflusher stage gets stuck memtables get flushed : { noformat } info [ ] 2013-05-31 statuslogger.java ( line 68 ) memtablepostflusher 1 32 0 info [ ] 2013-05-31 statuslogger.java ( line 73 ) compactionmanager 1 2 { noformat } makes ridiculous , time , data directory node 981gb free disk space ( reported du ) . primarily use stcs time aforementioned exception occurred , least one compaction task executing could easily involved 981gb ( ) worth input sstables . correct wrong cassandra counts data currently compacted available disk space . case , significant overestimation space required compaction since large portion data compacted expired overwrite . point though , cassandra crash disk space unless really actually disk space ( ie , dont consider 'phantom ' compaction disk usage flushing ) . seen one nodes die way alerts disk space even went . 2786 ms 1 collections 230275096 used ; max 8248098816\\n info [ ] 2013-08-07 250 statuslogger.java ( line 53 ) pool name active pending blocked\\n info [ ] 2013-08-07 291 statuslogger.java ( line 68 ) memtablepostflusher 1 1 0\\n info [ ] 2013-08-07 292 statuslogger.java ( line 68 ) flushwriter 0 0 0\\n info [ ] 2013-08-07 293 statuslogger.java ( line 68 ) commitlog_archiver 0 0 0\\n info [ ] 2013-08-07 294 statuslogger.java ( line 73 ) compactionmanager 0 0\\n info [ ] 2013-08-07 522 statuslogger.java ( line 85 ) messagingservice n/a 0 0\\n info [ ] 2013-08-07 523 statuslogger.java ( line 95 ) cache type size capacity keystosave provider\\n info [ ] 2013-08-07 524 statuslogger.java ( line 96 ) keycache 0 104857600 \\n info [ ] 2013-08-07 525 statuslogger.java ( line 102 ) rowcache 0 0 org.apache.cassandra.cache.serializingcacheprovider\\n info [ ] 2013-08-07 525 statuslogger.java ( line 109 ) columnfamily memtable ops data\\n info [ ] 2013-08-07 526 statuslogger.java ( line 112 ) system.local 0 0\\n info [ ] 2013-08-07 526 statuslogger.java ( line 112 ) system.peers 0 0\\n info [ ] 2013-08-07 527 statuslogger.java ( line 112 ) system.batchlog 0 0\\n info [ ] 2013-08-07 527 statuslogger.java ( line 112 ) system.nodeidinfo 0 0\\n info [ ] 2013-08-07 528 statuslogger.java ( line 112 ) system.locationinfo 0 0\\n info [ ] 2013-08-07 528 statuslogger.java ( line 112 ) system.schema 0 0\\n info [ ] 2013-08-07 529 statuslogger.java ( line 112 ) system.migrations 0 0\\n info [ ] 2013-08-07 529 statuslogger.java ( line 112 ) system.schema_keyspaces 8 251\\n info [ ] 2013-08-07 530 statuslogger.java ( line 112 ) system.schema_columns 398 24717\\n info [ ] 2013-08-07 530 statuslogger.java ( line 112 ) system.schema_columnfamilies 369 22187\\n info [ ] 2013-08-07 530 statuslogger.java ( line 112 ) system.indexinfo 0 0\\n info [ ] 2013-08-07 531 statuslogger.java ( line 112 ) system.range_xfers 0 0\\n info [ ] 2013-08-07 531 statuslogger.java ( line 112 ) system.peer_events 0 0\\n info [ ] 2013-08-07 532 statuslogger.java ( line 112 ) system.hints 0 0\\n info [ ] 2013-08-07 532 statuslogger.java ( line 112 ) system.hintscolumnfamily 0 0\\n info [ ] 2013-08-07 532 statuslogger.java ( line 112 ) system_traces.sessions 0 0\\n info [ ] 2013-08-07 533 statuslogger.java ( line 112 ) system_traces.events 0 0\\n '' 'after cassandra-4292 flushing checks space reserved compactions . check bunch pending compactions huge size . ' 'have seen lot instances people hitting reported recently . think probably shouldn\\'t fail stuff c * `` thinks '' might enough free space stuff reserved . probably good idea use reserving information hint drive pick jbod case drive fit data looking reserved pick one free space ( like used ) . ' 'patchset avoid prematurely declaring space https : //github.com/jbellis/cassandra/commits/5605 ' '+1 patch.\\n\\nthough ( maybe separate ticket ? ) still need add error handler flushrunnable postexecutor get blocked . ' `` bq . still need add error handler flushrunnable postexecutor get blocked\\n\\ni 'm sure want unblock -- flush errors definitely n't want commitlog segments getting cleaned . mind ? '' 'ah right.\\njust wondered something preventing filling postexecutor queue . ' `` committed . come good idea let 's open new ticket . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>ec2mrs ignores broadcast_rpc_address setting cassandra.yaml ec2mrs ignores broadcast_rpc_address setting cassandra.yaml . problematic users using ec2mrs internal rpc_address change introduced [ cassandra-5899|https : //issues.apache.org/jira/browse/cassandra-5899 ] , change results ec2mrs always using public ip regardless user set broadcast_rpc_address . private ip public ip ( since routable private ) . cassandra-5899 broadcasted { { rpc_address } } defaulted { { listen_address } } typically set private ip ec2 deployments . cassandra-5899 added ability choose ip broadcast via { { broadcast_rpc_address } } also changed { { ec2multiregionsnitch } } * always * broadcast public ip regardless { { broadcast_rpc_address } } makes impossible nodes advertise private ip client connections want to.\\n\\nthis patch updates { { ec2multiregionsnitch } } set { { broadcast_rpc_address } } public ip property unset allowing operators overide private ip want . \\n\\nbefore { { databasedescriptor } } setting { { broadcastrpcaddress = rpcaddress } } impossible know { { broadcastrpcaddress == null } } order decide whether override property { { ec2multiregionsnitch } } modified uses { { databasedescriptor.getbroadcastrpcaddress ( ) } } use { { fbutilities.getbroadcastrpcaddress ( ) } } instead fallback { { databasedescriptor.getrpcaddress ( ) } } { { databasedescriptor.getbroadcastrpcaddress ( ) == null } } .\\n\\npatch tests available : \\n\\n||2.2||3.0||3.9||trunk||\\n| [ branch|https : //github.com/apache/cassandra/compare/cassandra-2.2 ... ] | [ branch|https : //github.com/apache/cassandra/compare/cassandra-3.0 ... ] | [ branch|https : //github.com/apache/cassandra/compare/cassandra-3.9 ... ] | [ branch|https : //github.com/apache/cassandra/compare/trunk ... pauloricardomg : trunk-11356 ] |\\n| [ testall|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-2.2-11356-testall/lastcompletedbuild/testreport/ ] | [ testall|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-3.0-11356-testall/lastcompletedbuild/testreport/ ] | [ testall|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-3.9-11356-testall/lastcompletedbuild/testreport/ ] | [ testall|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-trunk-11356-testall/lastcompletedbuild/testreport/ ] |\\n| [ dtest|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-2.2-11356-dtest/lastcompletedbuild/testreport/ ] | [ dtest|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-3.0-11356-dtest/lastcompletedbuild/testreport/ ] | [ dtest|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-3.9-11356-dtest/lastcompletedbuild/testreport/ ] | [ dtest|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-trunk-11356-dtest/lastcompletedbuild/testreport/ ] |\\n\\ncould look [ ~thobbs ] ? thanks ! ' 'overall patch looks good . think need couple doc updates though . description change behavior news.txt would good . 3.9 trunk { { doc/source/operating/snitch.rst } } could updated explain behavior well.\\n\\n [ ~jasobrown ] former user ec2 snitch time make quick review well ? ' `` updated { { news.txt } } added following note { { doc/source/operating/snitch.rst } } 3.9 trunk : \\n\\nbq . default ec2multiregionsnitch advertises public instance ip `` rpc_address `` allowing cross-dc discovery token-aware clients may incur additional charges ec2 public ip access within local dc token-aware client used . order override behavior restrict token-aware clients local dc set `` broadcast_rpc_address `` instance 's private ip.\\n\\nafter writing wondered provide option { { ec2multiregionsnitch } } thought ultimately fix limitation adding { { broadcast_rpc_address } } column { { system.peers } } table drivers would pick either private { { rpc_address } } public { { broadcast_rpc_address } } depending client located probably fix another ticket . wdyt ? '' 'another ticket would definitely best . ' 'agreed . ' `` looks like test runs problematic 've restarted get clearer results.\\n\\n [ ~jasobrown ] last call adding second review : ) '' `` [ ~thobbs ] sorry missed batcall last several days . try get next 24-48 hours 's ok. '' 'cool would fine . ' `` 'm +1 patch except two minor issues : \\n\\n- 2.2 critical-fixes mode ? let 's commit 3.0 higher\\n- add couple ( simple ) tests { { fbutliities # getbroadcastrpcaddress } } ? easily see functionality correct * * 'm worried future if/when move things around accidentally break things . '' `` thanks review jason . nice call unit test updated patch testing { { fbutilities # getbroadcastrpcaddress } } resubmitted tests.\\n\\nregarding commit 2.2 think 're yet critical-fixes-only-mode 2.2 please correct 'm wrong . think somewhat critical sense 're prohibited setting private { { rpc_address } } using { { ec2mrs } } valid case possible cassandra-5899 . '' `` [ ~pauloricardomg ] thanks adding tests : ) \\n\\ni checked irc 're still good adding 2.2 'm +1 tests complete/pass . '' `` 'm also +1 committing 2.2. let know tests pass 'll get committed . '' 'test results look good marking ready commit . thanks ! ' 'great committed { { 91f7387e1f785b18321777311a5c3416af0663c2 } } 2.2 merged .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>errors reading bootstrapping loaded 4 node cluster 1m rows stress.py , decommissioned node , began bootstrapping performing constant reads others stress.py . sleeping 90s , bootstrapping node started throwing many errors like : error fatal exception thread thread [ read_stage:1270,5 , main ] java.lang.runtimeexception : service reads bootstrapping ! org.apache.cassandra.db.readverbhandler.doverb ( ) org.apache.cassandra.net.messagedeliverytask.run ( ) java.util.concurrent.threadpoolexecutor.runworker ( ) java.util.concurrent.threadpoolexecutor $ worker.run ( ) java.lang.thread.run ( ) began receiving timeout errors stress.py . [ `` bootstrap-after-decom ever worked ? think leave data system table 's going confuse things '' `` rm 'd everything decom . '' 'does bs vanilla no-decom cluster work ? ' 'no problem . ' 'patch attached makes new node properly announce bootstrap status . ' '+1 ' 'committed ' 'integrated cassandra # 545 ( see [ https : //hudson.apache.org/hudson/job/cassandra/545/ ] ) \\n fix setting bootstrap status startup.\\npatch jbellis ; reviewed brandonwilliams cassandra-1534\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>multislicetest.test_with_overlap * unit tests failing trunk example : https : //cassci.datastax.com/job/trunk_utest/623/testreport/org.apache.cassandra.thrift/multislicetest/ 1 ) initial sorting finish bound special cased start bound automatically handled comparator 2 ) later finish bound also needs special casing testing inclusion . covered additional unit tests . ' `` 'd comfortable asserting ( always regardless assertions enabled ) thrift path keep patch simple . multi slices new thing thrift world constraining sensibly ( inputs n't massage make sense ) seems reasonable . possible point contention would two ranges equal end/starts would reject easy understand meant . n't think 're severe casualty though.\\n\\nit 'd nice take opportunity simultaneously clean absc code longer enforce assumption 're imposing elsewhere.\\n\\nalso 's least one spot constructor called n't covered [ ~slebresne ] 's patch 'd suggest either moving assert constructor creating static method construction requires stipulating assert always enforced ( thrift ) assertions enabled . 'm little concerned easily introduce new code paths use incorrectly wo n't covered assertions stands . '' `` bq . 's least one spot constructor called n't covered sylvain lebresne 's patch\\n\\nthere one spots n't add assertion ones easy verify things ok construction . 'm fine putting assertion everywhere 're really freaked bug though ( happen n't share fear ) . `` 'looks good.\\n\\nthe actual result still different original result result merging slices pre-count.\\n { code } \\nsetcount ( 6 ) \\n .... \\n req.setcolumn_slices ( arrays.aslist ( columnslicefrom ( `` e '' `` '' ) columnslicefrom ( `` g '' `` '' ) ) ) ; \\n- assertcolumnnamematches ( arrays.aslist ( `` g '' `` e '' `` '' `` c '' `` b '' `` '' ) server.get_multi_slice ( req ) ) ; \\n+ assertcolumnnamematches ( arrays.aslist ( `` g '' `` f '' `` e '' `` '' `` c '' `` b '' ) server.get_multi_slice ( req ) ) ; \\n } \\n { code } \\n\\neven though cases changed test match results . code written make sense long run . ' `` 'm neutral asserts 're adding think consistent especially computationally cheap even enabled ( free disabled ) . cql3casconditions n't look ( at-a-glance- ) trivial would definitely produce safe slices. `` `` ok +1 'll backport 2.1 commit . thanks . '' `` bq . cql3casconditions n't look ( at-a-glance- ) trivial would definitely produce safe slices.\\n\\nit 's creating slices using { { slice ( ) } } method composites coming map sorted comparator . 'm happy adding assertion ( suppose jake chime backport ) . '' 'looks like tests org.apache.cassandra.cql3.multicolumnrelationtest started failing due assertion check ' 'the problem ( multicolumnrelationtest ) check columnslice assuming empty value still sort lower bound even reverse comparator always done { { abstracttype.reversecomparator } } . however somehow cassandra-5417 didn\\'t seem carried { { abstractctype.reversecomparator ( ) } } . anyway attaching v2 simply add `` proper '' handling empty { { abstractctype.reversecomparator ( ) } } . far tell breaks tests . note part won\\'t needed 2.0 backport since { { abstractype } } right thing there.\\n\\nv2 also adds assert cql3casconditions it.\\n ' 'committed thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>node restarted treated node joining ? hi , found recently every time restart node , nodes cluster treat restarted node new node joining issue node joining notification clients . traced code path hit peer node detected restarted node : src/java/org/apache/cassandra/gms/gossiper.java { code } private void handlemajorstatechange ( inetaddress ep , endpointstate epstate ) { ( ! isdeadstate ( epstate ) ) { ( endpointstatemap.get ( ep ) ! = null ) logger.info ( `` node { } restarted , '' , ep ) ; else logger.info ( `` node { } part cluster '' , ep ) ; } ( logger.istraceenabled ( ) ) logger.trace ( `` adding endpoint state `` + ep ) ; endpointstatemap.put ( ep , epstate ) ; // node restarted : subscriber take whatever action necessary ( iendpointstatechangesubscriber subscriber : subscribers ) subscriber.onrestart ( ep , epstate ) ; ( ! isdeadstate ( epstate ) ) markalive ( ep , epstate ) ; else { logger.debug ( `` marking `` + ep + `` alive due dead state '' ) ; markdead ( ep , epstate ) ; } ( iendpointstatechangesubscriber subscriber : subscribers ) subscriber.onjoin ( ep , epstate ) ; } { code } subscriber.onjoin ( ep , epstate ) ends calling onjoincluster server.java { code } src/java/org/apache/cassandra/transport/server.java public void onjoincluster ( inetaddress endpoint ) { server.connectiontracker.send ( event.topologychange.newnode ( getrpcaddress ( endpoint ) , server.socket.getport ( ) ) ) ; } { code } full trace code path skip intermedia function calls brief . upon receiving node joining notification , clients would go scan system peer table fetch latest topology information . since tens thousands client connections , scans put enormous load cluster . although newer version driver , client skips fetching peer table new node already existed local metadata , still curious node restarted handled node joining server side ? hit bug way supposed ? old java driver version 1.0.4 cassandra version 2.0.12. thanks ! pushed additional commit 2.2 branch forgot switch java 7 dev accidentally included 8ism . '' `` see dtest [ pull request|https : //github.com/riptano/cassandra-dtest/pull/983 ] fixing behavior restart node tests pushed_notifications_test.py . may also want add new dtest check new_node removed_node sent node joins leaves respectively n't think test specific moment . '' 'thanks everyone fixing issue ! ' `` 've rebased ( fix dtest failures ) kicked another set ci runs . \\nftr dtest jobs using [ branch|https : //github.com/beobal/cassandra-dtest/tree/11731 ] also includes [ ~stefania ] 's tests cassandra-11731.\\n\\n||branch||testall||dtest||\\n| [ 11038-2.2|https : //github.com/beobal/cassandra/tree/11038-2.2 ] | [ testall|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-11038-2.2-testall ] | [ dtest|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-11038-2.2-dtest ] |\\n| [ 11038-3.0|https : //github.com/beobal/cassandra/tree/11038-3.0 ] | [ testall|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-11038-3.0-testall ] | [ dtest|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-11038-3.0-dtest ] |\\n| [ 11038-3.7|https : //github.com/beobal/cassandra/tree/11038-3.7 ] | [ testall|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-11038-3.7-testall ] | [ dtest|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-11038-3.7-dtest ] |\\n| [ 11038-trunk|https : //github.com/beobal/cassandra/tree/11038-trunk ] | [ testall|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-11038-trunk-testall ] | [ dtest|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-11038-trunk-dtest ] |\\n '' `` lgtm.\\n\\npatches look good testall runs look good . looks like dtests got branched bad time brief problem dtest branch - rebased dtest branch master pushed [ jkni/cassandra-dtest/11038|https : //github.com/jkni/cassandra-dtest/tree/11038 ] . ccm changes latest trunk cdc merge 've rebased trunk branch pushed [ jkni/cassandra/11038-trunk-rebase|https : //github.com/jkni/cassandra/tree/11038-trunk-rebase ] . rebase clean rebase trunk branch commit . dtest runs update clean [ 2.2|http : //cassci.datastax.com/view/dev/view/jkni/job/jkni-11038-2.2-dtest/ ] [ 3.0|http : //cassci.datastax.com/view/dev/view/jkni/job/jkni-11038-3.0-dtest/ ] [ trunk|http : //cassci.datastax.com/view/dev/view/jkni/job/jkni-11038-trunk-dtest/ ] .\\n\\nstefania pr open [ cassandra-11731 ] fixes [ here|https : //github.com/riptano/cassandra-dtest/pull/983 ] . n't see pr added test . want ok 11731 pr pr added test commit [ ~beobal ] want ? '' `` thanks ! committed 2.2 { { 142f358f6958695c4248acb94b89b64e95ccc609 } } merged 3.0/trunk . \\n\\ni 've also merged stefania 's dtest pr opened [ second|https : //github.com/riptano/cassandra-dtest/pull/1049 ] additional test . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>nodetool cleanup ks replicas remove old data , silently complete user list : https : //lists.apache.org/thread.html/5d49cc6bbc6fd2e5f8b12f2308a3e24212a55afbb441af5cb8cd4167 @ % 3cuser.cassandra.apache.org % 3e multi-dc cluster , keyspaces replicated given dc , 'll unable run cleanup keyspaces dc , [ cleanup code see ranges exit early|https : //github.com/apache/cassandra/blob/4cfaf85/src/java/org/apache/cassandra/db/compaction/compactionmanager.java # l427-l441 ] nodetool cleanup ks replicas remove old data silently complete\\n\\n -- -- \\n ' `` hi [ ~jasonstack ] really appreciate patience time 's taken back . hope review weekend.\\r\\n\\r\\n [ ~krummas ] / [ ~iamaleksey ] - folks think versions ? 2.2 3.0 ? \\r\\n\\r\\n '' `` 'd probably go 3.0+ 2.2 acceptable . '' `` 've rebased patch 'm re-running ci took long review patch.\\r\\n\\r\\ngenerally patches look fine n't understand 're running method twice [ here|https : //github.com/jasonstack/cassandra/commit/b51c46565adf0d765ac6ded831469a2eca2939d8 # diff-ba6d3d8e296151fc283ef11ac4594e62r211 ] ( similar helper ) ? \\r\\n\\r\\ni 'm inclined remove one calls . marking ready-to-commit 'll merge ci finishes.\\r\\n '' `` [ ~jjirsa ] 's mistake 3.11 pr .. thanks fix . '' 'thank much patch patience . committed 3.0 { { 090f418831be4e4dace861fda380ee4ec27cec35 } } merged fixing 3.11 test way.\\r\\n\\r\\n ' 'github user asfgit closed pull request : \\n\\n https : //github.com/apache/cassandra-dtest/pull/1\\n ' 'thanks reviewing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>get_paged_slices n't reset startcolumn first row example , consider wordcount example ( see cassandra-3883 ) . wordcountsetup inserts 1000 rows , three columns : text3 , text4 , int1 . ( miscellaneous columns inserted rows , ignore . ) paging get_paged_slice calls count 99 , cfrecordreader first retrieve 33 rows , last call k. attempt fetch 99 columns , starting row k column text4 . bug fetch text4 * * subsequent row k+i , instead returning ( k , text4 ) , ( k+1 , int1 ) , ( k+1 , int3 ) , ( k+1 , text4 ) , etc . attached patch adds ability paging multiple rows . support added patch limited get_paged_slices requires ( particular using slicequeryfilter finish ! = `` '' supported new option ) . patch contains unit test . ' 'with 3883 patches get\\n\\n { noformat } \\n $ cat /tmp/word_count5/part-r-00000\\n0 250\\n1 250\\n2 250\\n3 250\\nword1 2002\\nword2 1\\n { noformat } \\n\\nwhich expected result.\\n\\n+1 ' 'committed thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>row cache cache partitions tables without clustering keys { code } mlsea-jjirsa01 : ~ jjirsa $ ccm start mlsea-jjirsa01 : ~ jjirsa $ echo `` describe table test.test ; `` | ccm node1 cqlsh create table test.test ( id int primary key , v text ) bloom_filter_fp_chance = 0.01 caching = { 'keys ' : 'all ' , 'rows_per_partition ' : '100 ' } comment = `` compaction = { 'class ' : 'org.apache.cassandra.db.compaction.sizetieredcompactionstrategy ' , 'max_threshold ' : '32 ' , 'min_threshold ' : ' 4 ' } compression = { 'chunk_length_in_kb ' : '64 ' , 'class ' : 'org.apache.cassandra.io.compress.lz4compressor ' } crc_check_chance = 1.0 dclocal_read_repair_chance = 0.1 default_time_to_live = 0 gc_grace_seconds = 864000 max_index_interval = 2048 memtable_flush_period_in_ms = 0 min_index_interval = 128 read_repair_chance = 0.0 speculative_retry = '99percentile ' ; mlsea-jjirsa01 : ~ jjirsa $ ccm node1 nodetool info | grep row row cache : entries 0 , size 0 bytes , capacity 100 mib , 0 hits , 0 requests , nan recent hit rate , 0 save period seconds mlsea-jjirsa01 : ~ jjirsa $ echo `` insert test.test ( id , v ) values ( 1 , ' ' ) ; `` | ccm node1 cqlsh mlsea-jjirsa01 : ~ jjirsa $ echo `` select * test.test id=1 ; `` | ccm node1 cqlsh id | v -- -- + -- - 1 | ( 1 rows ) mlsea-jjirsa01 : ~ jjirsa $ ccm node1 nodetool info | grep row row cache : entries 0 , size 0 bytes , capacity 100 mib , 0 hits , 0 requests , nan recent hit rate , 0 save period seconds mlsea-jjirsa01 : ~ jjirsa $ echo `` select * test.test id=1 ; `` | ccm node1 cqlsh id | v -- -- + -- - 1 | ( 1 rows ) mlsea-jjirsa01 : ~ jjirsa $ ccm node1 nodetool info | grep row row cache : entries 0 , size 0 bytes , capacity 100 mib , 0 hits , 0 requests , nan recent hit rate , 0 save period seconds mlsea-jjirsa01 : ~ jjirsa $ { code } http : //cassci.datastax.com/job/jeffjirsa-cassandra-12499-testall/lastbuild/testreport/ http : //cassci.datastax.com/job/jeffjirsa-cassandra-12499-dtest/lastbuild/testreport/ hours . \\n\\n ' '+1 ' 'committed { { eace9aaddfdd0059f52b1eb9b6902f999f04a447 } } \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>allow strings passed jmx authentication n't make sense allow object types . ci : \\r\\n * trunk\\r\\n * * https : //ci-cassandra.apache.org/job/cassandra-devbranch/285/\\r\\n * 3.11\\r\\n * * https : //ci-cassandra.apache.org/job/cassandra-devbranch/284/\\r\\n * 3.0\\r\\n * * https : //ci-cassandra.apache.org/job/cassandra-devbranch/289/\\r\\n * 2.2\\r\\n * * https : //ci-cassandra.apache.org/job/cassandra-devbranch/288/ ' 'committed [ 63f4da90c3c51d230c535265786dbc7a33c1ace9|https : //github.com/apache/cassandra/commit/63f4da90c3c51d230c535265786dbc7a33c1ace9 ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>re-apply mv updates commitlog replay node crashes commit log update local memtable update materialized view node replica could lose mv data . really issue rf=1 since replicas likely apply successfully . case fix mv updates always applied even commit log replay ( care re-add mutations commit log ) . [ `` [ patch|https : //github.com/tjake/cassandra/tree/10164 ] \\n [ tests|http : //cassci.datastax.com/job/tjake-10164-testall/1/ ] \\n [ dtests|http : //cassci.datastax.com/job/tjake-10164-dtest/1/ ] \\n\\n * cleaned areas trying catch write timeouts submitmv since n't possible 's done locally actual view updates async.\\n\\n * added logic make mv updates cl replay write commit log ( since always flush cl replay anyway ) \\n\\n * added logic avoid cl updating mutations streamed sstable ( flush transaction complete ) \\n\\n * found/fixed little bug builder would build &gt; 128 rows partition . 'll add test ... '' 'added wide partition builder test ' '+1 ' 'committed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>convenience workflow replacing dead node replacing dead node new one common operation , `` nodetool removetoken '' followed bootstrap inefficient ( re-replicating data first remaining nodes , new one ) manually bootstrapping token `` less '' old one 's , followed `` nodetool removetoken '' slightly painful prone manual errors . first question : would expose tool ecosystem ? needs startup-time option new node , ca n't nodetool , messing config xml definitely takes `` convenience '' . one-off -dreplacetoken=xxy argument ? \\n * /cassandra/trunk/news.txt\\n * /cassandra/trunk/src/java/org/apache/cassandra/config/databasedescriptor.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/hintedhandoffmanager.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/rowmutation.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/dht/bootstrapper.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/gms/endpointstate.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/gms/gossiper.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/gms/versionedvalue.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/service/loadbroadcaster.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/service/migrationmanager.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/service/storageproxy.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/service/storageservice.java\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>selectstatement.parameters fields inspectable custom indexes query handlers selectstatement.parameters fields inspectable custom indexes query handlers https : //github.com/jeremiahdjordan/cassandra/tree/c9858-21\\nhttps : //github.com/jeremiahdjordan/cassandra/tree/c9858-22\\nhttps : //github.com/jeremiahdjordan/cassandra/tree/c9858-30\\n ' 'committed 2.1 { { e726cf6d6b1a21abd0b7cf35775b7a980b1009ed } } merged 2.2 trunk thanks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>clean gossip notifications rest system description [ `` add onalive ondead methods gossiper listener interface listeners n't try infer less accurate circumstantial evidence\\n '' 'update convict well suspect ' 'looks ok . +1 ' 'it looks like gossiper.instance ( ) .register ( _ ) called single thread since isalive synchronized register/unregister probably well . ' ' commit change think could use intensive thread safety analysis . ( another ticket . ) ' `` integrated cassandra # 249 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/249/ ] ) \\n add synchronized register/unregister methods remove unused code . patch stu hood jbellis \\nadd onalive ondead methods gossiper listener interface listeners n't try infer less accurate circumstantial evidence\\npatch jbellis ; reviewed jaakko laine \\n '' 'committed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>column family expose count metrics dropped mutations . please take look discussion cassandra-10580 . opened latency dropped mutations exposed metric column families . [ `` n't see much sense exposing latency metrics dropped mutations per-cf basis since wo n't vary much per-cf basis rather per-node basis ( also internal vs cross node ) global dropped mutation latency metric suffice tune node timeouts accordingly . keeping count dropped mutations per cf may make sense though may want monitor distribution dropped mutations across different cfs . '' 'that makes sense . updating title reflect . ' `` provided patch top https : //issues.apache.org/jira/secure/attachment/12777927/trunk-all-comments.patch\\n\\ni add unit tests messaging service see pattern checking metrics thorough unit tests skipping . change verified local installation visual vm . \\n\\nit possible optimize ks/cf lookup messagingservice.java ( maybe map ) hoping 's necessary . '' 'any updates ? ' `` thanks patch . comments : \\n- please rebase latest trunk.\\n- { { messagingservice.updatedroppedmutationcount } } use { { keyspace.open ( mutation.getkeyspacename ( ) ) .getcolumnfamilystore ( uuid ) } } fetch cfs instead iterating { { columnfamilystore.all ( ) } } also perform null check cfs null ( table dropped example ) .\\n- { { updatedroppedmutationcount ( messagein message ) } } need check { { message.payload instanceof collection &lt; ? &gt; } } since { { droppable_verbs } } operates collection mutations.\\n- { { storageproxy.performlocally } } add { { optional &lt; mutation &gt; } } argument receives { { optional.absent ( } } 's mutation . similarly { { localmutationrunnable } } receive { { optional &lt; mutation &gt; } } count { { ! mutationopt.isempty ( ) } } \\n- { { tablestats } } removed { { maximum tombstones per slice } } metric mistake . '' `` thanks . included collection realize schema_ * verb n't part droppable_verbs . good point.\\n\\ni 'll submit rebased patch shortly . '' 'attached 10866-trunk.patch . ' `` thanks patch . 's looking better . 'll need print { { droppedmutations } } metric { { tablestats } } command verify 's correctly displayed { { nodetool tablestats } } command . also { { localmutationrunnable } } 'll need make { { mutationopt } } field final initialize empty constructor ( otherwise { { nullpointerexception } } .\\n\\nsome style nits : \\n * remove { { updatedroppedmutationcount ( messagein message ) } } ( since 's used ) instead conditional { { message.payload instanceof imutation } } check { { incrementdroppedmessages ( messagein message long timetaken ) } } call { { updatedroppedmutationcount ( ( imutation ) message.payload ) } } there.\\n * { { localmutationrunnable } } pass optional { { messagingservice.incrementdroppedmutations } } perform conditional there.\\n * { { updatedroppedmutationcount ( imutation mutation ) } } replace null mutation check assertion since never null.\\n * * also need create variable { { columnfamilyids } } iterate loop directly { { mutation.getcolumnfamilyids ( ) } } .\\n * use meaningful commit message '' 'please submit patch addressed . ' 'attached . please take look get chance ! ' 'any updates ? ' `` lgtm let 's wait test results marking ready commit.\\n\\n||trunk||\\n| [ branch|https : //github.com/apache/cassandra/compare/trunk ... pauloricardomg : trunk-10686 ] |\\n| [ testall|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-trunk-10686-testall/lastcompletedbuild/testreport/ ] |\\n| [ dtest|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-trunk-10686-dtest/lastcompletedbuild/testreport/ ] | '' 'tests look good . marking ready commit . ' 'thanks . ' 'looks like needs rebased following cassandra-10477 . ' 'rebased . ' 'thanks [ ~anubhavk ] [ ~pauloricardomg ] . committed [ { { 66d3428 } } |https : //git1-us-west.apache.org/repos/asf ? p=cassandra.git ; a=commit ; h=66d3428e3fe64851fa7587ee69b53e20bb7c09b5 ] .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>gossip throws illegalstateexception starting second node , gossip throws illegalstateexception ks rf &gt; 1 defined existing 1-node cluster . able define keyspaces cluster size . bring nodes load schema . ' 'on second thought think right solution disallow ks creation number live nodes support replication factor . ' '+1 ' `` integrated cassandra # 509 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/509/ ] ) \\n complain n't enough nodes support requested rf . patch gdusbabek reviewed jbellis . cassandra-1343\\n '' `` something similar still happening beta1\\ni 've opened ticket results error message however believe different since n't happen trying create ks bringing new node : \\nhttps : //issues.apache.org/jira/browse/cassandra-1467 '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>uncompressed sizes used estimate space compaction compressed sstables using uncompressed data size estimating enough compact sstables . means easily refuse compaction clearly enough room compact . maybe rename .length ( ) uncompressedlength ( ) well ? \\n\\notherwise +1 ' 'actually renaming length ( ) uncompressedlength ( ) proved good idea missed quite bunch places ondisklength used . attached v2 . ' '+1 ' 'committed thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>add shutdownhook flush commitlog replaces periodic_with_flush approach cassandra-1780 / cassandra-1917 could create serversocket address /127.0.0.1:9170.\\n [ junit ] \\tat org.apache.thrift.transport.tserversocket. &lt; init &gt; ( ) \\n [ junit ] \\tat org.apache.thrift.transport.tserversocket. &lt; init &gt; ( ) \\n [ junit ] \\tat org.apache.cassandra.thrift.tcustomserversocket. &lt; init &gt; ( ) \\n [ junit ] \\tat org.apache.cassandra.thrift.cassandradaemon.setup ( ) \\n { noformat } \\n\\nfirst test runs fine second errors ( 'm guessing error message ) first jvm still hanging around third test n't start even long junit timeout . '' 'aha batch cl executor shutdown thread created started . v2 attached.\\n\\nclitest still failing though error along embeddedcassandraservicetest . sure tell junit `` wait previous jvm exit completely starting next test . '' ' `` +1 . \\n\\ni 'm beginning think something introduced ssl patch altered behavior sockets . 've seen odd socket errors twice last days running unit tests think removetest . fwiw n't see errors running tests patch . '' `` bq . 'm beginning think something introduced ssl patch altered behavior sockets\\n\\nnow 'm getting clitest failures w/o patch . think might something . '' `` hmm n't quite right drain similar want ( flushing every cf could take ) . '' 'v3 shuts mutation stage + commitlog hook ' '+1 ' 'committed ' 'integrated cassandra-0.7 # 216 ( see [ https : //hudson.apache.org/hudson/job/cassandra-0.7/216/ ] ) \\n add jvm shutdownhook sync commitlog\\npatch jbellis ; reviewed gdusbabek cassandra-1919\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>sstablereader.loadsummary may leave open file { { sstablereader.loadsummary } } catches _ioexception_ tries delete { { summariesfile } } , { { istream } } still open file locked , { { fileutils.deletewithconfirm } } fails , least windows attaching patch close stream unlock file ' `` 'finally ' block close istream think n't problem current code . '' `` lgtm ; committed\\n\\n ( n't see yuki 's comment . problem catch block runs finally tries delete close works linux windows . ) '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>cassandra 3.10 : classcastexception threadawaresecuritymanager https : //www.mail-archive.com/user @ cassandra.apache.org/msg51603.html ch/qos/logback/classic/logger\\r\\n\\xa0\\xa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>upgrade thrift 0.6 description [ `` regen thrift code v1-0002 patch n't apply.\\n\\notherwise +1 '' 'committed . ' 'integrated cassandra # 838 ( see [ https : //hudson.apache.org/hudson/job/cassandra/838/ ] ) \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>duplicate rows returned clause repeated values value repeated within clause repeated rows returned repeats : cqlsh &gt; create table t1 ( c1 text primary key ) ; cqlsh &gt; insert t1 ( c1 ) values ( ' ' ) ; cqlsh &gt; select * t1 ; c1 -- -- cqlsh &gt; select * t1 c1 = ' ' ; c1 -- -- cqlsh &gt; select * t1 c1 ( ' ' ) ; c1 -- -- cqlsh : dslog &gt; select * t1 c1 ( ' ' , ' ' ) ; c1 -- -- [ `` kind intended behavior . best behavior ? n't know though 'm sure matters much practice tbh . order resulting rows following order values ( unless explicit ordering takes precedence course ) kind suggest consider values list rather set perspective 's probably entirely crazy return duplicate results case . particular use prepared marker server expect list set values ( changing would really break users ) . 's easy enough avoid duplication client side n't want duplicates.\\n\\ndo n't get wrong 'm saying returning duplicate case would inferior rather n't see big problem current behavior 'd rather introduce breaking change even small one good reason . '' 'if performing calculation summing results answer would wrong . \\ni suppose arguable know better put duplicates clause bit generating query aggregating selection parameters separate sources.\\nmy background much extensive relational databases postgres ( take preferred example ) would get back one row query terms correctness absolutely would expect . ' `` [ ~slebresne ] closed wo n't fix problem left open ? '' 'irc discussion shows desire fix 3.0 warn 2.1 . ' 'no dba developer familiar sql expect behavior . 100 % find unexpected many conclude bug.\\n\\ni understand cql sql people expect similar semantics . ' 'this behavior apparently already changed trunk { { selectstatement } } refactoring cassandra-7981 . ' 'the patch 2.1 make sure cassandra log warning first time user execute query containing restriction duplicate values primary key.\\n\\nthe patch trunk add unit tests check behavior fix { { changes.txt } } files . ' ' [ ~snazy ] review ? ' 'sure : ) ' '+1 2.1 patch . triggers warning example ticket description.\\n\\nthe trunk patch tests duplicate values partition key.\\nit checks duplicate values clustering key ( even 2.1 return duplicates ck duplicates ) .\\nyou simply add assertions like { { select * % k1 ( ? ? ) k2 = ? } } { { select * % k1 ( ? ? ) k2 ( ? ? ) } } . ' 'adds missing tests . ' '+1\\n\\ncommitted 0c2eaa9 ( 2.1 ) + 732986b ( trunk merge-commit )</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>logging : info log displaying number rows read saved cache startup part commit revision c9270f4e info logging number rows read saved cache working . happening incrementing counter cachedrowsread columnfamilystore.initrowcache ( ) . small change . ' 'committed thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>nodetool repair triggers oom customer 3 node cluster 500mb data node { noformat } [ cassandra @ nbcqa-chc-a02 ~ ] $ nodetool status note : ownership information include topology ; complete information , specify keyspace datacenter : ch2 =============== status=up/down |/ state=normal/leaving/joining/moving -- address load tokens owns host id rack un 162.150.4.234 255.26 mb 256 33.2 % 4ad1b6a8-8759-4920-b54a-f059126900df rac1 un 162.150.4.235 318.37 mb 256 32.6 % 3eb0ec58-4b81-442e-bee5-4c91da447f38 rac1 un 162.150.4.167 243.7 mb 256 34.2 % 5b2c1900-bf03-41c1-bb4e-82df1655b8d8 rac1 [ cassandra @ nbcqa-chc-a02 ~ ] $ { noformat } run repair command , system runs oom 45 minutes nothing else running { noformat } [ cassandra @ nbcqa-chc-a02 ~ ] $ date fri sep 19 utc 2014 [ cassandra @ nbcqa-chc-a02 ~ ] $ nodetool repair -st -9220354588320251877 -et -9220354588320251873 sep 19 , 2014 pm clientcommunicatoradmin checker-run warning : failed check connection : java.net.sockettimeoutexception : read timed { noformat } run oom { noformat } error [ ] 2014-09-19 cassandradaemon.java ( line 199 ) exception thread thread [ , main ] java.lang.outofmemoryerror : java heap space org.apache.cassandra.io.util.randomaccessreader. &lt; init &gt; ( ) org.apache.cassandra.io.compress.compressedrandomaccessreader. &lt; init &gt; ( ) org.apache.cassandra.io.compress.compressedrandomaccessreader.open ( ) org.apache.cassandra.io.util.compressedpoolingsegmentedfile.createreader ( ) org.apache.cassandra.io.util.poolingsegmentedfile.getsegment ( ) org.apache.cassandra.io.sstable.sstablereader.getfiledatainput ( ) org.apache.cassandra.db.columniterator.simpleslicereader. &lt; init &gt; ( ) org.apache.cassandra.db.columniterator.sstablesliceiterator.createreader ( ) org.apache.cassandra.db.columniterator.sstablesliceiterator. &lt; init &gt; ( ) org.apache.cassandra.db.filter.slicequeryfilter.getsstablecolumniterator ( ) org.apache.cassandra.db.filter.queryfilter.getsstablecolumniterator ( ) org.apache.cassandra.db.collationcontroller.collectalldata ( ) org.apache.cassandra.db.collationcontroller.gettoplevelcolumns ( ) org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns ( ) org.apache.cassandra.db.columnfamilystore.getcolumnfamily ( ) org.apache.cassandra.db.keyspace.getrow ( ) org.apache.cassandra.db.slicefromreadcommand.getrow ( ) org.apache.cassandra.service.storageproxy $ localreadrunnable.runmaythrow ( ) org.apache.cassandra.service.storageproxy $ droppablerunnable.run ( ) java.util.concurrent.threadpoolexecutor.runworker ( unknown source ) java.util.concurrent.threadpoolexecutor $ worker.run ( unknown source ) java.lang.thread.run ( unknown source ) { noformat } cassandra process pegs 1 8 cpu 's 100 % { noformat } top - 11 days , , 2 users , load average : 0.54 , 0.60 , 0.65 tasks : 175 total , 1 running , 174 sleeping , 0 stopped , 0 zombie cpu0 : 0.0 % us , 0.0 % sy , 0.0 % ni,100.0 % id , 0.0 % wa , 0.0 % hi , 0.0 % si , 0.0 % st cpu1 :100.0 % us , 0.0 % sy , 0.0 % ni , 0.0 % id , 0.0 % wa , 0.0 % hi , 0.0 % si , 0.0 % st cpu2 : 0.0 % us , 0.0 % sy , 0.0 % ni,100.0 % id , 0.0 % wa , 0.0 % hi , 0.0 % si , 0.0 % st cpu3 : 0.7 % us , 0.3 % sy , 0.0 % ni , 99.0 % id , 0.0 % wa , 0.0 % hi , 0.0 % si , 0.0 % st cpu4 : 0.3 % us , 0.3 % sy , 0.0 % ni , 99.3 % id , 0.0 % wa , 0.0 % hi , 0.0 % si , 0.0 % st cpu5 : 0.3 % us , 0.3 % sy , 0.0 % ni , 99.0 % id , 0.0 % wa , 0.0 % hi , 0.0 % si , 0.3 % st cpu6 : 0.0 % us , 0.3 % sy , 0.0 % ni , 99.7 % id , 0.0 % wa , 0.0 % hi , 0.0 % si , 0.0 % st cpu7 : 0.3 % us , 0.3 % sy , 0.0 % ni , 99.3 % id , 0.0 % wa , 0.0 % hi , 0.0 % si , 0.0 % st mem : 16332056k total , 16167212k used , 164844k free , 149956k buffers swap : 0k total , 0k used , 0k free , 8360056k cached pid user pr ni virt res shr % cpu % mem time+ command 2161 cassandr 20 0 11.5g 6.9g 227m 107.8 44.0 java 9942 root 20 0 106m 2320 1344 1.0 0.0 dhclient-script 28969 opscente 20 0 4479m 188m 12m 0.7 1.2 java 5123 cassandr 20 0 1788m 107m 28m 0.3 0.7 java 1 root 20 0 19228 1352 1072 0.0 0.0 init 2 root 20 0 0 0 0 0.0 0.0 kthreadd 3 root rt 0 0 0 0 0.0 0.0 migration/0 4 root 20 0 0 0 0 0.0 0.0 ksoftirqd/0 5 root rt 0 0 0 0 0.0 0.0 migration/0 6 root rt 0 0 0 0 0.0 0.0 watchdog/0 7 root rt 0 0 0 0 0.0 0.0 migration/1 8 root rt 0 0 0 0 0.0 0.0 migration/1 9 root 20 0 0 0 0 0.0 0.0 ksoftirqd/1 10 root rt 0 0 0 0 0.0 0.0 watchdog/1 11 root rt 0 0 0 0 0.0 0.0 migration/2 12 root rt 0 0 0 0 0.0 0.0 migration/2 13 root 20 0 0 0 0 0.0 0.0 ksoftirqd/2 14 root rt 0 0 0 0 0.0 0.0 watchdog/2 15 root rt 0 0 0 0 0.0 0.0 migration/3 16 root rt 0 0 0 0 0.0 0.0 migration/3 17 root 20 0 0 0 0 0.0 0.0 ksoftirqd/3 18 root rt 0 0 0 0 0.0 0.0 watchdog/3 19 root rt 0 0 0 0 0.0 0.0 migration/4 20 root rt 0 0 0 0 0.0 0.0 migration/4 21 root 20 0 0 0 0 0.0 0.0 ksoftirqd/4 22 root rt 0 0 0 0 0.0 0.0 watchdog/4 23 root rt 0 0 0 0 0.0 0.0 migration/5 { noformat } 12gb heapdump , memory leak suspects show following trace { noformat } thread stack rmi tcp connection ( 1621 ) -162.150.4.235 org.apache.cassandra.service.storageservice.createrepairrangefrom ( ljava/lang/string ; ljava/lang/string ; ) ljava/util/collection ; ( ) org.apache.cassandra.service.storageservice.forcerepairrangeasync ( ljava/lang/string ; ljava/lang/string ; ljava/lang/string ; zljava/util/collection ; ljava/util/collection ; [ ljava/lang/string ; ) ( ) sun.reflect.nativemethodaccessorimpl.invoke0 ( ljava/lang/reflect/method ; ljava/lang/object ; [ ljava/lang/object ; ) ljava/lang/object ; ( native method ) sun.reflect.nativemethodaccessorimpl.invoke ( ljava/lang/object ; [ ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) sun.reflect.delegatingmethodaccessorimpl.invoke ( ljava/lang/object ; [ ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) java.lang.reflect.method.invoke ( ljava/lang/object ; [ ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) sun.reflect.misc.trampoline.invoke ( ljava/lang/reflect/method ; ljava/lang/object ; [ ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) sun.reflect.generatedmethodaccessor8.invoke ( ljava/lang/object ; [ ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) sun.reflect.delegatingmethodaccessorimpl.invoke ( ljava/lang/object ; [ ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) java.lang.reflect.method.invoke ( ljava/lang/object ; [ ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) sun.reflect.misc.methodutil.invoke ( ljava/lang/reflect/method ; ljava/lang/object ; [ ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2 ( ljava/lang/reflect/method ; ljava/lang/object ; [ ljava/lang/object ; ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2 ( ljava/lang/object ; ljava/lang/object ; [ ljava/lang/object ; ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) com.sun.jmx.mbeanserver.mbeanintrospector.invokem ( ljava/lang/object ; ljava/lang/object ; [ ljava/lang/object ; ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) com.sun.jmx.mbeanserver.perinterface.invoke ( ljava/lang/object ; ljava/lang/string ; [ ljava/lang/object ; [ ljava/lang/string ; ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) com.sun.jmx.mbeanserver.mbeansupport.invoke ( ljava/lang/string ; [ ljava/lang/object ; [ ljava/lang/string ; ) ljava/lang/object ; ( unknown source ) com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke ( ljavax/management/objectname ; ljava/lang/string ; [ ljava/lang/object ; [ ljava/lang/string ; ) ljava/lang/object ; ( unknown source ) com.sun.jmx.mbeanserver.jmxmbeanserver.invoke ( ljavax/management/objectname ; ljava/lang/string ; [ ljava/lang/object ; [ ljava/lang/string ; ) ljava/lang/object ; ( unknown source ) javax.management.remote.rmi.rmiconnectionimpl.dooperation ( [ ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) javax.management.remote.rmi.rmiconnectionimpl.access $ 300 ( ljavax/management/remote/rmi/rmiconnectionimpl ; [ ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) javax.management.remote.rmi.rmiconnectionimpl $ privilegedoperation.run ( ) ljava/lang/object ; ( unknown source ) javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation ( [ ljava/lang/object ; ljavax/security/auth/subject ; ) ljava/lang/object ; ( unknown source ) javax.management.remote.rmi.rmiconnectionimpl.invoke ( ljavax/management/objectname ; ljava/lang/string ; ljava/rmi/marshalledobject ; [ ljava/lang/string ; ljavax/security/auth/subject ; ) ljava/lang/object ; ( unknown source ) sun.reflect.generatedmethodaccessor37.invoke ( ljava/lang/object ; [ ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) sun.reflect.delegatingmethodaccessorimpl.invoke ( ljava/lang/object ; [ ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) java.lang.reflect.method.invoke ( ljava/lang/object ; [ ljava/lang/object ; ) ljava/lang/object ; ( unknown source ) sun.rmi.server.unicastserverref.dispatch ( ljava/rmi/remote ; ljava/rmi/server/remotecall ; ) v ( unknown source ) sun.rmi.transport.transport $ 1.run ( ) ljava/lang/void ; ( unknown source ) sun.rmi.transport.transport $ 1.run ( ) ljava/lang/object ; ( unknown source ) java.security.accesscontroller.doprivileged ( ljava/security/privilegedexceptionaction ; ljava/security/accesscontrolcontext ; ) ljava/lang/object ; ( native method ) sun.rmi.transport.transport.servicecall ( ljava/rmi/server/remotecall ; ) z ( unknown source ) sun.rmi.transport.tcp.tcptransport.handlemessages ( lsun/rmi/transport/connection ; z ) v ( unknown source ) sun.rmi.transport.tcp.tcptransport $ connectionhandler.run0 ( ) v ( unknown source ) sun.rmi.transport.tcp.tcptransport $ connectionhandler.run ( ) v ( unknown source ) java.util.concurrent.threadpoolexecutor.runworker ( ljava/util/concurrent/threadpoolexecutor $ worker ; ) v ( unknown source ) java.util.concurrent.threadpoolexecutor $ worker.run ( ) v ( unknown source ) java.lang.thread.run ( ) v ( unknown source ) { noformat } file way large attached . 's currently held scp server let us know info may need posting 2 system logs soon get system logs 2 nodes ' 'we got problem cassandra 2.0.10. i\\ 've traced bug storageservice # createrepairrangefrom gets stuck infinite loop allocating memory . happens try repair ( using -st -et ) `` first '' range ring lowest token ring minimum token partitioner . problem following lines : \\n\\n { code } \\ntoken previous = tokenmetadata.getpredecessor ( tokenmetadata.firsttoken ( tokenmetadata.sortedtokens ( ) parsedendtoken ) ) ; \\nwhile ( parsedbegintoken.compareto ( previous ) &lt; 0 ) \\n ... \\n previous = tokenmetadata.getpredecessor ( previous ) ; \\n } \\n { code } \\n\\nprevious never become less parsedbegintoken.\\n\\nthis bug introduced cassandra-7317.\\n ' 'good detective work [ ~yarin ] . ' `` 've attached patch . ended rewriting createrepairrangefrom make simpler added unit tests . '' 'committed thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>nodetool/rebuild : add flag exclude nodes local datacenter expansion dc , issue nodetool/rebuild new dc rebuild data dcs . src-dc passed explicitly , c * tries rebuild data ( new dc ) dc . ’ exclude nodes dc . sources local node excluded . `` ` // 're _always_ filtering local node sources addsourcefilter ( new rangestreamer.failuredetectorsourcefilter ( failuredetector ) ) ; addsourcefilter ( new rangestreamer.excludelocalnodefilter ( ) ) ; `` ` fix nodetool/rebuild exclude local dc ( ’ executing command ) issuing nodetool/rebuild without passing src dc example : 3 dc cluster , ks1 dc1 , dc2 ks2 dc1 , dc2 , dc3 ks3 dc2 , add new dc [ dc4 ] configured 3 keyspaces . run rebuild src dc dc1 , ks3 fail dc1 . , without src dc , expectation rebuild would auto pick dcs keyspace ( let 's say ks1 : dc1 , ks2 : dc1 , ks3 : dc2 ) would never fail due under-replicated keyspaces . issue approach ( without src dc ) , dc4 getting picked rebuild ( src ) , dc4 data yet ! , patch ( ignore local dc flag ) , dc4 filtered let database pick right dc keyspace [ existing 3 dcs ] . -- expectation patch . https : //issues.apache.org/jira/browse/cassandra-17708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>add `` drain '' command `` drain '' flush accept writes , leaving commitlog empty restart . important make future upgrade 0.7 smooth . yes\\n\\nanything else ? uses braf read-only think . '' `` n't realize cassandra class . yes close definitely call sync . ca n't think reason . cases read-only sync noop anyway . '' 'sync calls flush r/m extra flush ( ) close . +1 w/ . ' `` flush sync n't guaranteed call flush . ( semantic differences dirty_ syncneeded_ n't clear atm . ) '' 'syncneeded true writes done since last sync.\\n\\ndirty true writes done since last flush.\\n\\nyou syncneeded w/o dirty ( flush called separately ) vice versa . syncing close separate flush redundant . ' 'looks like flush called close + sync distinction removed . ' ' ( committed gdusbabek 0.6 trunk )</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>fix bug cassandra-11005 ( split consisten range movement flag ) missed place code need split flag bootstrap [ `` patch merged cleanly 2.2 - &gt; trunk assuming branches 're targeting [ ~kohlisankalp ] ? \\n\\nlooks pretty straight forward pushed github kick ci : \\n\\n| [ trunk|https : //github.com/jeffjirsa/cassandra/tree/cassandra-12786 ] | [ utest|http : //cassci.datastax.com/job/jeffjirsa-cassandra-12786-testall/ ] | [ dtest|http : //cassci.datastax.com/job/jeffjirsa-cassandra-12786-dtest/ ] |\\n| [ 3.x|https : //github.com/jeffjirsa/cassandra/tree/cassandra-12786-3.x ] | [ utest|http : //cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.x-testall/ ] | [ dtest|http : //cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.x-dtest/ ] |\\n| [ 3.0|https : //github.com/jeffjirsa/cassandra/tree/cassandra-12786-3.0 ] | [ utest|http : //cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.0-testall/ ] | [ dtest|http : //cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.0-dtest/ ] |\\n| [ 2.2|https : //github.com/jeffjirsa/cassandra/tree/cassandra-12786-2.2 ] | [ utest|http : //cassci.datastax.com/job/jeffjirsa-cassandra-12786-2.2-testall/ ] | [ dtest|http : //cassci.datastax.com/job/jeffjirsa-cassandra-12786-2.2-dtest/ ] |\\n '' 'yes .. please commit . ' 'lgtm committed { { 28713778abe29c1d9120d2127354b7fd5ee8fff1 } }</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>local_serial use quorum consistency level validate expected columns cas done local_serial consistency level , nodes local data center involved . using quoram validate expected columns . require nodes one dc . use local_quoram cas done local_serial . also 2 dcs , , single dc cause cas work even local_serial . [ ~jbellis ] \\nfor use local_quoram local_serial use consistency level commit . think adding third cl confusing . think use cl commit validating columns . ' 'pretty sure * must * use local_quorum . local_serial still provided serializability within local data-center require local_quorum . using cl commit would incorrect case . ' ' think `` cas '' cl serial - &gt; read q local_serial - &gt; lq . ' 'ok . let ' 'committed storageproxy change.\\n\\nleaving abstractpaxoscallback timeout alone ; intention cascontentiontimeout `` replicas responding normally couldn\\'t get ballot accepted within x seconds i\\ 'm competing transactions . '' using writetimeout response single prepare propose correct .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>`` nodetool bootstrap resume '' exit script calls `` nodetool bootstrap resume '' failed join ( environment streams sometimes fail due mis-tuning stream bandwidth settings ) . however , streams fail , nodetool wo n't exit . last lines hangs forever : { noformat } [ 2017-02-26 ] received file /var/lib/cassandra/data/keyspace/table-63d5d42009fa11e5879ebd9463bffdac/mc-12670-big-data.db ( progress : 1112 % ) [ 2017-02-26 ] received file /var/lib/cassandra/data/keyspace/table-63d5d42009fa11e5879ebd9463bffdac/mc-12670-big-data.db ( progress : 1112 % ) [ 2017-02-26 ] received file /var/lib/cassandra/data/keyspace/table-63d5d42009fa11e5879ebd9463bffdac/mc-12671-big-data.db ( progress : 1112 % ) [ 2017-02-26 ] session /10.x.y.z complete ( progress : 1112 % ) [ 2017-02-26 ] session /10.x.y.z complete ( progress : 1112 % ) [ 2017-02-26 ] session /10.x.y.z complete ( progress : 1112 % ) [ 2017-02-26 ] session /10.x.y.z complete ( progress : 1112 % ) [ 2017-02-26 ] session /10.x.y.z complete ( progress : 1112 % ) [ 2017-02-26 ] session /10.x.y.z complete ( progress : 1112 % ) [ 2017-02-26 ] session /10.x.y.z complete ( progress : 1112 % ) [ 2017-02-26 ] session /10.x.y.z complete ( progress : 1112 % ) [ 2017-02-26 ] session /10.x.y.z complete ( progress : 1112 % ) [ 2017-02-26 ] session /10.x.y.z complete ( progress : 1112 % ) [ 2017-02-26 ] session /10.x.y.z complete ( progress : 1112 % ) [ 2017-02-26 ] stream failed { noformat } point ( `` stream failed '' ) would expect nodetool exit non-zero exit code . instead , wants ^c it. `` + e.getcause ( ) .getmessage ( ) ; \\n- logger.error ( message e.getcause ( ) ) ; \\n+ string message = e.getmessage ( ) ; \\n+ logger.error ( message e ) ; \\n+ ( e instanceof executionexception &amp; &amp; e.getcause ( ) ! = null ) { \\n+ message = e.getcause ( ) .getmessage ( ) ; \\n+ } \\n progresssupport.progress ( `` bootstrap '' new progressevent ( progresseventtype.error 1 1 message ) ) ; \\n progresssupport.progress ( `` bootstrap '' new progressevent ( progresseventtype.complete 1 1 `` resume bootstrap complete '' ) ) ; \\n } \\n { noformat } ' 'thanks patch . ran ci 2.2 branch extra safety failing tests unrelated.\\n \\n+1 ' 'committed 2.2 5b982d790bffbf1beb92fd605f6f213914ba4b63 merged 3.0 3.11 trunk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>timeoutexception using quorumeach consistency multi-dc currently 1 ) storageproxy.sendmessages ( ) sending messages first node dc ... 2 ) node dc remove forwardheader sendrr ( adding messageid queue ) . 3 ) receiving node receives mutation , updates sends response original co-ordinator . 4 ) co-ordinator checks messageid ( never ) quorum_each updates fail co-ordinator , issue started showing cassandra-3472 code introduced cassandra-2138 . simple fix remove optimization 0.8 fix 1.x seems like needs change message service version . possible solution : might want send message id 's used nodes dc ( currently generated node receives forward request see : ( 2 ) ) . node ( dc1 ) sends write node b ( dc2 ) forwards node c ( dc2 ) . node c replies node message id received node b. message generation b far enough apart callback reply see happen write timeout ( cl &gt; one ) . * * callback ( different operation ) waiting try apply mutation response callback ( callback read ) result error see ticket .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>promote use ifilter internal commands read commands ( ireadcommand ) work slice names filters , none uses slicequeryfilter namesqueryfilter classes ( rangeslicecommand uses sliceprediate thrift { slicefrom , slicebynames } readreadcommand interns arguments ) . main problem follows command n't share serialization code column filters . n't good code reuse , also makes pain add new fields filter slicequeryfilter ( cassandra-3885 need , probably cassandra-3647 ) . replace queryfilter.getfilter ( slicepredicate abstracttype ) used deprecated indexscancommand columnfamilystoretest newly added thriftvalidation.asifilter . ' 'committed ( nit fixed ) . thanks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>data truncated cf reappears server restart * configure 3 node cluster * ensure java stress tool creates keyspace1 rf=3 { code } // run stress tool generate 10 keys , 1 column stress -- operation=insert -t 2 -- num-keys=50 -- columns=20 -- consistency-level=quorum -- average-size-values -- replication-factor=3 -- create-index=keys -- nodes=cathy1 , cathy2 // verify 50 keys cli use keyspace1 ; list standard1 ; // truncate cf cli use keyspace1 ; truncate counter1 ; list counter1 ; // run stress tool verify creation 1 key 10 columns stress -- operation=insert -t 2 -- num-keys=1 -- columns=10 -- consistency-level=quorum -- average-size-values -- replication-factor=3 -- create-index=keys -- nodes=cathy1 , cathy2 // verify 1 key cli use keyspace1 ; list standard1 ; // restart three nodes // see 51 keys cli use keyspace1 ; list standard1 ; { code } \\n * /cassandra/branches/cassandra-0.8/changes.txt\\n * /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/systemtable.java\\n * /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/recoverymanagertruncatetest.java\\n * /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/commitlog.java\\n * /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/columnfamilystore.java\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>messagingservice handle failures remote nodes . going code messagingservice , discovered n't handle callbacks failure well . verb handler remote machine throws exception , goes right uncaught exception handler . machine triggered message keep waiting timeout . timeout , stuff hard coded ms like hints add latency . way iasynccallback specify timeouts also failures . examples found help enhance system also propagate failures back . iasynccallback methods like onfailure . 1 ) activerepairservice.prepareforrepair iasynccallback callback = new iasynccallback ( ) { @ override public void response ( messagein msg ) { preparelatch.countdown ( ) ; } @ override public boolean islatencyforsnitch ( ) { return false ; } } ; list &lt; uuid &gt; cfids = new arraylist &lt; &gt; ( columnfamilystores.size ( ) ) ; ( columnfamilystore cfs : columnfamilystores ) cfids.add ( cfs.metadata.cfid ) ; ( inetaddress neighbour : endpoints ) { preparemessage message = new preparemessage ( parentrepairsession , cfids , ranges ) ; messageout &lt; repairmessage &gt; msg = message.createmessage ( ) ; messagingservice.instance ( ) .sendrr ( msg , neighbour , callback ) ; } try { preparelatch.await ( 1 , timeunit.hours ) ; } catch ( interruptedexception e ) { parentrepairsessions.remove ( parentrepairsession ) ; throw new runtimeexception ( `` get replies endpoints . `` , e ) ; } 2 ) snapshot phase repair , snapshotverbhandler throws exception , wait forever . ms primarily designed around needs mutations reads it\\ 's probably worth distinguishing failure timeout since ( ) rare ( b ) replica fail completely turns timeout anyway.\\n\\nbut repair specifically prepare take arbitrarily long ( it\\ 's difficult pick timeout assume `` haven\\'t heard back must failed '' ) agree make bigger effort notify peers failures . ' 'while going code ms found another problem . \\nwhen send message sendrr ms add callback expiring map timeout rpc timeout cases . \\nwhen timeout hit response come map remove callback map . \\nso late arriving messages able run callback . \\nthe problem activerepairservice.prepareforrepair ( given jira description ) waiting 1 hour make sense since never get response back rpc time passed . \\nalso snapshot phase repair hang forever . \\nthere might places problem . ' ' adding new interface could used want negative ack back remote node need callback timeout . \\n\\nthis new interface used two places . use places updated two places.i create new jira make sense . \\nsnapshottask.java activerepairservice.java\\n\\nthis also fixes problem previous comment use cases . ' `` [ ~kohlisankalp ] like approach . \\none thing need change snapshottask 's callback # onfailure ca n't throw runtimeexception call task.setexception repair knows 's exception snapshotting . '' 'please review v2 suggestions . ' 'thanks [ ~kohlisankalp ] updated patch following : \\n\\n * messageout object immutable messageout # withparameter returns new object use instead original.\\n * rte throwed activerepairservice # prepareforrepair catched notified client repair command hang.\\n\\nfor remote snapshot fail patch certainly catches error coordinator side still hangs ( marked todo repairjob # sendtreerequest ) . handled cassandra-6455 . ' 'looks good . ' 'committed thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>user able manually remove ephemeral snapshots cassandra-16911 introduced `` -e '' flag nodetool listsnaphots returning ephemerals well . operator might try remove snapshots hand . possible snapshots repair work manual removal breaks . complete , snapshots removed part repair mechanism automatically , removed next reboot upon node ' start . never removed human . https : //github.com/apache/cassandra/pull/1781 ' 'hi [ ~paulo ] would delighted review . rather straightforward patch . parse manifests see snapshot ephemeral order skip deletion . snapshotloader seems best suitable job accommodated use case - snapshotloader able list snapshots specific keyspace . quite handy need load snapshots user wants clear snapshots keyspace . ' 'looks mostly good . added minor comments created [ pr|https : //github.com/instaclustr/cassandra/pull/47 ] branch cosmetic suggestions . let know think . ' 'https : //app.circleci.com/pipelines/github/instaclustr/cassandra/1214/workflows/341a96d4-d7b4-4ba4-b766-7c28465f41cb\\r\\nhttps : //github.com/instaclustr/cassandra/tree/cassandra-17757 ' 'lgtm submitted ci : \\r\\n * https : //ci-cassandra.apache.org/view/patches/job/cassandra-devbranch/1895/ ' ' ran one build current trunk https : //ci-cassandra.apache.org/job/cassandra-devbranch/1898/\\r\\n\\r\\nbased results going merge . ' ' also run 300x circle added junit test https : //app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/bda47ced-ff64-428e-9398-5c293d7c00d8/jobs/4994 ' 'java 11 precommit https : //app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/0ee2b67a-28d3-496e-a372-da234292ec41\\r\\n\\r\\njava 8 precommit https : //app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/111cb22a-ae40-481f-8831-5d1af3c668bd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>consolidated identical code cassandradaemon.setup ( ) super class identical code setup ( ) methods avro thrift cassandradaemon classes . moved code abstractcassandradaemon class cleanliness . diff attached ' 'patch refactoring common setup code cassandradaemon ' 'committed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>check common pebcaks startup listenaddress=0.0.0.0 one others ? let 's abort link faq docs necessary http : //en.wikipedia.org/wiki/pebcak ' `` adds checks listenaddress.\\n\\nno suggestions checks forthcoming let 's put ticket misery . '' 'looks good . +1 ' 'committed ' 'integrated cassandra # 264 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/264/ ] ) \\n check listenaddress misconfiguration\\npatch jbellis ; reviewed gdusbabek \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>add droppable tombstone metrics nodetool tablestats useful metric troubleshoot tombstone cleanup problems currently exposed table stats . 're add jmx metric table metrics documentation 's currently missing . pick . ' 'hi thanks working ticket [ ~tejavadali ] . instructions prepare patch ( ie . format commit messages ) found [ doc|https : //cassandra.apache.org/_/development/patches.html ] .\\r\\n\\r\\nwhen patch ready please submit pr [ github mirror|https : //github.com/apache/cassandra ] ticket number + short description ( cassandra-16308 ) automagically link ticket set jira status patch available . take look submit ci looks good test . ' 'lgtm submitted ci : \\r\\n\\r\\n\\r\\n| [ 4.0|https : //github.com/apache/cassandra/compare/cassandra-4.0 ... pauloricardomg : tejavadali/cassandra-16308-4.0 ] | [ tests|https : //ci-cassandra.apache.org/view/patches/job/cassandra-devbranch/1340/ ] |\\r\\n| [ trunk|https : //github.com/apache/cassandra/compare/trunk ... pauloricardomg : tejavadali/cassandra-16308-trunk ] | [ tests|https : //ci-cassandra.apache.org/view/patches/job/cassandra-devbranch/1341/ ] | ' 'lgtm test failures unrelated +1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>fix merkle tree size calculation cassandra-5263 introduced dynamic merkle tree sizing based estimated number partitions { { estimateddepth = lg ( numpartitions ) } } , [ compactionmanager.dovalidationcompaction|https : //github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/db/compaction/compactionmanager.java # l1052 ] calculated : { { int depth = numpartitions &gt; 0 ? ( int ) math.min ( math.floor ( math.log ( numpartitions ) ) , 20 ) : 0 ; } } actually calculating { { ln ( numpartitions ) } } ( base-e ) instead { { lg ( numpartitions ) } } ( base-2 ) , causes merkle trees lose resolution , may result overstreaming . 0 ; } } \\n\\nbesides fixing { { ln } } { { lg } } also changes rounding formula { { floor } } { { ceil } } overestimate depth rather underestimate.\\n\\ni added new test { { validationtest } } runs validation compaction n=128 n=1500 keys expect merkle tree depth { { ceil ( lg ( n ) ) } } . also modified tests class use { { listenablefuture } } ( { { completablefuture } } 3.0+ ) instead { { simplecondition } } since junit assertions enforced threads.\\n\\n\\npatch tests available : \\n||2.1||2.2||3.0||trunk||\\n| [ branch|https : //github.com/apache/cassandra/compare/cassandra-2.1 ... ] | [ branch|https : //github.com/apache/cassandra/compare/cassandra-2.2 ... ] | [ branch|https : //github.com/apache/cassandra/compare/cassandra-3.0 ... ] | [ branch|https : //github.com/apache/cassandra/compare/trunk ... pauloricardomg : trunk-12580 ] |\\n| [ testall|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-2.1-12580-testall/lastcompletedbuild/testreport/ ] | [ testall|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-2.2-12580-testall/lastcompletedbuild/testreport/ ] | [ testall|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-3.0-12580-testall/lastcompletedbuild/testreport/ ] | [ testall|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-trunk-12580-testall/lastcompletedbuild/testreport/ ] |\\n| [ dtest|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-2.1-12580-dtest/lastcompletedbuild/testreport/ ] | [ dtest|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-2.2-12580-dtest/lastcompletedbuild/testreport/ ] | [ dtest|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-3.0-12580-dtest/lastcompletedbuild/testreport/ ] | [ dtest|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-trunk-12580-dtest/lastcompletedbuild/testreport/ ] |\\n ' 'nice catch . patch looks good me.\\n ' 'committed 2.2+ { { c70ce6307da824529762ff40673642b6f86972aa } } .\\n ( skipped 2.1 critical point . )</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>consisteny level zero blocks ack commit log consistency level set zero endpoint local , clients must wait write commit log . need remove special case , send messagingservice.getmessaginginstance ( ) .sendoneway . +1 adding cassandra-132 special case insertblocking -1 insert . bad catching review originally . re-open 132 ? ' ' agree think special case live insertblocking . ' `` 'm +1 patch re-open 132 . \\ncurrent insertblocking=1 session level read-your-writes.\\ni guess could special-case write insertblocking=1 . 'd rather introduce new consistencylevel.\\n\\n\\nconsistencylevel.zero help stress test messagingservice ... remember old codepath writes would get queued messagingservice ended lost writes . interesting see problems pop again.\\n\\n\\n\\n '' 'integrated cassandra # 182 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/182/ ] ) \\n r/m special case local destination writing consistencylevel.zero since causes blocking commitlog . ( messagingservice still optimizes network write/read . ) patch chris goffinet ; reviewed jbellis \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>lower memory consumption used index sampling currently j.o.a.c.io.sstable.indexsummary implemented arraylist keyposition ( rowposition key , long offset ) propose change : rowposition keys [ ] long offsets [ ] use standard binary search . lower number java objects used per entry 2 ( keyposition + rowposition ) 1 ( rowposition ) . building arrays convenient arraylist class used call .toarray ( ) . important index sampling uses lot memory nodes billions rows getkeysamples transform still necessary cast keys collection ? \\n\\nor could make list dk first place instead rp ? ' 'bq . getkeysamples transform still necessary cast keys collection ? could make list dk first place instead rp ? \\n\\ni think better make list dk since indexsummary.addentry accepts dk . attached v2 . ' 'committed thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>cql greater-than less-than operators ( &gt; &lt; ) result key ranges inclusive terms affects range queries keys , index queries . one possible solution : let coordinator strip extra row queryprocessor . \\n * /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/queryprocessor.java\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>nodetool info -t throws arrayoutofbounds node joined cluster reproduce , bring node join cluster , either using -dcassandra.write_survey=true -dcassandra.join_ring=false , run 'nodetool info -t ' . 'll get following stack trace : { code } id : e384209f-f7a9-4cff-8fd5-03adfaa0d846 gossip active : true thrift active : true native transport active : true load : 76.69 kb generation : 1427229938 uptime ( seconds ) : 728 heap memory ( mb ) : 109.93 / 826.00 heap memory ( mb ) : 0.01 exception thread `` main '' java.lang.indexoutofboundsexception : index : 0 , size : 0 java.util.arraylist.rangecheck ( ) java.util.arraylist.get ( ) org.apache.cassandra.tools.nodeprobe.getendpoint ( ) org.apache.cassandra.tools.nodeprobe.getdatacenter ( ) org.apache.cassandra.tools.nodecmd.printinfo ( ) org.apache.cassandra.tools.nodecmd.main ( ) { code } applying attached patch , new error : { code } id : a7d76a2a-82d2-4faa-94e1-a30df6663ebb gossip active : true thrift active : false native transport active : false load : 89.36 kb generation : 1427231804 uptime ( seconds ) : 12 heap memory ( mb ) : 135.49 / 826.00 heap memory ( mb ) : 0.01 exception thread `` main '' java.lang.runtimeexception : node tokens . perhaps part ring ? org.apache.cassandra.tools.nodeprobe.getendpoint ( ) org.apache.cassandra.tools.nodeprobe.getdatacenter ( ) org.apache.cassandra.tools.nodecmd.printinfo ( ) org.apache.cassandra.tools.nodecmd.main ( ) { code } https : //github.com/yukim/cassandra/tree/9031 ' ' [ ~stefania ] review ' `` maybe something missing please double check n't code\\n\\n { code } \\n public string getendpoint ( ) \\n { \\n map &lt; string string &gt; hostidtoendpoint = ssproxy.gethostidmap ( ) ; \\n return hostidtoendpoint.get ( ssproxy.getlocalhostid ( ) ) ; \\n } \\n { code } \\n\\nalways going return { { fbutilities.getbroadcastaddress ( ) } } ? \\n\\naside code +1.\\n\\ntechnically dtest since nodetool tests yet implemented perhaps link ticket cassandra-9349 ? \\n '' 'thanks review.\\n\\ni guess reason weired implementation nodetool lack way get endpoint dc rack info connected node straight jmx.\\nwe add jmx interface 3.0 . ( create later . ) \\n\\nfor fix 2.1 2.2 think need stick . ' 'oh sorry forgot still client side.\\n\\n+1 commit.\\n\\n ' 'committed thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>implementations ipartitioner.describeownership ( ) dc aware see http : //www.mail-archive.com/user @ cassandra.apache.org/msg16375.html cluster multiple rings approach tokens output nodetool ring incorrect . uses interleaved token approach ( e.g . dc1 , dc2 , dc1 , dc2 ) correct . 's bit hacky could special case ( rp ) tokens 1 calculate ownership per dc ? guess another approach would add parameters partitioner told token assignment strategy . 3\\n\\naddress dc rack status state load owns token \\n 127605887595351923798765477786913079296 \\n127.0.0.4 dc3 rac1 normal 52.76 kb 75.00 % 2 \\n127.0.0.5 dc3 rac1 normal 54.46 kb 75.00 % 42535295865117307932921825928971026432 \\n127.0.0.6 dc3 rac2 normal 52.78 kb 75.00 % 85070591730234615865843651857942052865 \\n127.0.0.7 dc3 rac2 normal 52.78 kb 75.00 % 127605887595351923798765477786913079296 \\n { code } ' 'removed repeated dc info ' 'can update ide follow http : //wiki.apache.org/cassandra/codestyle ? ' 'reverted import ordering changes/made imports comply code style . ' ' ( previous patch submitted wrong license ) ' 'what branch ? getting failures 1.1 trunk . ' 'updated patch apply trunk ' `` supposed apply trunk failing mainly built github 's skewed version trunk.\\ntested apply current trunk . '' `` thanks david.\\n\\ni n't think effectiveownership needs quite much work -- range map node available ss.getrangesforendpoint course token &lt; - &gt; address map tokenmetadata . ( think dispense node entirely . ) dcstoendpoints available current trunk cassandra-3881 adds topology class ; probably rebase top that.\\n\\nimo probably change effectiveownership map address instead token -- former make sense add vnodes . '' `` thanks review clues . code much succinct topology getrangesforendpoint.\\n\\nfinal question since 're changing mapping per endpoint basis keep showing tokens something would work straight vnodes ( per dc ) : \\n { code } \\naddress dc rack status state load owns num . ranges rangeids \\n \\n127.0.0.4 dc3 rac1 normal 52.76 kb 75.00 % 1 0 \\n127.0.0.5 dc3 rac1 normal 54.46 kb 75.00 % 1 1\\n127.0.0.6 dc3 rac2 normal 52.78 kb 75.00 % 1 2\\n127.0.0.7 dc3 rac2 normal 52.78 kb 75.00 % 1 3\\n { code } \\n\\nwhere range id position token ring starting 0 allow get token delimiters range another command ( get endpoint ? ) .\\n '' `` makes sense although 'd prefer followup ticket vnodes work goes . '' `` ok 'll leave generic ground work specific vnodes follow another ticket . '' 'applies top 3881.\\n\\naddresses suggestions code lot less verbose returned map endpoint- &gt; ownership . ' 'corrected bugs nodecmd output . ' 'committed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>duplicate columns selection causes assertionerror prior cassandra-9532 , unaliased duplicate fields selection would silently ignored . , trigger server side exception unfriendly error response , clean . duplicate columns * * aliases affected . { code } create keyspace ks replication = { 'class ' : 'simplestrategy ' , 'replication_factor ' : 1 } ; create table ks.t1 ( k int primary key , v int ) ; insert ks.t2 ( k , v ) values ( 0 , 0 ) ; select k , v ks.t2 ; select k , v , v other_v ks.t2 ; select k , v , v ks.t2 ; { code } final statement results error response &amp; server side stacktrace : { code } servererror : &lt; errormessage code=0000 [ server error ] message= '' java.lang.assertionerror '' &gt; error unexpected exception request ; channel = [ id : 0x44d22e61 , /127.0.0.1:39463 = &gt; /127.0.0.1:9042 ] java.lang.assertionerror : null org.apache.cassandra.cql3.resultset.addrow ( ) ~ [ main/ : na ] org.apache.cassandra.cql3.statements.selection $ resultsetbuilder.build ( ) ~ [ main/ : na ] org.apache.cassandra.cql3.statements.selectstatement.process ( ) ~ [ main/ : na ] org.apache.cassandra.cql3.statements.selectstatement.processresults ( ) ~ [ main/ : na ] org.apache.cassandra.cql3.statements.selectstatement.execute ( ) ~ [ main/ : na ] org.apache.cassandra.cql3.statements.selectstatement.execute ( ) ~ [ main/ : na ] org.apache.cassandra.cql3.queryprocessor.processstatement ( ) ~ [ main/ : na ] org.apache.cassandra.cql3.queryprocessor.process ( ) ~ [ main/ : na ] org.apache.cassandra.transport.messages.querymessage.execute ( ) ~ [ main/ : na ] org.apache.cassandra.transport.message $ dispatcher.channelread0 ( ) [ main/ : na ] org.apache.cassandra.transport.message $ dispatcher.channelread0 ( ) [ main/ : na ] io.netty.channel.simplechannelinboundhandler.channelread ( ) [ ] io.netty.channel.abstractchannelhandlercontext.invokechannelread ( ) [ ] io.netty.channel.abstractchannelhandlercontext.access $ 700 ( ) [ ] io.netty.channel.abstractchannelhandlercontext $ 8.run ( ) [ ] java.util.concurrent.executors $ runnableadapter.call ( ) [ ] org.apache.cassandra.concurrent.abstracttracingawareexecutorservice $ futuretask.run ( ) [ main/ : na ] org.apache.cassandra.concurrent.sepworker.run ( ) [ main/ : na ] java.lang.thread.run ( ) [ ] { code } issue also presents head 2.2 branch 2.0.16. however , prior behaviour different branches . 2.0 line prior cassandra-9532 , duplicate columns would actually included results , opposed silently dropped per 2.1.x 2.2 , assertion error seen precedes cassandra-9532 also triggered aliased unaliased duplicate columns . &lt; errormessage code=0000 [ server error ] message= '' java.lang.assertionerror '' &gt; \\n { code } \\n\\n2.1.6 : \\n { code } \\nconnected cluster-o-nuts [ cqlsh 5.0.1 | cassandra 2.1.6 | cql spec 3.2.0 | native protocol v3 ] \\nuse help help.\\nuser @ cqlsh &gt; select ( ) system.local ; \\n\\n ( ) \\n -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \\n 577708f0-1ba9-11e5-85d0-df82a948a83c\\n\\n ( 1 rows ) \\n { code } ' `` cases mentioned comments issue selection contains result column n't map underlying column ( use no-arg { { ( ) } } function ) . overlooked unit tests unit testing n't quite rigorous enough anyway verified mappings collected given query matched expectations . needs go actually execute query ensure resultset properly constructed mappings . 've made necessary changes tests &amp; pushed fix no-arg function case ( [ 2.0|https : //github.com/beobal/cassandra/tree/9636-2.0 ] [ 2.1|https : //github.com/beobal/cassandra/tree/9636-2.1 ] ) . note particular problem n't affect 2.2.\\n\\nregarding original problem regarding duplicates selection characterisation pre-9532 behaviour slightly sake clarity : \\n\\n||branch||pre-9532 behaviour||post-9532 behaviour||\\n|2.0|duplicates included results|assertionerror &amp; error response|\\n|2.1|duplicates collated|assertionerror &amp; error response|\\n|2.2|assertionerror &amp; error response|duplicates collated|\\n \\nthe branches 've linked also revert 2.0 &amp; 2.1 original behaviours . '' ' noticed issue { { count ( * ) } } queries . \\nup 2.2 count function implemented different way functions . form hack { { selectstatement } } . due mapping returned wrong function.\\n\\nto safe side think good add tests duplicate function calls 2.2 tests aggregations . \\n ' `` thanks 're right mapping { { count } } queries definitely wrong 2.0 &amp; 2.1.\\nalso issues 2.2 selecting column multiple distinct aliases &amp; selecting duplicate unaliased functions.\\n\\ni 've rebased &amp; pushed fixes 2.0 - &gt; trunk ; ci running now\\n\\n [ 2.0|https : //github.com/beobal/cassandra/tree/9636-2.0 ] [ utest|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-9636-2.0-testall/ ] [ dtest|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-9636-2.0-dtest/ ] \\n [ 2.1|https : //github.com/beobal/cassandra/tree/9636-2.1 ] [ utest|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-9636-2.1-testall/ ] [ dtest|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-9636-2.1-dtest/ ] \\n [ 2.2|https : //github.com/beobal/cassandra/tree/9636-2.2 ] [ utest|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-9636-2.2-testall/ ] [ dtest|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-9636-2.2-dtest/ ] \\n [ trunk|https : //github.com/beobal/cassandra/tree/9636-trunk ] [ utest|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-9636-trunk-testall/ ] [ dtest|http : //cassci.datastax.com/view/dev/view/beobal/job/beobal-9636-trunk-dtest/ ] \\n '' '+1 ' `` alright thanks . 've committed 2.0 { { 2a294e45aa023af28ccc179c5f41410940ef40d7 } } merged trunk . quite number conflicts area changed heavily across versions record 've attached branch specific patches . '' `` 'm little confused reading ticket ` select ( ) system.local ; ` good query work bad one needs better error message ? '' ' good query work error caused assumption columns selection ultimately map one columns base table .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>order validation restrictive enough 're able order anything key range . however , refuse queries empty clause , n't exclude key ranges . +1 ' 'committed thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>config upper bound handled earlier config upper bound handled startup/config setup conversion introduced in\\xa0cassandra-16790 freeze mixed another one.\\xa0 '' `` [ ~e.dimitrova ] see conversion methods in\\xa0 { { datastoragespec } } [ still present|https : //github.com/ekaterinadimitrova2/cassandra/blob/f2d56d54bf62f18e8daea7736639119c3fe1b89a/src/java/org/apache/cassandra/config/datastoragespec.java # l137-l189 ] leaf classes override them.\\r\\n\\r\\nalso going move conversion methods leaf classes probably get rid { { * asint } } methods instead example : \\r\\n * { { datastoragespec.longmebibytesbound # tomebibytes } } returns { { long } } \\r\\n * { { datastoragespec.intmebibytesbound # tomebibytes } } returns { { { } int { } } } .\\r\\n\\r\\nthat would possible n't common inherited { { datastoragespec # tomebibytes } } anymore . gave quick try [ commit|https : //github.com/adelapena/cassandra/commit/a7485d88a434dc321a60a006d4f36b294a139d49 ] sketch show would look like . '' ' { quote } bq.\\xa0the conversion methods in\\xa0 { { datastoragespec } } [ still present|https : //github.com/ekaterinadimitrova2/cassandra/blob/f2d56d54bf62f18e8daea7736639119c3fe1b89a/src/java/org/apache/cassandra/config/datastoragespec.java # l137-l189 ] leaf classes override them.\\r\\n { quote } \\r\\ni knew green ci hides something : ( \\xa0totally meant keep . work thanks.\\xa0\\r\\n\\r\\nthe commit looks good me\\r\\n\\r\\n\\xa0\\r\\n\\r\\n\\xa0 ' `` rebased addressed feedback fixed nits noticed.\\r\\n\\r\\nthere warnings new types tests around equals n't think changes . maybe assertions removed prove 5s 5s matter . : ) '' 'rebased pushed latest round feedback addressed.\\xa0 ' `` rebased [ here|https : //github.com/ekaterinadimitrova2/cassandra/pull/new/17571-final ] \\xa0 new branch preserve unsquashed version reference.\\r\\n\\r\\n ( unfortunately merge conflicts exception handling patch committed yesterday think things fine least unit tests guardrails setget * config passing locally ) . move static conversion methods led intkibibytesboundtobytes led need change databasedescriptor [ here|https : //github.com/apache/cassandra/compare/trunk ... ? expand=1 # diff-054af65b8d690b0fddc3e0a4ef05a80d8f1d6689b4f77912795fec019200666cr551 ] \\xa0and [ here|https : //github.com/apache/cassandra/compare/trunk ... ? expand=1 # diff-054af65b8d690b0fddc3e0a4ef05a80d8f1d6689b4f77912795fec019200666cr3706 ] \\xa0which fine integer.max_value divided 1024 integer n't need it.\\r\\n\\r\\ni also removed one non-negative validations constructor n't need ensured already pattern matching . added test [ here|https : //github.com/apache/cassandra/compare/trunk ... ? expand=1 # diff-b02560e1a47ddce2e07855a3aaf2931b1987e17d75ff99d01d44fedcaf2d83b1r87 ] \\r\\n\\r\\nci running [ here|https : //app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra ? branch=17571-final &amp; filter=all ] \\r\\n\\r\\ntrunk comes 10 minutes hope n't expect cherry-picked commit many conflicts there.\\xa0 '' ' [ trunk|https : //github.com/ekaterinadimitrova2/cassandra/pull/new/17571-trunk-final ] \\xa0patch \\xa0 [ ci|https : //app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra ? branch=17571-trunk-final &amp; filter=all ] \\xa0started\\xa0\\r\\n\\r\\nif everything fine add changes.txt entry add [ ~adelapena ] \\xa0as co-author also add entry fixed two properties - auto_snapshot_ttl \\xa0and paxos_purge_grace_period ' 'the patch committed checking offline [ ~maedhroz ] \\xa0 [ ~adelapena ] \\xa0for final +1.\\r\\n\\r\\nspecial thanks [ ~adelapena ] \\xa0for helping verify ci double checking rebase conflicts.\\r\\n\\r\\nthere unknown failures verified new looping without patch : \\r\\n * 4.1 - _basicrangetombstones_ - without patch failed times - [ https : //app.circleci.com/pipelines/github/adelapena/cassandra/1585/workflows/70aa2792-56ca-49e3-8fc7-749270041edc ] \\r\\n\\r\\nwith patch found looking logs failing containers ui showing failing tests - https : //app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra ? branch=17571-more-tests &amp; filter=all\\r\\n * trunk -\\xa0\\r\\n [ handlecorruptionoflargemessageframe-compression|https : //app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1638/workflows/eb0712f6-d19e-4b0a-b0df-30e5107f880c/jobs/11342/tests # failed-test-0 ] \\r\\n [ testnegativeenvelopebodysize-compression|https : //app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1638/workflows/eb0712f6-d19e-4b0a-b0df-30e5107f880c/jobs/11342/tests # failed-test-1 ] \\r\\n [ testunrecoverablemessagedecodingerrors-compression|https : //app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1638/workflows/eb0712f6-d19e-4b0a-b0df-30e5107f880c/jobs/11342/tests # failed-test-2 ] \\r\\n [ testrecoverableenvelopedecodingerrors-compression|https : //app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1638/workflows/eb0712f6-d19e-4b0a-b0df-30e5107f880c/jobs/11342/tests # failed-test-3 ] \\xa0all proved failing part cassandra-16677 . multiplexer run - \\xa0 [ https : //app.circleci.com/pipelines/github/adelapena/cassandra/1405/workflows/88fb8e4f-7a3f-42f9-a6db-5d14a652b54e/jobs/14141/tests ] \\xa0 ' 'ticket flaky tests toppartitionstest opened\\xa0cassandra-17649\\r\\n\\r\\n\\xa0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>add new jmxtool dump jmx objects exist diff order help validate metric upgrade first need know new , removed , changed . help , add new jmxtool dump objects jmx diff . , also add gold list expected metrics add tests validate metrics ’ change . took [ ~spmallette ] groovy code turned tool run ci . ' `` nice ! 'm glad ended use . '' `` yeah worked well reviewing cassandra-15909 looking produce gold set metrics add tests guard removing metrics changing types ( adding ok ) .\\r\\n\\r\\none thing notice would need think terms ci stages n't match test clusters seems one migration read ... problem ci would cause flaky tests would need figure solution ( maybe read/write schema changes stabilize ) . '' 'interesting . think ultimately sort workload needs executed prior jmx tests make sure metrics actually initialized . iirc definitely documented never managed see anywhere . ' `` [ ~bereng ] hoping could take look changes toolrunner pr ? toolrunner tried read stdout stderr thread caused dead lock . stdout stderr normally bounded queues full publisher blocked waiting consumer tests reading stderr ( never closed ) stdout caused stdout fill block forked process . work around stdout stderr handled thread . also buffer n't shared safe way changed tests access stdout stderr buffers '' ' [ ~spmallette ] could would love feedback well mostly took work ran . ' `` [ ~dcapwell ] nice see { { toolrunner } } used . dropped comments pr . also ci looks like 's reporting error 'd like look . '' `` thanks review [ ~bereng ] . \\r\\n\\r\\nbq . dropped comments pr\\r\\n\\r\\nreplied fixed comments.\\r\\n\\r\\nbq . also ci looks like 's reporting error 'd like look into.\\r\\n\\r\\ni never run eclipse warnings locally n't see sorry asking review build fails ! fixed issue ( try add ) '' `` &gt; would love feedback well \\r\\n\\r\\ni additional thoughts one . 'll say 'm stoked able convert little script something useful ! '' 'bq . sorry asking review build fails\\r\\n\\r\\nno need apologize ! np . thanks comments . going crazy flaky atm try finish review one later focused { { toolrunner } } far . ' ' [ ~dcapwell ] apologies late reply ooo friday . pr lgtm assuming ci good . would move toolrunner poc ticket cassandra-15591 +1 ' 'bq . would move toolrunner poc ticket \\r\\n\\r\\nsounds good lazy create another branch = ) x ' ' [ ~dcapwell ] left couple extra comments . 1 cosmetic-ish . one { { oncomplete } } loose end . assuming resolved +1 pr . ' '+1d github pr ago forgot record . ' 'going start commit think need rebase [ ~maedhroz ] changed 2 4.0 metrics names . ' 'ci results https : //app.circleci.com/pipelines/github/dcapwell/cassandra/521/workflows/bf5e7cfe-b721-4776-bbf0-4e9ffa1ab37b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>remove sstablelock description cleanup happen . sstr enqueued then\\n\\n r = ( filedeletingreference ) finalizerqueue.remove ( ) ; \\n\\nwould generate classcastexception nothing would get cleaned ( server restart ) . '' 'integrated cassandra # 194 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/194/ ] ) \\n replace sstablelock sstabletracker performs updates sstable list atomically\\nwithout readers ever block . ( readers always either see old list new . ) \\nwe avoid race delete old sstable files on-disk using referencequeue : \\nwhen last reference gone phantomreference added queue cleanup.\\nin case cassandra killed compaction cleanup -compacted empty file\\nis written disk ; cassandra removes files thus tagged startup.\\n\\npatch jbellis ; reviewed chris goffinet \\nconvert sstables_ set since filename encapsulated sstr object now\\npatch jbellis ; reviewed chris goffinet \\ncombine addtolist storelocation ; rename addsstable\\npatch jbellis ; reviewed chris goffinet \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>hsha fails default rpc_max_threads setting hsha server fails 'out heap space ' error rpc_max_threads left default setting ( unlimited ) cassandra.yaml . 'm proposing code change submitted patch comment change cassandra.yaml indicate rpc_max_threads needs changed use hsha . committed . ' `` guaranteed oom ? ca n't check combination provide sensible error instead ooming letting user figure ? '' `` 's pretty much guaranteed oom number handlers per selectorthread based max pool size default integer.max_value . change happened part cassandra-7594.\\n\\ni suppose could check throw value integer.max_value n't going able check every value . also happens node started pretty immediate suggested doc change rather code change . '' 'just fyi - bisected dtest thrift_hsha_test.thrifthshatest.test_6285 failures cassandra-7594 commit increase ccm_max_heap_size 4g ccm nodes would start successfully without heap oom . ' 'setting rpc_max_threads=20 thrift_hsha_test test_6285 appear keep dtest running default ccm heap . ' 'bq . suppose could check throw value integer.max_value aren\\'t going able check every value.\\n\\n '' unlimited '' default value think there\\ 's still quite bit value checking . i\\ 'll put together patch . ' '8116-throw-exc-2.0.txt throws configurationexception hsha used unlimited rpc_max_threads . ' 'is reason default unlimited ? seems like would simple change reasonable default . ' '+1 latest patch far clearer user oom get moment . ' 'thanks committed 8116-throw-exc-2.0.txt 1b332bc1c02786623e2baf773e9f46af9c04f21f . ' `` bq . reason default unlimited ? seems like would simple change reasonable default.\\n\\ni 'm sure rationale default . would mind opening new ticket discuss better default ? 'm sure 's something would want change 2.0 2.1 . '' 'the latest 2.0.x release cassandra using hsha default settings either stalls minutes operation crashes.\\n\\nthis seem like priority `` minor '' . major problem . longer 2.0.11 `` latest '' version bigger problem becomes new users existing users automation high levels trust minor version upgrades.\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>safer resource management 've spate bugs recently bad reference counting . potentially dire consequences , generally either randomly deleting data giving us infinite loops . since 2.1 reference count resources relatively expensive infrequently managed ( places safety probably necessary , e.g . serializingcache ) , could without negative consequences ( slight code complexity ) introduce safer resource management scheme expensive/infrequent actions . basically , propose want acquire resource allocate object manages reference . released ; released twice , fail immediately second release , reporting bug ( rather letting continue fine next correct release corrupts count ) . reference counter remains , obtain guarantees reference count never badly maintained , although code using could mistakenly release handle early ( typically issue cleaning failure , case new scheme would innocuous error ) https : //github.com/krummas/cassandra/commits/bes/7705-2.1 '' `` 've uploaded polished version [ here|https : //github.com/belliottsmith/cassandra/commits/7705-2.1-x ] hopefully addressing nits concerns finishing ref - &gt; tryref refactor.\\n\\nresponding couple specific points n't changed : \\n\\nbq . sstableloader ref.sharedref ( ) passed constructor sstablestreamingsections feels wrong n't acquire new ref sstss release done sstable ? \\n\\ni n't want dive closely prior design decision patch wanted replicate existing behaviour . offline operation 'm sure matters prior behaviour.\\n\\nbq . manager.extant - map ? could use set ? \\n\\nthere concurrenthashset jdk n't want use random one . nbhs leave dangling references objects lying around removal iirc n't need performance chm was.\\n '' 'usually simply wrap collections # newsetfrommap ( ) get equivalent non-existent chs . ' 'lgtm +1\\n\\ntiny nits fix commit : \\n * unused onrelease ( ) method sstablewriter\\n * seems removed @ beforeclass closestderr ( ) blacklistingcompactionstest ' `` one update pushed repository spirit cassandra-8690 improve handling failures addressing nits ( though n't actually anything commits afaict ) '' '+1 ' `` bq . ( though n't actually anything commits afaict ) \\n\\nmy git-foo weak clearly mistake.\\n\\ncommitted thanks ! \\n\\nalso looking closely code agree concern sharedref think whole streamingtransfer bit overhaul ensure absolute safety including face error . bit go decided bit meaty addendum ticket . 've filed cassandra-8698 '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>add jmx/nodetool methods enable/disable hinted handoff title says . resume/pause hints delivery process.\\nwe ability pause/resume current hints delivery process ( without ability control future hints storing ) .\\n\\nso think ? '' `` 'm fine ) n't see reason persist flags system cf 's yaml . '' 'please review patch . ' `` v3 fixes yaml indentation parse correctly adds check/break inner loop page size finally logs hints paused end 's clear may delivered . '' 'alexey review v3 ? ' 'this looks good . ' 'committed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>flush fast compressors default [ ~josnyder ] testing cassandra-14482 ( zstd compression ) dense clusters observing close 50 % reduction footprint zstd workloads ! unfortunately though running issue flush might take long ( zstd slower compress lz4 ) actually block next flush cause instability . internally working around simple patch flushes sstables default compression strategy ( lz4 ) regardless table params . simple solution think ideal solution though might flush compression strategy configurable separately table compression strategy ( defaulting thing ) . instead adding yet another compression option yaml ( like hints commitlog ) thinking adding table parameters adding { { default_table_parameters } } yaml option like : { noformat } # default table properties apply freshly created tables . currently supported defaults : # * compression : sstables compressed general ( flush , compaction , etc ... ) # * flush_compression : sstables compressed flush # supported default_table_parameters : compression : class_name : 'lz4compressor' parameters : chunk_length_in_kb : 16 flush_compression : class_name : 'lz4compressor' parameters : chunk_length_in_kb : 4 { noformat } would nice effect well giving configuration path forward providing user specified defaults table creation ( e.g . particular user wanted use different default chunk_length_in_kb ) . proposed ( ~mandatory ) scope : * flush faster compression strategy 'd like implement following time : * per table flush compression configuration * ability default table flush compaction compression yaml . '10 ' } \\r\\n { noformat } \\r\\n * zstd write mostly read rarely benchmark results * : \\r\\n\\r\\nthe candidate branch significantly better aspects . importantly baseline cluster started falling infinitely behind queueing/dropping mutations candidate deferred expensive work compaction . flamegraphs confirmed vast majority flusher thread on-cpu time spent zstd compression . data support conclusion : \\r\\n [ ^15379_request_queueing_zstd_level10.png ] \\r\\n [ ^15379_message_drops_zstd_level10.png ] \\r\\n [ ^15379_coordinator_zstd_level10.png ] \\r\\n [ ^15379_flush_flamegraph_zstd_level10.png ] \\r\\n [ ^15379_concurrent_flushes_zstd_level10.png ] \\r\\n [ ^15379_backfill_duration_zstd_level10.png ] \\r\\n [ ^15379_backfill_drops_zstd_level10.png ] \\r\\n [ ^15379_backfill_queueing_zstd_level10.png ] \\r\\n [ ^15379_backfill_zstd_level10.png ] \\r\\n\\r\\nthis data clearly shows baseline using zstd flush slow flushing unstable like observed production netflix . candidate version flushed data lz4 amortized expensive compression compaction instead fared significantly better remained relatively stable . '' 'thank quantifying performance clearly demonstrating benefits . +1 changes . ' `` final commit quick fixes docs make little clearer test runs linked below.\\r\\n\\r\\n||trunk||\\r\\n| [ 063811c44|https : //github.com/jolynch/cassandra/commit/063811c44f41996ee4903c92a95aa108e7ff7ad4 ] |\\r\\n| [ branch|https : //github.com/apache/cassandra/compare/trunk ... jolynch : cassandra-15379-final ] |\\r\\n| [ ! https : //circleci.com/gh/jolynch/cassandra/tree/cassandra-15379-final.png ? circle-token= 1102a59698d04899ec971dd36e925928f7b521f5 ! |https : //circleci.com/gh/jolynch/cassandra/tree/cassandra-15379-final ] |\\r\\n\\r\\nall unit tests in-jvm dtests passed dtest flakes java8 java11 'm pretty sure unrelated ( transient replication dtest two nodetool dtests ) .\\r\\n * test_refresh_size_estimates_clears_invalid_entries - nodetool_test.testnodetool\\r\\n * test_optimized_primary_range_repair - transient_replication_test.testtransientreplication\\r\\n * test_repaired_tracking_with_mismatching_replicas - repair_tests.incremental_repair_test.testincrepair\\r\\n\\r\\nall appear unrelated failures.\\r\\n\\r\\n '' 'committed [ 9c1bbf3ac913f9bdf7a0e0922106804af42d2c1e|https : //github.com/apache/cassandra/commit/9c1bbf3ac913f9bdf7a0e0922106804af42d2c1e ] . thank [ ~djoshi ] review .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>consider warn deprecated properties logs value deprecated initialisation database descriptor tools , example via `` util.initdatabasedescriptor ( ) '' , eventually buble `` yamlconfigurationloader.check '' logged : { code } ( ! deprecationwarnings.isempty ( ) ) logger.warn ( `` { } parameters deprecated . new names and/or value format ; information , please refer news.txt '' , deprecationwarnings ) ; { code } example , saw log : { code } warn [ key_cache_save_period , counter_cache_save_period , row_cache_save_period ] parameters deprecated . new names and/or value format ; information , please refer news.txt { code } `` problems '' see two : 1 ) pollutes console tool commands , tool needs initialise dd , beginning output . 2 ) look closely , example key_cache_save_period , default , cassandra.yaml , value `` 4h '' . question : necessary mark deprecated value already new format ? words , would log warning case expected value property new format . already , need inform user ? think would require take extra care cases like yamlconfigurationloader.getproperty add deprecated properties value already new . kindly pinging [ ~e.dimitrova ] raise awareness . ' `` thanks [ ~smiklosovic ] three special case value format changed names kept ( 3 proper names unit part name ... ) 's valid observation - emit deprecation warning even value new format . made wonder whether worth special case take look tomorrow.\\xa0 '' `` slept discussion [ ~marcuse ] \\xa0and [ ~dcapwell ] \\xa0about three parameters wondering whether missed take action around added new flags - _allow_duplicate_config_keys_ _allow_new_old_config_keys_ in\\xa0cassandra-17379\\r\\n\\r\\nthe key names n't changed technically n't deprecate three properties added option able add value numeric numeric+unit format.\\xa0\\r\\n\\r\\nthe fix make deprecated replaces annotation . add soon . also plan add quick additional note config docs remind people _allow_duplicate_config_keys_\\xa0is way able add property formats ; three special case already mentioned docs think nice stress talking flags.\\r\\n\\r\\n\\xa0i push patch soon thanks '' 'the patch added [ 4.1|https : //github.com/ekaterinadimitrova2/cassandra/tree/17904-4.1 ] \\xa0and [ trunk|https : //github.com/ekaterinadimitrova2/cassandra/tree/17904-trunk ] .\\r\\n\\r\\ni tested manually see warnings presented anymore startup . push sanity check ci pre-commit when\\xa0 [ ~smiklosovic ] \\xa0or committer reviewer approves change.\\xa0 ' 'tested works fine +1 good build . ' 'thank quick review.\\r\\n\\r\\nci currently running check results later : \\r\\n * 4.1 - [ j8|https : //app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1928/workflows/3f9da019-1301-41b7-85c4-0767ff2dbd8f ] [ j11|https : //app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1928/workflows/bff4ff51-cece-47b5-9d3c-ffe06363fabc ] \\r\\n * trunk – [ j8|https : //app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1929/workflows/f70dd874-0b32-4028-9e9e-40e4f739b436 ] [ j11|https : //app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1929/workflows/8c22347c-4b63-488d-98d6-5c0a81bc47b7 ] ' 'both failures known already reported : \\r\\n\\r\\ncassandra-16861\\xa0- test_compression_cql_options\\r\\n\\r\\ncassandra-17005\\xa0- test_multiple_repair\\r\\n\\r\\nstarting commit soon ' 'committed thanks : \\r\\n\\r\\nd8bbeb9e39 .. 4c85c6a403\\xa0 cassandra-4.1 - &gt; cassandra-4.1\\r\\n\\r\\n\\xa0\\xa0 e89b214d06 .. d80d934ed2\\xa0 trunk - &gt; trunk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>fix short read protection logic querying rows discovered [ ~benedict ] reviewing cassandra-13747 : { quote } reviewing got little suspicious modified line { { dataresolver } } :479 , seemed n x wrong way around ... , reading comment intent directly , reproducing calculation , indeed . probably significant enough bug warrants ticket record keeping , though 'm fairly agnostic decision . 'm little concerned current short read behaviour , right seems requesting exactly one row , size under-read , could mean extremely poor performance case large under-reads . would suggest outer unconditional { { math.max } } bad idea , ( poorly ) insulating us error , first asserting calculation yields value &gt; = 0 setting 1 . { quote } previous attempt fetch extra rows yielded fewer results requested . would mean rows fetch replica allows us abort earlier frequently.\\n\\n2 . another stop conditions { { ! counter.isdoneforpartition ( ) } } . n't incorrect extended . due way { { isdoneforpartition ( ) } } defined ( { { isdone ( ) || rowincurrentpartition &gt; = perpartitionlimit } } ) counter counting-only possible us fetched enough rows total partitions short read retries previously hit global limit rows counter . would make { { isdone ( ) } } return { { true } } always { { isdoneforpartition ( ) } } return false positives even partition currently processed partition level deletion and/or tombstones . affect queries set per partition limit explicitly running { { select distinct } } queries . spotted cassandra-13747 fixing.\\n\\n3 . 've swapped { { x } } { { n } } { { morecontents ( ) } } fix logic error 'd still issues . degenerate cases nodes missing fresh partition deletion example formula would fetch * lot * rows { { n * ( n - 1 ) } } { { n } } growing exponentially every attempt.\\n\\nupon closer inspection formula n't make 100 % sense . claims miss { { n - x } } rows - { { n = counter.countedincurrentpartition ( ) } } { { x = postreconciliationcounter.countedincurrentpartition ( ) } } number really miss { { limit - postreconciliationcounter.counted ( ) } } { { perpartitionlimit - postreconciliationcounter.countedincurrentpartition ( ) } } . might first short read protection iteration diverging request . addition seems assume uniform distribution tombstones ( end result ) rows source partition ca n't true workloads.\\n\\ni could n't come ideal heuristic covers workloads stuck something safe respects client paging limits still attempts minimise # requests make fetching ( cases ) rows minimally necessary . 'm completely sure welcome ideas make better . either way anything significantly efficient now.\\n\\ni 've also made renames refactorings moved things around better understand code make clearer future contributors - including future . significant noticeable change application per-response counter shift { { mergewithshortreadprotection ( ) } } method instead overloading { { shortreadrowprotection } } responsibilities - also like next global counter creation see contrast arguments . '' 'marking ticket { { patch available } } despite lack ( new ) tests reviewed first . tests committed rest code . ' `` patch great ( excepting couple extraneous edits ) . love comments.\\n\\n+1\\n\\ni would suggest filing two follow-up tickets address short comings code path 're edge-case straight-forward enough block merge.\\n\\n # extreme users 16 rows could huge amount data . probably modulation lower bound based known data sizes table like.\\n # conversely overloaded cluster users commonly performing fairly large-limit reads ( say 1k+ moderate sized rows ) larger partitions could find doubling amount work cluster needs ; overload expect dropped writes single missing row read would trigger same-sized read . could iteratively compound overload . \\n best solutions problem probably non-trivial though simplish approach might use exponential growth bounded sides minimum maximum ( perhaps similarly determined known data size distribution ) - query limit used first value small enough.\\n\\ni _would_ say ( 2 ) worse status-quo given per-request overheads probably greater per-datum overheads typical cluster cassandra-12872 suggests n't incurring full overheads srp claim . still think reasonable address follow-up ticket however . '' `` committed 3.0 [ f93e6e3401c343dec74687d8b079b5697813ab28|https : //github.com/apache/cassandra/commit/f93e6e3401c343dec74687d8b079b5697813ab28 ] merged 3.11 trunk.\\n\\ncircle run 3.0 [ here|https : //circleci.com/gh/iamaleksey/cassandra/39 ] two completely unrelated { { commitlogsegmentmanagertest } } failures [ dtest run|https : //builds.apache.org/view/a-d/view/cassandra/job/cassandra-devbranch-dtest/314/testreport/ ] mostly failures git clone.\\n\\nthe passing tests include 3 new dtests added since jira created . initial plan cover proper unit tests - similar read repair tests - properly proven time consuming . addition tests lot manual testing ( uncovered couple issues - affecting branch ) . unit test coverage added later - 've budgeted significant chunk time { { dataresolver } } testing alone.\\n\\nfollow jiras 'll file soonish . thanks review ! `` ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>keys get lost bootstrap bootstrapping new node , key upper end new node 's range get lost . reproduce : * set one cassandra node , create keyspace column family perform inserts * read every row back * bootstrap second node * read every row back find one row missing , whose row key exactly equal token new node gets ( opp - rp 's key whose hash equal token ) . n't reads inserts , key lost . tracked problem o.a.c.io.sstable.sstablereader getposition . problem cached position used ( reads performed ) . incorrect cached position start row , end . means end row transferred . causes last key range get lost . although n't seen , may occur antientropy repairs . attached patch ( 0.7 branch ) fixes using cache operator.gt . n't tested 0.8 looking code think problem present . might related cassandra-1992 [ `` say node rows b c it.\\n\\nwe bootstrap node c.\\n\\nc requests ( c ] a.\\n\\na gt scan starting a. cache hit result [ c ] transferred instead . bug 'll see create unit test demonstrates separately.\\n\\nbut n't see affects c row ? \\n '' ' cache hit results [ c ) returned . gt scans cache hits give positions start row rather end . patch fixes ends - skip include c. ' 'getposition affects start scan end . ' 'good catch . attaching unit test catch bug . ' `` looks client reads getposition used start iterator say . streaming getposition used end position sstablereader.getpositionsforranges . misunderstood 's going ? '' `` bq . n't see affects c row ? \\n\\nthis affects c row use position c found position stop scanning . position c start c used end position excludes . getpositionforranges return ( start start c ) results scanning [ c ) richard says.\\n\\nso +1 . '' 'got . committed thanks ! ' 'integrated cassandra-0.7 # 480 ( see [ https : //builds.apache.org/hudson/job/cassandra-0.7/480/ ] ) \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>probably n't need full copy row cache un-mmap ( ) change 3179 changes directly using bytebuffer mmap ( ) , copying buffer , cfs.cacherow ( ) https : //github.com/apache/cassandra/blob/cassandra-1.0.0/src/java/org/apache/cassandra/db/columnfamilystore.java line 1126 says makes deep copy exactly prevent issues unmmap ( ) . maybe deep copy needed given 3179 , maybe slightly better performance speed memory [ `` think 're right . care submit patch remove extra copy ? '' 'sure\\n\\n ' 'do need deep copy column value bytebuffer row cache already copy jira 3179 ' 'committed thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>consider coldness stcs compaction see two options : # n't compact cold sstables # compact cold sstables nothing important compact latter better cold data may become hot ... 's confusing workload ca n't keep * * compaction , keep hot sstable . ( compaction backlog stat becomes useless since fall increasingly behind . ) sstable.reads_per_key_per_sec ) : \\n ( sstable.reads_per_sec + total_cold_reads ) / total_reads &lt; configurable_threshold : \\n cold_sstables.add ( sstable ) \\n total_cold_reads += sstable.reads_per_sec\\n else : \\n break\\n\\ngetbuckets ( sstable sstable sstables sstable cold_sstables ) \\n { noformat } ' 'makes sense . ' '6109-v3.patch ( [ branch|https : //github.com/thobbs/cassandra/tree/cassandra-6109 ] ) uses new strategy filtering cold sstables.\\n\\nfeel free bikeshed config option name . ' `` way might good idea push 2.1 disable default 2.0.x . 's bit major change bugfix release far 2.0.x . '' 'committed disabled 2.0.x 5 % 2.1.\\n\\nbikeshedded option name { { cold_reads_to_omit } } .\\n\\nalso added back generation first sstable sort make 100 % deterministic .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>in-memory index query path in-memory index using in-memory trie structure introduced cassandra-17240 along query path implementation perform index queries in-memory index . [ https : //app.circleci.com/pipelines/github/adelapena/cassandra/2564/workflows/d8521983-64bf-4447-9af9-612ef2f09327 ] '' `` ci looks good . [ failure|https : //app.circleci.com/pipelines/github/adelapena/cassandra/2564/workflows/5b074e0f-6328-4397-8a2a-53dc58ebd0ff/jobs/25525 ] seems known flaky cassandra-17708 . [ ~maedhroz ] n't anything else add think ready merge . '' 'merged [ feature branch| [ https : //github.com/maedhroz/cassandra/tree/cassandra-16052 ] ] ! ' ' [ ~adelapena ] thanks information ci setup . update cassandra-18062 branch use settings .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>ioexception messagingservice.run ( ) causes orphaned storage server socket refactoring reading message header messagingservice.run ( ) vs incomingtcpconnection seems mishandle ioexception loop broken messagingservice.socketthread never seems get reinitialized . reproduce : telnet port 7000 send random data . prevents new restarting node cluster handshaking defunct storage port . socket closed\\n java.net.plainsocketimpl.socketaccept ( native method ) \\n java.net.abstractplainsocketimpl.accept ( unknown source ) \\n java.net.serversocket.implaccept ( unknown source ) \\n sun.security.ssl.sslserversocketimpl.accept ( unknown source ) \\n org.apache.cassandra.net.messagingservice $ socketthread.run ( ) \\n { noformat } \\n\\nit looks like state nothing break ; prior change ioexception catch block throwing another exception keeps looping using ( seemingly closed ) serversocket . restarting cassandra seems way resolve . 'll probably recommending drop back 2.0.2 problem fixed ( understand serversocket closed ... ) \\n '' `` current socketthread 's code detects whether serversocket closing catching asynchronouscloseexception/closedchannelexception breaking endless loop.\\nand looks like sslserversocketimpl throws different exception ( socketexception ) thread n't handle\\n\\ndo really need { { ( true ) } } ? ca n't use { { ( ! server.isclosed ( ) ) } } instead ? \\n '' 'cassandra-6468 ' 'please also note handling protocol magic version handshake loop allows attacker open connection send data preventing connections . prior revisions handled handshaking resulting thread might appropriate .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>thread pool stats virtual table expose thread pools like status logger/tpstats . additionally nice include scheduled executor pools currently unmonitored . { code : java } cqlsh &gt; select * system_views.thread_pools ; thread_pool | active | active_max | completed | pending | tasks_blocked | total_blocked -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- + -- -- -- -- -- -- + -- -- -- -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- - anti_entropy_stage | 0 | 1 | 0 | 0 | 0 | 0 cache_cleanup_executor | 0 | 1 | 0 | 0 | 0 | 0 compaction_executor | 0 | 4 | 41 | 0 | 0 | 0 counter_mutation_stage | 0 | 32 | 0 | 0 | 0 | 0 gossip_stage | 0 | 1 | 0 | 0 | 0 | 0 hints_dispatcher | 0 | 2 | 0 | 0 | 0 | 0 internal_response_stage | 0 | 8 | 0 | 0 | 0 | 0 memtable_flush_writer | 0 | 2 | 5 | 0 | 0 | 0 memtable_post_flush | 0 | 1 | 20 | 0 | 0 | 0 memtable_reclaim_memory | 0 | 1 | 5 | 0 | 0 | 0 migration_stage | 0 | 1 | 0 | 0 | 0 | 0 misc_stage | 0 | 1 | 0 | 0 | 0 | 0 mutation_stage | 0 | 32 | 247 | 0 | 0 | 0 native_transport_requests | 1 | 128 | 28 | 0 | 0 | 0 pending_range_calculator | 0 | 1 | 2 | 0 | 0 | 0 per_disk_memtable_flush_writer_0 | 0 | 2 | 5 | 0 | 0 | 0 read_repair_stage | 0 | 8 | 0 | 0 | 0 | 0 read_stage | 0 | 32 | 13 | 0 | 0 | 0 repair_task | 0 | 2147483647 | 0 | 0 | 0 | 0 request_response_stage | 0 | 8 | 0 | 0 | 0 | 0 sampler | 0 | 1 | 0 | 0 | 0 | 0 scheduled_fast_tasks | 0 | 2147483647 | 1398 | 1 | 0 | 0 scheduled_heartbeat | 0 | 2147483647 | 14 | 1 | 0 | 0 scheduled_hotness_tracker | 0 | 2147483647 | 0 | 1 | 0 | 0 scheduled_non_periodic_tasks | 0 | 2147483647 | 10 | 0 | 0 | 0 scheduled_optional_tasks | 0 | 2147483647 | 5 | 8 | 0 | 0 scheduled_summary_builder | 0 | 2147483647 | 0 | 1 | 0 | 0 scheduled_tasks | 0 | 2147483647 | 194 | 74 | 0 | 0 secondary_index_management | 0 | 1 | 0 | 0 | 0 | 0 validation_executor | 0 | 2147483647 | 0 | 0 | 0 | 0 view_build_executor | 0 | 1 | 0 | 0 | 0 | 0 view_mutation_stage | 0 | 32 | 0 | 0 | 0 | 0 { code } simply using existing { { dataset } } . tiny number thread pools 's opinion worth complicate things semi-persistent implementation.\\r\\n\\r\\nthat said 'm huge fan query jmx names building column values metrics . 's dissimilar serializing things maps deserializing back consumption virtual tables . dealing { { localawareexecutorservice } } instances directly instead going jmx metrics . external code all.\\r\\n\\r\\nthe closest currently { { stagemanager.stages } } map every thread pool . wonder either put relevant thread pools n't currently stages map different registry somewhere executors.\\r\\n\\r\\ni started branch [ here|https : //github.com/iamaleksey/cassandra/tree/14523-review ] - far cleanups nits 'm half-way coding meant . update ticket 'm done . '' `` [ ~cnlwsu ] made requested changes . pushed bit [ here|https : //github.com/iamaleksey/cassandra/commits/14523-4.0 ] . cleans existing things bit makes consistent implements override single-partition-key reads among things.\\r\\n\\r\\nit 's going ci [ here|https : //circleci.com/workflow-run/f3956836-29d7-4570-aef2-07fa2b6cae11 ] . ci happy ok latest changes [ ~cnlwsu ] 'll commit version . '' 'committed trunk [ d5ae2ae481545b1fb2332b46013088f2f8cea636|https : //github.com/apache/cassandra/commit/d5ae2ae481545b1fb2332b46013088f2f8cea636 ] thanks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>diagnostic events guardrails add diagnostic events guardrails , monitor type guardrail triggered . patch adding diagnostic events guardrails : \\r\\n||pr||ci||\\r\\n| [ trunk|https : //github.com/apache/cassandra/pull/1485 ] | [ j8|https : //app.circleci.com/pipelines/github/adelapena/cassandra/1340/workflows/58cc1a02-7a5a-4d60-869f-c698b56d66df ] [ j11|https : //app.circleci.com/pipelines/github/adelapena/cassandra/1340/workflows/2ea9c9a1-288f-4c64-9839-611e640da42b ] |\\r\\n\\r\\nit creates new type diagnostic event called [ { { guardrailevent } } |https : //github.com/adelapena/cassandra/blob/17197-trunk/src/java/org/apache/cassandra/db/guardrails/guardrailevent.java ] . two types guardrail event { { warned } } { { { } failed { } } } matching soft hard activation guardrail . events carry two properties name specific guardrail specific error message.\\r\\n\\r\\nas testing consumer/listener new type diagnostic events attached every { { { } guardrailtester { } } } every time check activation guardrail also check proper events also emitted . ' `` important detail messages emitted guardrail triggered might contain user data . none current guardrails proposed guardrails collection items size ( cassandra-17153 ) include primary key offending row 's likely incoming guardrails same.\\r\\n\\r\\nthis user data n't included diagnostic events n't sent external systems monitoring diagnostic events . added [ commit|https : //github.com/apache/cassandra/pull/1485/commits/97329b94fb3de770aaae64880cef2b9cff857a00 ] pr allows redact sensitive data messages included guardrail diagnostic events . approach redacting messages based previous patch [ ~gerrrr ] .\\r\\n\\r\\ni 'd fine separate ticket given n't yet guardrail publishing user data . however since security thing think 'd prefer include diagnostic events n't miss adding guardrails.\\r\\n\\r\\nhere ci updated patch : \\r\\n||pr||ci||\\r\\n| [ trunk|https : //github.com/apache/cassandra/pull/1485 ] | [ j8|https : //app.circleci.com/pipelines/github/adelapena/cassandra/1345/workflows/1638df69-2729-4222-872f-4f3e081bff1b ] [ j11|https : //app.circleci.com/pipelines/github/adelapena/cassandra/1345/workflows/ef04d0d2-04a4-4675-83db-7fb0ffe21ea4 ] | '' 'cc [ ~smiklosovic ] [ ~dcapwell ] \\xa0 ' 'cassandra-17430 added identifying names guardrails identically proposed patch . simplifies patch bit rebasing top changes : \\xa0\\r\\n||pr||ci||\\r\\n| [ trunk|https : //github.com/apache/cassandra/pull/1485 ] | [ j8|https : //app.circleci.com/pipelines/github/adelapena/cassandra/1360/workflows/7aeb9214-e7fd-40d0-b551-5740c577004e ] [ j11|https : //app.circleci.com/pipelines/github/adelapena/cassandra/1360/workflows/657078e3-a6c2-4305-95c7-66c472fe3bc1 ] | ' ' 100 % . ' ' [ ~adelapena ] could please rebase top current trunk ? getting conflicts trying rebase . ' `` great news ! think rebased recently pr n't show conflicts rebased case.\\r\\n||pr||ci||\\r\\n| [ trunk|https : //github.com/apache/cassandra/pull/1485 ] | [ j8|https : //app.circleci.com/pipelines/github/adelapena/cassandra/1368/workflows/88ab802f-ccea-4176-942b-914551df13bd ] [ j11|https : //app.circleci.com/pipelines/github/adelapena/cassandra/1368/workflows/039fc777-3663-488a-b635-750ecccc0552 ] | '' `` ok dropped suggestion add test lines otherwise lgtm . check ci also lgtm repeated runs 2 failing workers . test results uploaded looking failed logs tests looked passed . 's probably env issue causing red . would need confirmed new repeatable run triggered . otherwise +1 . '' `` thanks review . added suggestion 'm running ci let 's see better luck workers : \\r\\n||pr||ci||\\r\\n| [ trunk|https : //github.com/apache/cassandra/pull/1485 ] | [ j8|https : //app.circleci.com/pipelines/github/adelapena/cassandra/1375/workflows/5133718b-8143-4382-87df-ea2b1c3e7df4 ] [ j11|https : //app.circleci.com/pipelines/github/adelapena/cassandra/1375/workflows/a7bfde6d-4e03-428b-9886-3671213c0f1c ] | '' `` seems new tests flaky 'm trying figure 's going . '' `` seems problem { { { } cqltester { } } } 's asynchronous cleanup task messing tests already found fixing flaky tests . task produces asynchronous flush tables created test . since tables contain large collections asynchronous flushing emit diagnostic events might visible next tests.\\r\\n\\r\\ni modified tests immediately drop tables termination n't anything async cleanup task . also disabled autocompaction created tables since might problematic too.\\r\\n\\r\\nhere ci doubling number iterations : \\r\\n||pr||ci||\\r\\n| [ trunk|https : //github.com/apache/cassandra/pull/1485 ] | [ j8|https : //app.circleci.com/pipelines/github/adelapena/cassandra/1384/workflows/93f9dee0-e2c8-4223-9b07-298d2f4a1ebe ] [ j11|https : //app.circleci.com/pipelines/github/adelapena/cassandra/1384/workflows/d4fdc205-eed0-4e8a-babc-874602646fd6 ] | '' `` yes makes sense . checked new ci convinced good commit . nice thing repeteable feature : ) +1\\r\\n\\r\\ni think 're waiting stefan +1 iiuc '' '+1 ' 'thanks reviews.\\r\\n\\r\\ncommitted { { trunk } } [ 143a5e8b064e442970182cfb349b4f0826683e85|https : //github.com/apache/cassandra/commit/143a5e8b064e442970182cfb349b4f0826683e85 ] .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>bound statement executions fail adding collection-type column adding collection-type column existing table , executions statements already prepared result server error ( error code 0 ) , error message { { java.lang.arrayindexoutofboundsexception } } . reproduce . { code : java } session.execute ( `` create table tbl1 ( text , b text , c text , primary key ( , b ) ) '' ) ; //prepare initially preparedstatement ps = session.prepare ( `` select , b , c tbl1 '' ) ; //insert data session.execute ( `` insert tbl1 ( , b , c ) values ( 'a1 ' , 'b1 ' , 'c1 ' ) '' ) ; //executes successfully expected session.execute ( ps.bind ( ) ) ; //add column collection type session.execute ( `` alter table tbl1 add set &lt; text &gt; '' ) ; //all following executions fail session.execute ( ps.bind ( ) ) ; { code } notes : - occurs select fields ( select * ) - occurs c * 2.0. probably cassandra-7910 applied 2.1+ - occurs column added collection type ( list / set / map ) - occurs select statements using column family , already prepared . repreparing hosts fixes issue , , user normally restart existing application ( even existing apps/apps versions n't handle new field ) . 2\\n\\tat org.apache.cassandra.cql3.statements.columngroupmap.add ( ) \\n\\tat org.apache.cassandra.cql3.statements.columngroupmap.access $ 200 ( ) \\n\\tat org.apache.cassandra.cql3.statements.columngroupmap $ builder.add ( ) \\n\\tat org.apache.cassandra.cql3.statements.selectstatement.processcolumnfamily ( ) \\n\\tat org.apache.cassandra.cql3.statements.selectstatement.process ( ) \\n\\tat org.apache.cassandra.cql3.statements.selectstatement.processresults ( ) \\n\\tat org.apache.cassandra.cql3.statements.selectstatement.execute ( ) \\n\\tat org.apache.cassandra.cql3.statements.selectstatement.execute ( ) \\n\\tat org.apache.cassandra.cql3.queryprocessor.processstatement ( ) \\n\\tat org.apache.cassandra.cql3.queryprocessor.processprepared ( ) \\n\\tat org.apache.cassandra.transport.messages.executemessage.execute ( ) \\n\\tat org.apache.cassandra.transport.message $ dispatcher.messagereceived ( ) \\n\\tat org.jboss.netty.channel.simplechannelupstreamhandler.handleupstream ( ) \\n\\tat org.jboss.netty.channel.defaultchannelpipeline.sendupstream ( ) \\n\\tat org.jboss.netty.channel.defaultchannelpipeline $ defaultchannelhandlercontext.sendupstream ( ) \\n\\tat org.jboss.netty.handler.execution.channelupstreameventrunnable.dorun ( ) \\n\\tat org.jboss.netty.handler.execution.channeleventrunnable.run ( ) \\n\\tat java.util.concurrent.threadpoolexecutor.runworker ( ) \\n\\tat java.util.concurrent.threadpoolexecutor $ worker.run ( ) \\n\\tat java.lang.thread.run ( ) \\n { code } ' `` problem c * 2.0 { { selectstatement } } aliases { { cfdefinition } } instead { { cfmetadata } } former post-processed version { { cfmetadata } } suited cql . except altering table { { cfmetadadata } } rebuilds new updated { { cfdefinition } } prepared statements still alias old version updated information . result compute broken index { { columngroupmap.builder } } ( comparator passed proper one collections infos { { hascollections } } flag outdated ) . good fix probably { { selectstatement } } alias { { cfmetadata } } directly call { { cfmetadata.getcfdef } } needs { { cfdefinition } } object . [ ~blerer ] look soonish ? \\n\\ni 'll note wo n't affect 2.1+ cassandra-7910 . '' ' planing look right : - ) ' 'the patch changes { { selectstatement } } store { { cfmetadata } } instead { { cfdefinition } } . { { cfdefinition } } created { { execute } } methods passed methods need . ' '+1 committed ( 2.0 merged { { -- strategy=ours } } 2.1 since nothing applies ) .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>assert error storageproxy.submithint 2.1-rc1 . assert error hector based client ends nodes message ( single node cluster ) . assume client connection got closed . info compacting [ sstablereader ( path= ' c : \\cassandra-2.1\\data\\test\\sipdb- 58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-3-data.db ' ) , sstablereader ( path=' c : \\cassandra-2.1\\data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka- 1-data.db ' ) , sstablereader ( path= ' c : \\cassandra-2.1\\data\\test\\sipdb-58f51090ee6511 e3815625991ef2b954\\test-sipdb-ka-4-data.db ' ) , sstablereader ( path= ' c : \\cassandra-2 .1\\data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-2-data.db ' ) , stablereader ( path= ' c : \\cassandra-2.1\\data\\test\\sipdb-58f51090ee6511e3815625991ef2 b954\\test-sipdb-ka-6-data.db ' ) ] error exception thread thread [ , main ] java.lang.assertionerror : localhost/127.0.0.1 org.apache.cassandra.service.storageproxy.submithint ( storageproxy.jav ) ~ [ ] org.apache.cassandra.service.storageproxy.mutate ( 3 ) ~ [ ] org.apache.cassandra.service.storageproxy.mutatewithtriggers ( storagep ) ~ [ ] org.apache.cassandra.thrift.cassandraserver.doinsert ( cassandraserver . ) ~ [ ] org.apache.cassandra.thrift.cassandraserver.doinsert ( cassandraserver . ) ~ [ ] org.apache.cassandra.thrift.cassandraserver.batch_mutate ( cassandraser ) ~ [ ] org.apache.cassandra.thrift.cassandra $ processor $ batch_mutate.getresul ( ) ~ [ ] org.apache.cassandra.thrift.cassandra $ processor $ batch_mutate.getresul ( ) ~ [ ] org.apache.thrift.processfunction.process ( ) ~ [ ] org.apache.thrift.tbaseprocessor.process ( ) ~ [ li ] org.apache.cassandra.thrift.customtthreadpoolserver $ workerprocess.run ( ) ~ [ ] java.util.concurrent.threadpoolexecutor.runworker ( threadpoolexecutor . ) ~ [ ] java.util.concurrent.threadpoolexecutor $ worker.run ( threadpoolexecutor ) ~ [ ] java.lang.thread.run ( ) ~ [ ] info 1 mutation messages dropped last 5000ms bug affects wte handling w/ cl.any . ' `` bikeshedding keep equality check submithint -- worried could hide bugs silently . prefer force callers think 're . '' `` sure agree 's trivial committed anyway . thanks . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>add support reversedtype would nice add native syntax use reversedtype . 'm sure anything sql inspired , would propose something like : { noformat } create table timeseries ( key text , time uuid , value text , primary key ( key , time desc ) ) { noformat } alternatively , desc could also put column name definition one argument putting pk instead apply keys . syntax naturally imply semantic ) .\\n\\nbut anyway since we\\ 're going nowhere i\\ 'm giving up.\\n\\ni\\ 'm attaching patch alternative you\\ 've suggested . think that\\ 's best solution cql pure technical point point solution it.\\n ' `` may misreading looks like treats { { order x desc asc } } { { order asc x desc } } since iterates hashmap 's entryset order . simple fix might use linkedhashmap . '' `` hashmap talking ? 's { { stmt.parameters.orderings } } actual order elements map used code n't matter . 'll note test patch https : //github.com/riptano/cassandra-dtest/blob/master/cql_tests.py ( line { { require ( ' # 4004 ' ) } } needs commented actually run test ) showing order x desc asc reverse order asc x desc.\\n\\nas side note 've convinced 4004_alternative.txt maybe bit natural creating new reversed type 'm good that.\\n\\n '' `` concern order terms { { order } } clause relevant -- dtest c1 always given first ( valid ) . n't think code would reject { { c2 desc c1 asc } } although should.\\n\\nso 'm looking either reference ordering { { columnaliases } } ordered map { { definedorderings } } 'm seeing . '' `` 're right . attached v2 fixes ( 've added check catch dtest ) . '' '+1 ' 'committed thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>nodetool compactionstats say disk compaction writing description https : //github.com/apache/cassandra/pull/1801\\r\\n\\r\\none test repeatedly failing ( related pr ) ' '+1 ' `` { quote } one test repeatedly failing ( related pr ) \\r\\n { quote } \\r\\ni guess 's { { { } org.apache.cassandra.tools.standaloneupgraderonsstablestest.testupgradesnapshot { } } } . seems caused patch since [ repeated runs ci|https : //app.circleci.com/pipelines/github/adelapena/cassandra/2009/workflows/e32acbf3-05e2-4cdb-a60d-154aa4326f78 ] [ immediately previous commit|https : //github.com/adelapena/cassandra/commit/c4b1c0614e42b4ea2064822d31c28aa5d4f1450a ] n't hit it.\\r\\n\\r\\ni opened cassandra-17849 . '' `` bq . one test repeatedly failing ( related pr ) \\r\\n\\r\\ni unfortunately confused cassandra-17804.\\r\\n\\r\\nbq . opened cassandra-17849 it.\\r\\n\\r\\nthanks 'll take look . '' 'interesting thinking flake totally unrelated failing timeout . repeated run ticket though.\\n\\ni take look too.\\n\\n\\nsent protonmail mobile\\n\\n\\n\\n\\\\</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>make consistency level user-level auth reads writes configurable reads auth-related tables execute { { local_one } } . 'd like make configurable , default still { { local_one } } . [ link|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/87/workflows/12f26230-abb0-4a67-947b-63f5ff18020c ] '' '+1 ' `` one minor change commit - 'd originally\\xa0performed { { cassandrarolemanager.getallroles ( ) } } call cl.quorum deliberately pr flipped authproperties configured level . reverted cl.quorum javadocced method 's clear future cleared w/benjamin slack.\\r\\n\\r\\ngoing push dtest pr shortly . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>improve leveledscanner work estimation see https : //issues.apache.org/jira/browse/cassandra-5222 ? focusedcommentid=13577420 &amp; page=com.atlassian.jira.plugin.system.issuetabpanels : comment-tabpanel # comment-13577420 filters sstables intersecting range given\\n\\ni might totally misunderstood comment cassandra-5222 though . \\n\\nit also `` solves '' cassandra-5249 another way left code though since amount work needs done . ' `` would say go ahead rip emptycompactionscanner.\\n\\ni also hoping could get better estimate sstables range 're scanning part sstable . one way would lookups in-memory indexsummary start end range assume 1/10 rows summary correspond 1/10 size disk . obviously 100 % accurate lot better throwing entire sstable size time . '' 'does better job estimating work\\n\\nalso removes emptycompactionscanner changes sstableboundedscanner constructor back taking range ( felt bit cleaner ) ' 'my ocd wanted get rid == null test . : ) ' 'looks like missed negation : \\nif ( intersecting.isempty ( ) ) \\n\\nv4 attached fix ' 'thanks committed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>auto-guessed memtable sizes high 've seen two cases memtable sizes large , causing ooming . too-small memtables hurt performance , too-large hurts worse start gc storming . [ `` 'd like introduce dependency number user cfs 's available 've picked value . cuts size half feels scientific old default 1gb heap size gives us memtable throughput 64mb also old default . '' '+1 ' 'committed ' 'integrated cassandra # 573 ( see [ https : //hudson.apache.org/hudson/job/cassandra/573/ ] ) \\n reduce automaticallychosen memtablesizes 50 % \\npatch jbellis ; reviewed brandonwilliams cassandra-1641\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>reading cardinality statistics.db failed issue sstable metadata visible system.log , messages says : { noformat } warn [ thread-6 ] 2018-07-25 - reading cardinality statistics.db failed /opt/data/disk5/data/keyspace/table/mc-big-data.db . { noformat } although file . message appeared 've changed compaction strategy sizetiered leveled . compaction strategy changed region region ( total 3 regions ) coincided double client write traffic increase . tried run nodetool scrub rebuilt sstable , fix issue . hard define steps reproduce , probably : # run stress tool write traffic # load change compaction strategy siretiered leveled bunch hosts # add write traffic reading code said metadata broken , `` estimating keys done using index summary '' . [ https : //github.com/apache/cassandra/blob/cassandra-3.0.17/src/java/org/apache/cassandra/io/sstable/format/sstablereader.java # l247 ] hey vitali thanks report . [ ~krummas ] idea going ? would changing compaction strategies cause issue ? ' ' [ ~nezdali ] could post logs ? ' 'this due stcs - &gt; lcs . behavior one cluster lcs heavy writes.\\xa0stcs never configured . ' 'looks like happen table metric ` estimatedpartitioncount ` [ queried |https : //github.com/apache/cassandra/blob/d049c6b9b4af4f663aac2bf90d860c3b0c20684a/src/java/org/apache/cassandra/metrics/tablemetrics.java # l307 ] - grabs canonical sstables without referencing many sstables might get compacted away calculating partition count get warning\\r\\n\\r\\nif case really problem ( annoying warn message log files ) \\r\\n\\r\\n [ ~rha ] could verify querying metric ? \\r\\n\\r\\n [ ~rha ] / [ ~nezdali ] could pause querying metric check error stops appearing ? \\r\\n\\r\\nthanks providing logs email btw [ ~nezdali ] ' 'thanks marcus . stopped quering\\xa0estimatedpartitioncount metric 1 node . ' 'unfortunately help failed cardinality message still log . ' `` query partition count datadog jmx agent happens time.\\xa0i 'll try disabled - although n't work [ ~nezdali ] '' `` removing { { estimatedpartitioncount } } datadog 's cassandra.yaml one node n't see warning 4 days.\\r\\n\\r\\n [ ~nezdali ] double check change effective ( e.g . monitoring service restarted etc . ) ? \\xa0 '' `` alias metric called { { estimatedrowcount } } also disabled\\r\\n\\r\\ni 'll work fix '' 'tested warning log whole day . turned monitoring system querying keys via jmx filters based configuration file . ' ' [ ~krummas ] created patch uploaded ticket\\xa0 [ ^14647-trunk-1.patch ] ' `` { { tablemetrics } } change looks good ( grabbing refs sstables checking key count ) n't think need { { sstablereader.refandgetapproximatekeycount } } part - cases called always already ref sstable . '' ' [ ~krummas ] updated patch.\\xa0\\r\\n||branch||circleci||\\r\\n| [ 14647-trunk|https : //github.com/apache/cassandra/compare/trunk ... ] | [ link|https : //circleci.com/gh/nvharikrishna/cassandra/3 # tests/containers/2 ] | ' '+1\\r\\n\\r\\nre-ran tests failures look unrelated ; \\r\\n\\r\\nhttps : //circleci.com/workflow-run/264bb5cb-fd18-4ea2-8b1d-768d9ab96d94 ' 'sorry delay - committed { { ded62076e7fdfd1cfdcf96447489ea607ca796a0 } } trunk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>secondary indexes always rebuilt startup following cassandra-10130 , bug introduced causes 2i rebuilt startup , even index already built . indexes created column family initialization marked `` built '' avoid rebuilding needlessly.\\n\\n -- -- \\n ' 'this caused calling { { sim # markindexesbuilding } } creating index column family initialization marks index `` built '' causes index initialization task rebuild it.\\n\\ngiven there\\ 's need mark index new column family created ( index `` built '' definition can\\'t concurrent indexing ) pass boolean { { createindex ( ) } } distinguish index creation different times i.e . column family [ created|https : //github.com/sbtourist/cassandra/blob/cassandra-13725/src/java/org/apache/cassandra/db/keyspace.java # l394 ] [ reloaded|https : //github.com/sbtourist/cassandra/blob/trunk/src/java/org/apache/cassandra/db/columnfamilystore.java # l129 ] .\\n\\nsuch solution implemented following patch new dtest verifying : \\n| [ trunk|https : //github.com/apache/cassandra/pull/135 ] | [ dtest|https : //github.com/apache/cassandra-dtest/pull/2 ] |\\n\\ntest runs progress internal ci report results soon they\\ 're ready.\\n ' 'both patch dtest look good ci results seem ok +1 . ' 'committed [ 6e19e81db8e4c43bf5ef33308de1ae79916bb61c|https : //github.com/apache/cassandra/commit/6e19e81db8e4c43bf5ef33308de1ae79916bb61c ] . ' 'dtests committed [ b724df80d3bbb55b6b41845633e3a9034116f3be|https : //github.com/apache/cassandra-dtest/commit/b724df80d3bbb55b6b41845633e3a9034116f3be ] .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>add latency metrics dropped messages production cluster , seeing large number dropped mutations . minimum , print time thread took get scheduled thereby dropping mutation ( also print message / mutation helps figuring column family affected ) . help find right tuning parameter write_timeout_in_ms . change small storageproxy.java messagingtask.java . submit patch shortly . care sanity check ? ' ' [ ~joshuamckenzie ] +1 thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>throw invalid partitioner class add cause exception allow log spun cassandra-13158 exception creating partition mask original error hides fix problem . [ circle ci|https : //circleci.com/gh/dcapwell/cassandra/tree/feature % 2fimproveerrormessagewhenpartitionisrejected ] ' ' [ ~brandon.williams ] could review ? since involved jira would good get look . ' `` full review quickly skimming pr . please add unit test reasonably possible . tests { { databasedescriptor } } would good add new ones touch existing code n't unit test . needs cover change . '' ' [ ~djoshi ] pushed 2 tests failure case one yaml one properties . ' 'committed thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>run lcs repaired unrepaired data user leveled compaction configured , run unrepaired repaired data . think would make things lot easier end users would simplify migration incremental repairs well , user runs incremental repair nice leveled unrepaired data , wont need drop l0 , instead start moving sstables unrepaired leveling straight repaired leveling idea could two instances leveledcompactionstrategy move sstables instances incremental repair run ( let lcs totally oblivious whether handles repaired unrepaired data ) . probably apply compaction strategy , run two instances remove repaired/unrepaired logic strategy . https : //github.com/krummas/cassandra/commits/marcuse/8004 ' 'bq . new methods default implementations old compaction strategies still work even don\\'t track sstables - get sstables compact cfs split repaired/unrepaired . might twice per call \\'getnextbackgroundtasks\\ ' though fine since mark sstables compacting\\n\\nhmm yeah guess fine long using strategy repaired un-repaired probably ok. issue see start something like lcs repaired stcs un-repaired . stcs wasn\\'t tracking sstables would trash everything . call 3rd party repair twice 2nd call decide `` nothing '' move . ' 'yeah always use strategy repaired unrepaired ' `` ca n't shake feeling better major-er refactor guess much 2.1 - wait 3.0 changes.\\n\\nwrt new acs methods - claiming default implementation thus n't breaking compatibility dishonest . stock implementations track sstables thus * * override would 3rd party compaction strategy implementation . however given importance change ( making incremental repair usable ) 'm okay breaking long commit makes 2.1.2. please mark acs # addsstable ( ) acs # removesstable ( ) abstract least nobody gets burned silently.\\n\\nminor nits : \\n- reference redundant dtcs addsstable ( ) removesstable ( ) ( make dtcs.options private final dtcs.sstables final ) \\n- 'this ' nit lcs stcs\\n\\nother lgtm/+1 - agree comments make modifications commit . '' 'once last thing . safe sstables set simple hashset wrt visibility concurrent updates ? ' 'committed\\n\\nbq . safe sstables set simple hashset\\n\\neverything synchronized wrappingcompactionstrategy ok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>include metadata system keyspace schema_ * tables ` system.schema_keyspaces ` , ` system.schema_columnfamilies ` , ` system.schema_columns ` virtual tables allow clients query schema layout information cql . invaluable users start make use cql-only protocol ( cassandra-2478 ) , since way determine certain information available columnfamilies , keyspaces , show metadata . however , system keyspace , columnfamilies , represented schema_ * tables : { noformat } cqlsh &gt; select * system.schema_keyspaces `` keyspace '' = 'system ' ; cqlsh &gt; cqlsh &gt; select * system.schema_columnfamilies `` keyspace '' = 'system ' ; cqlsh &gt; cqlsh &gt; select * system.schema_columns `` keyspace '' = 'system ' ; cqlsh &gt; { noformat } would greatly helpful clients introspection minimum ( say , example , cqlsh ) able get information structure availability schema-definition tables . attempting load already loaded column family system.batchlog\\n\\nin fact n't even need create new keyspace - start/stop cassandra couple times.\\n '' `` problem load table system table system ones already loaded n't loaded second time . attaching patch ignore system keyspaces reading schema tables ( write client sake never need internally let n't ) . '' '+1 ' 'alright fix committed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>cql3 token queries broken currently select statement uses token ( ) predicate breaks `` bad input '' tracing logic error caused gettokenbounds assumes token term actual token string pass tokenizer +1 committed thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>remove ability disable dynamic snitch entirely 've moved dynamic snitch `` new , default '' `` well tested , default true , '' 's time take next step `` reason disable , keeping option around lets people shoot foot . '' ben coverston points conservative approach would leave option remove example config file first . ' `` 'm conservative approach . general 'm favor fully disabling options fear people shooting foot think always end resulting pissing power users one point . case ds able disable debugging purposes may prove useful . 'm totally hidden options listed advanced docs proper warnings . '' `` defaulting last two major releases 've never heard anyone disabling 'm fine removing option . '' 'patch removes dynamic_snitch boolean example configuration ( defaulting true ) sets default badness threshold 0.1\\n ' '+1 ' 'committed ' ' agree dynamic snitch super stable 99.99999 % time everyone wants . however always remain option . imagine situation want dynamic snitch . example user may want snitch always sends reads predictable places . case able disable dynamic would bad . ' `` ca n't anyway failure detector force reads different replica thinks preferred one . '' `` experimenting workload split oltp olap workload using network topology strategy . oltp keyspace replicas ( virtual ) dcs . oltp would written nodes dc1 read olap input dc2 . n't know yet right approach dynamic snitch ( read repair ) case nodes hope avoid olap reads show dc1 . without able switch dynamic snitch - sure able achieve workload split ( proven gives performance benefit ) . '' 'you use badness threshold cassandra-1519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>handle ir prepare phase failures less race prone waiting results handling incremental repairs coordinator begins sending { { prepareconsistentrequest } } message participants , may also include coordinator . participants run anti-compactions upon receiving message report result operation back coordinator . receive failure response participants , fail-fast { { coordinatorsession.handleprepareresponse ( ) } } , turn completes { { preparefuture } } { { repairrunnable } } blocking . repair command terminate error status , expected . issue case node coordinator participant , may end local session submitted anti-compactions , executed without coordination coordinator session ( node ) . may result situations running repair commands right another , may cause overlapping execution anti-compactions cause following ( misleading ) message show logs cause repair fail : `` prepare phase incremental repair session % failed encountered intersecting sstables belonging another incremental repair session ( % ) . starting incremental repair session previous one completed . check nodetool repair_admin hung sessions fix . '' proposed fixes [ here|https : //github.com/bdeggleston/cassandra/commit/02d7d9e09983db0d4661486b17adc375e17be24f ] '' 'lgtm +1 ' 'committed trunk as\\xa09bde713ee8883f70d130efb6290ec0e6daea524f thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>anticompaction log message n't include parent repair session id appears even though incremental repair enabled default post c * -3.0 ( means end repair session , anti-compaction step needs executed ) , n't include parent repair session uuid log message anti-compaction log entries . makes observing activities related incremental repair session difficult . see following : { noformat } debug [ ] 2016-07-13 - got anticompaction request anticompactionrequest { parentrepairsession=27103de0-489d-11e6-a6d6-cd06faa0aaa2 } org.apache.cassandra.repair.messages.anticompactionrequest @ 34449ff4 &lt; ... &gt; &lt; snip &gt; &lt; ... &gt; info [ ] 2016-07-13 - starting anticompaction trivial_ks.weitest 1/ [ bigtablereader ( path='/var/lib/cassandra/data/trivial_ks/weitest-538b07d1489b11e6a9ef61c6ff848952/mb-1-big-data.db ' ) ] sstables info [ ] 2016-07-13 - sstable bigtablereader ( path='/var/lib/cassandra/data/trivial_ks/weitest-538b07d1489b11e6a9ef61c6ff848952/mb-1-big-data.db ' ) fully contained range ( -9223372036854775808 , -9223372036854775808 ] , mutating repairedat instead anticompacting info [ ] 2016-07-13 - completed anticompaction successfully { noformat } initial submission anti-compaction task compactionmanager still reference parent repair session uuid , subsequent anti-compaction log entries missing parent repair session uuid . [ 3.0|https : //github.com/tommystendahl/cassandra/tree/cassandra-12186-30 ] [ 3.x|https : //github.com/tommystendahl/cassandra/tree/cassandra-12186-3x ] [ trunk|https : //github.com/tommystendahl/cassandra/tree/cassandra-12186-trunk ] . ' 'lgtm thanks ! submitted ci make sure break tests commit everything looks right : \\n\\n||trunk||\\n| [ branch|https : //github.com/apache/cassandra/compare/trunk ... pauloricardomg : trunk-12186 ] |\\n| [ testall|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-trunk-12186-testall/lastcompletedbuild/testreport/ ] |\\n| [ dtest|http : //cassci.datastax.com/view/dev/view/paulomotta/job/pauloricardomg-trunk-12186-dtest/lastcompletedbuild/testreport/ ] | ' 'committed 2256778726319fb76b6d85c4a47a957116c78147 3.0 merged . thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>move system cfs table description [ `` commit message 04 reads\\n\\n dbmanager obscures rather clarifies going . remove it.\\n\\n point stopped calling dbmanager.instance ( ) table.onstart was\\n getting called . move cassandradaemon n't rely side\\n effect run it.\\n\\nthe others visible filename . '' 'note hints using one row per table whereas record table info hints cf . ' `` ca n't apply 0002 cleanly trunk . could rebase ? thanks `` 'deleted old patches uploaded new ' 'the patch looks fine . ' 'committed ' `` integrated cassandra # 119 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/119/ ] ) \\n cleanup cfs.onstart\\npatch jbellis ; reviewed jun rao \\ndbmanager obscures rather clarifies going . remove it.\\n\\nat point stopped calling dbmanager.instance ( ) table.onstart was\\nnot getting called . move cassandradaemon n't rely side\\neffect run it.\\n\\npatch jbellis ; reviewed jun rao \\nuse normal table system metadata\\npatch jbellis ; reviewed jun rao \\nr/m recycle cf undocumented/incomplete loader code using it\\npatch jbellis ; reviewed jun rao \\nmove hints cf system_table\\npatch jbellis ; reviewed jun rao \\n '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>implement user/keyspace throughput scheduler support multiple applications top single cassandra cluster ( protect badly behaving clients ) simple scheduler client operations would beneficial . since tasks short lived , sufficient scheduler would probably need manage queue incoming requests , weight based assigned id . id could dynamically determined using ip , userid keyspace instance , runnable would assigned id . keyspace may incoming requests b may case block syncqueue would block b able process 's requests b request arrives . \\nmore 's flood requests taskcount blocks release called whereas syncqueue would process everything . note however wo n't effective async requests . either way throttles incoming requests . \\nalso rr kinda fair rr sense 100 requests ( throttle say 40 ) would n't necessarily block request b assuming requests arrive simultaneously.\\n\\nthe queuesize added avoid busy wait scenario requests . \\n '' 'reordered semaphores rr.schedule ( ) little v5 ' 'v5 ever slightly slower wait/notify approach correctness simplicity triumph . +1 ! \\n\\n { quote } \\nstress.py default reads ( 1mm rows 5 cols 50 threads ) \\n\\nnosched\\n139 secs\\n\\nroundrobin ( throttle_limit = 80/default ) \\n153 secs\\n { quote } ' 'rebased committed ' 'integrated cassandra # 491 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/491/ ] ) \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>missing role manager cassandra.yaml causes unexpected behaviour upgrading 2.2+ , new { { role_manager } } option added { { cassandra.yaml } } , instance default { { cassandrarolemanager } } created initialization { { databasedescriptor } } . problem set role options supported { { crm } } depends configured { { iauthenticator } } , point time always { { allowallauthenticator } } . stackoverflow post describes problem ; configured authenticator { { passwordauthenticator } } , role manager allow roles created using { { password } } option , . http : //stackoverflow.com/questions/31820914/in-cassandra-2-2-unable-to-create-role-containing-password simple workaround ensure yaml contains role manager option { code } role_manager : cassandrarolemanager { code } https : //github.com/carlyeks/cassandra/commit/204f7bdd8ea0a18d5c642cb7d42104749b82a62b\\n\\nwhich looks reasonable 'm sure resources excluded client mode 'd rather halfway . '' `` bq . 'm sure resources excluded client mode 'd rather halfway.\\n\\nnot sure completely follow ; n't resources excluded client mode rather * * client mode ( cassandra.yaml n't read ) n't attempt figure { { system_auth } } tables modifiable . seems reasonable iauthenticator/iauthorizer/irolemanager set yaml wo n't correct anyway . '' `` want recommit carl 's patch ? '' `` sure wanted check n't misunderstanding meant '' `` committed [ ~carlyeks ] 's additions { { 0c0f1ff1b1051627f38a8bf6cb0776241586dfce } } .\\ni notice since revert { { uftest.testtypeswithandwithoutnulls } } failing timeout . doubly weird given revert plus test fine 3.0 &amp; trunk . 've also run running { { uftest } } loop locally seen errors 65 runs . \\n\\n [ ~mshuler ] ideas ? also another oddity { { cqlsstablewriterclienttest } } ( offending test 2.2 moved ) never failed 3.0 trunk even revert . would expected 3.0 hit npe [ build|http : //cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_testall/90/ ] indeed running test locally revision fails exactly expected way . \\n\\n '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>concurrentmodificationexception executing 'nodetool cleanup ' adding new node existing cluster ( 7 already started nodes ) , waiting minutes sure data migration new node completed , began use command nodetool cleanup sequentially old node . issued command third node , minutes got concurrentmodificationexception . ~ $ nodetool cleanup error : null -- stacktrace -- java.util.concurrentmodificationexception java.util.arraylist $ itr.checkforcomodification ( unknown source ) java.util.arraylist $ itr.next ( unknown source ) org.apache.cassandra.db.index.secondaryindexmanager.deletefromindexes ( ) org.apache.cassandra.db.compaction.compactionmanager $ cleanupstrategy $ full.cleanup ( ) org.apache.cassandra.db.compaction.compactionmanager.docleanupone ( ) org.apache.cassandra.db.compaction.compactionmanager.access $ 400 ( ) org.apache.cassandra.db.compaction.compactionmanager $ 5.execute ( ) org.apache.cassandra.db.compaction.compactionmanager $ 2.call ( ) java.util.concurrent.futuretask.run ( unknown source ) java.util.concurrent.threadpoolexecutor.runworker ( unknown source ) java.util.concurrent.threadpoolexecutor $ worker.run ( unknown source ) java.lang.thread.run ( unknown source ) null\\n -- stacktrace -- \\njava.util.concurrentmodificationexception\\n java.util.arraylist $ itr.checkforcomodification ( ) \\n java.util.arraylist $ itr.next ( ) \\n org.apache.cassandra.db.index.secondaryindexmanager.deletefromindexes ( ) \\n org.apache.cassandra.db.compaction.compactionmanager $ cleanupstrategy $ full.cleanup ( ) \\n org.apache.cassandra.db.compaction.compactionmanager.docleanupone ( ) \\n org.apache.cassandra.db.compaction.compactionmanager.access $ 400 ( ) \\n org.apache.cassandra.db.compaction.compactionmanager $ 5.execute ( ) \\n org.apache.cassandra.db.compaction.compactionmanager $ 2.call ( ) \\n java.util.concurrent.futuretask.run ( ) \\n java.util.concurrent.threadpoolexecutor.runworker ( ) \\n java.util.concurrent.threadpoolexecutor $ worker.run ( ) \\n java.lang.thread.run ( ) ' `` update happening across multiple nodes 'm guessing stacktrace 's related secondary indexes . '' 'additional details tried following procedures result : \\n\\nrestart cassandra cleanup.\\nrepair -pr cleanup ( errors repair ) \\nrepair cleanup ( errors repair ) \\nnodetool scrub cleanup ( errors scrub ) \\nnodetool rebuild_index ( index table ) cleanup ( errors rebuild_index ) ' 'seems reuse cleanupstrategy threads multithreaded cleanup patch fixes ' '+1 ' 'committed thanks ' ' @ marcus clear . affects concurrent compactions multithreaded compactions anyway removed 2.1 . \\n\\nas temporary workaround released set concurrent compactors 1 yaml .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>cas may return false still commit insert paxos proposer proposes value/update propose fail , guarantee whether value accepted ultimately . paxos guarantees 'll agree `` '' value ( given round case ) , guarantee proposer agreed upon value know . particular , given proposal least one accepter accepted quorum , value might ( 's guaranteed either ) replayed ( committed ) another proposer . currently , proposer proposes update u rejected , sleep bit retry u. u accepted least one acceptor , proposer b might replay u , succeed commit . retry happens , prepare , check condition , probably find conditions n't apply anymore since u committed already . thus return false , even though u fact committed . unfortunately 'm sure easy way proposer whose propose fails know update prevail eventually . mean acceptable solution see would return user `` n't know '' ( exception instance ) . annoying proposal rejected wo n't extremely rare occurrence , even relatively light contention , returning `` n't know '' often bit unfriendly . attaching v4 version ( equivalent v2 updated comment v3 ) .\\n '' '+1 ' 'committed thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>allow get_slice operate sc subcolumns , post cassandra-185 currently index subcolumn level . want add scan-based get_slice want add subcolumn index ? latter complicates storage format . ' 'for ticket happy say `` still assume supercolumns fit memory '' \\n\\njust want expose slice api level since get_columns_since goes away w/ cassandra-185 ' ' add get_slice supercolumn tests . add back updated timesorttest . need pass gcbefore filters slice count correctly.\\n ' 'all tests pass looks good . +1 ' 'committed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>dropping column families n't clean secondary indexes description patch replaces cf graveyard cfs.removeallsstables ( recursive handle 2ary index files ) ' `` putting deletion table.dropcf ( ) risky wrt ks cf renames . wo n't hurt anything right delete tries remove old files . side-effect ever went away 'd deleting valid files . stands patch 'd trying cleanup sstables files moved underneath probably n't best thing try.\\n\\nis good ticket lump renamekeyspace renamecolumnfamily n't address secondary indices new one ? '' 'otoh requiring caller calls dropcf also call methods finish job poor encapsulation implementing rename drop + add implementation detail . neither approach completely satisfactory imo.\\n\\n02 adds support 2ary indexes rename methods . ' ' ( tweaked defstest require renaming leave empty directory behind old name ) ' 'dropcf intended unload cfs table instance indicated comment . 0003 cleans interface renaming acts files like drop . ' `` +1 'd like 0003 included . '' 'committed w/ 03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>harden jmx resolving beanshooter issues fix jmx security vulnerabilities reported murray mcallister , multiple jmx vulnerabilities default cassandra configuration 3.0 , 3.11 , 4.0 trunk , across java 8 java 11. limited authenticated jmx users . vulnerabilities : 1 . ( java 8 11 ) remote java library loading execution via mlet 2 . ( java 11 ) remote java file reads via diagnosticcommandmbean's compilerdirectivesadd implementation leaking arbitrary file contents 3 . ( java 11 ) remote .so library loading via jvmti qtc-de/beanshooter jmx enumeration tool uses mechanisms others : https : //github.com/qtc-de/beanshooter/blob/2ec4f7a4b44a29f52315973fe944eb34bc772063/beanshooter/src/de/qtc/beanshooter/mbean/diagnostic/dispatcher.java # l48 remote file reads via compilerdirectiveadd appear reproducible java 8 ( cassandra- { 3.0,3.11 } , java 1.8.0_345-b01 adoptium / temurin ) . using qtc-de/beanshooter cassandra-3.0 ( a78db628 ) : { code } $ java -jar target/beanshooter-3.0.0-jar-with-dependencies.jar diagnostic read -- verbose 127.0.0.1 7199 /tmp/hello [ - ] method signature compilerdirectivesadd ( [ ljava.lang.string ; ) exist endpoint . [ - ] invoked deployed mbean , make sure correct version deployed . [ - ] continue . { code } java 8 also appears vulnerable remote library loading : { code } $ java -jar target/beanshooter-3.0.0-jar-with-dependencies.jar diagnostic load -- verbose 127.0.0.1 7199 /tmp/hello [ - ] method signature jvmtiagentload ( [ ljava.lang.string ; ) exist endpoint . [ - ] invoked deployed mbean , make sure correct version deployed . [ - ] continue . { code } java 8 appear vulnerable mlet : { code } $ java -jar target/beanshooter-3.0.0-jar-with-dependencies.jar tonka deploy -- stager-url http : //localhost:8000 127.0.0.1 7199 [ + ] starting mbean deployment . [ + ] [ + ] deplyoing mbean : tonkabean [ + ] [ + ] mbean class known server . [ + ] starting mbean deployment . [ + ] [ + ] deplyoing mbean : mlet [ + ] mbean object name defaultdomain : type=mlet successfully deployed . [ + ] [ + ] loading mbean http : //localhost:8000 [ + ] [ + ] creating http server : [ + ] creating mlethandler endpoint : / [ + ] creating jarhandler endpoint : /fb0f34fe7c4f456bb44c07d9650dbf1e [ + ] starting http server . [ + ] [ + ] incoming request : localhost [ + ] requested resource : / [ + ] sending mlet : [ + ] [ + ] class : de.qtc.beanshooter.tonkabean.tonkabean [ + ] archive : fb0f34fe7c4f456bb44c07d9650dbf1e [ + ] object : mlettonkabean : name=tonkabean , id=1 [ + ] codebase : http : //localhost:8000 [ + ] [ + ] incoming request : localhost [ + ] requested resource : /fb0f34fe7c4f456bb44c07d9650dbf1e [ + ] sending jar file md5sum : 39d35ebd20aee73fbb83928584a530d7 [ + ] [ + ] mbean object name mlettonkabean : name=tonkabean , id=1 successfully deployed . { code } java 11 appears vulnerable three vulnerabilities , using jdk adoptium / temurin 11.0.16.1+1 cassandra-4.0 ( 5beab63b ) . patch fixes issues introducing new system property : ` cassandra.jmx.security.profile ` , set `` restrictive '' ( default ) `` lax '' . restrictive profile blocks mechanisms three vulnerabilities , introducing jmx mbeanserveraccesscontroller . users use lax profile require mechanisms , use authorization proxy specifying ` cassandra.jmx.authorizer ` . committed [ b2660bcf78ccf08a3a0ae39a8c9ffb397efef9ff|https : //github.com/apache/cassandra/commit/b2660bcf78ccf08a3a0ae39a8c9ffb397efef9ff ] .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>support wrapped range queries want support scanning keyx keya x &gt; . ( thus alphabet would include x z . ) important allow hadoop scan key ring exactly . add wrapped range support + test ' '+1 looks good . ' 'committed ' 'integrated cassandra # 357 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/357/ ] ) \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>make request dropping accurate based discussion cassandra-2819 , make bookkeeping request times accurate . maybe sending integer timeout instead long save us bandwidth : ) ' `` bq . maybe sending integer timeout instead long save us bandwidth\\n\\nwhat mind ? milliseconds gets large int less year . even 100ths seconds . cutting 10ths second seems like 're losing much resolution . '' ' { quote } \\nwhat mind ? \\n { quote } \\ni talking lower part long 2 billion milliseconds ( 2147483.648 seconds ) count give us enough timeout ... varint save 4 bytes value . makes sense ? clear earlier . ' 'v2 sends low-order timestamp int . ' `` v3 fixes itc byte arithmetic per vijay 's suggestion.\\n\\n ( { { | ( input.readint ( ) &lt; &lt; 4 &gt; &gt; 4 ) } } also works vijay 's version simpler. `` '+1 \\nsorry delay spent lot time wondering test cases failing : ) ( looking wrong places ) \\nlooks like thats following setting sp\\n\\n { code } \\nprivate static final boolean optimize_local_requests = false ; \\n { code } \\n\\nonce set true good commit guess.\\nthanks ! ' `` oops n't mean include patch . ( cassandra-4617 open fix . ) '' 'committed ' `` think working expected.\\ntruncate hangs two node cluster using 1.2.0-beta1 binary truncate response gets dropped every time ( clitest trunk failing today ) .\\n\\ni think casting long system.currenttimemillis int fragile 's causing line\\n\\nhttps : //github.com/apache/cassandra/blob/cassandra-1.2.0-beta1/src/java/org/apache/cassandra/net/messagedeliverytask.java # l43\\n\\nalways evaluates true.\\nwhen tried system.currenttimemillis like 1348691631776 currenttime like 71900832 . '' 'the rebuilding logic v3 turns rely sign bit int 1 gets and-ed long gets sign-extended : \\n\\n { code } \\n . long foo = 0xffffffffffffffffl ; \\n int bar = 0xf0000000 ; \\n system.out.println ( long.tohexstring ( foo &amp; bar ) ) ; \\n system.out.println ( long.tohexstring ( foo &amp; 0 ) ) ; \\n { code } \\n\\nthis outputs\\n { noformat } \\nfffffffff0000000\\n0\\n { noformat } \\n\\nthus high order bit lower 32 bits currenttimemillis instead zero zeros entire high 32 long trying rebuild with.\\n\\npushed suggested alternative 822ee88a38b3862d60b50748382ddf7957907cec rely sign extension.\\n ' 'looks like still need minor edit 822ee88a38b3862d60b50748382ddf7957907cec change clitest passes.\\n\\nhttps : //git-wip-us.apache.org/repos/asf ? p=cassandra.git ; a=blobdiff ; f=src/java/org/apache/cassandra/net/incomingtcpconnection.java ; h=02b40d19855f87bbe82151c2f33b92119e32003c ; hp=eeb6b317bfbbac71e4c6d3e0a2253cd57922e707 ; hb=447fcef48630ea3f60a2c97f76910dbfa1a334f5 ; hpb=f3e24bd5162eede8ad13abc9c85c90dd971fc110\\n\\n\\n\\n\\n ' 'thanks vijay !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>add repair streaming preview would useful able estimate amount repair streaming needs done , without actually streaming . main motivation something validating cassandra-9143 production , ’ imagine could also useful tool troubleshooting . https : //github.com/bdeggleston/cassandra-dtest/tree/13257 ' '+1 ' 'this new feature covered docs news.txt . ' ' [ ~spodxx @ gmail.com ] added commit adding docs news.txt stuff [ here|https : //github.com/bdeggleston/cassandra/commit/dd3efd19179dae6297c95444c623c128976cb658 ] take look ? \\n\\ni made small change nodetool doc generator link generated docs pages . also added line upgrading/incremental repair section recommends users run full repair upgrading using incremental repair 3.x ' `` +1\\n\\nthanks taking opportunity add content repair page blake ! 'll add comments additional content separate pr top it. `` 'no problem thanks reminding me.\\n\\ncommitted { { 4cfaf855c404256a9dd281d5066cc076232d72ff } }</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>allow disabling slab allocation arena allocation ( cassandra-2252 ) dramatically improves garbage collection performance , limit number tables practically defined schema since minimum 1mb heap per table . although dozens hundreds tables defined almost certainly bad idea ( design smell relational database ) , 's relatively straightforward allow disabling slaballocator . note i\\ 've deliberately omitted setting cassandra.yaml ; see `` almost certainly bad idea . '' ' 'v2 fixes constructor visibility ' 'lgtm . ' 'committed 1.2.10 2.0.1 ( 2.0.0 ) .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>support total/recent latency histogram metrics range slices metrics histogram pretty bad non-normal data like latencies ( empirically tested theoretically ) untrustworthy 99th percentile . applications care percentiles statistically accurate version beneficial . adding deprecated methods like latency histograms cassandra-7338 temporarily would help . 2.1 branch . cassandra-5657 solves everything 3.0. committed thanks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>improve pre-cleanup estimate disk space required compaction sums sizes ranges estimate output size order pick target drive . anti-compactions directly drop ranges sstables , volume data dropped easily calculated using sstable index . using knowledge estimate columnfamilystore.getexpectedcompactedfilesize would allow 'nodetool cleanup ' run less 50 % disk available , case ranges dropped ( post node-move ) . 60 % full nodes become impossible cleanup largest sstable ( 256gb ) . ' `` ( targetting 1.1.1 think 's minor change . case let 's put back back burner . ) '' 'attaching patch calculate estimated compacted file size eliminating ranges performing cleanup . ' ' think makes `` / 2 '' guess obsolete ? ' `` right . need halve since actually calculating exact size cleanup'ed sstable . '' 'committed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>cluster topology change may produce false unavailable queries coordinator processes query , first gets { { replicationstrategy } } ( rs ) keyspace decide peers contact . , gets rs perform liveness check requested cl . rs volatile filed keyspace , possible 2 getter calls return different rs values presence cluster topology changes , e.g . add node , etc . scenario , check second step throw unexpected unavailable . perspective query , cluster satisfy cl . use consistent view rs peer selection cl liveness check . word , steps reference rs object . also clear easier reason clients . queries made topology change . [ ! https : //ci-cassandra.apache.org/job/cassandra-devbranch-dtest-upgrade/209/badge/icon ! |https : //ci-cassandra.apache.org/job/cassandra-devbranch-dtest-upgrade/209/ ] \\r\\n\\r\\n\\r\\n\\r\\n ' '+1 ' 'starting commit\\r\\n\\r\\nci results : \\r\\n||branch||source||circle ci||jenkins||\\r\\n|trunk| [ branch|https : //github.com/yifan-c/cassandra/tree/commit_remote_branch/cassandra-16545-trunk-a70927c8-3771-4980-809d-c36119b6b351 ] | [ build|https : //app.circleci.com/pipelines/github/yifan-c/cassandra ? branch=commit_remote_branch % 2fcassandra-16545-trunk-a70927c8-3771-4980-809d-c36119b6b351 ] | [ build|https : //ci-cassandra.apache.org/blue/organizations/jenkins/cassandra-devbranch/detail/cassandra-devbranch/658/pipeline ] |\\r\\n\\r\\nci jenkins unrelated failures.\\xa0 ' 'committed trunk [ b915688|https : //github.com/apache/cassandra/commit/b915688ea878aaa284f5cedeb799c5f797c4d824 ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>clean ksmetadata , cfmetadata many conversion methods thrift avro native , potential source bugs . \\n * /cassandra/trunk/test/unit/org/apache/cassandra/config/databasedescriptortest.java\\n * /cassandra/trunk/test/unit/org/apache/cassandra/config/columndefinitiontest.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/config/columndefinition.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/cql/dropindexstatement.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/migration/updatecolumnfamily.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/migration/migration.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/cql/queryprocessor.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/migration/addkeyspace.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/thrift/cassandraserver.java\\n * /cassandra/trunk/test/unit/org/apache/cassandra/thrift/thriftvalidationtest.java\\n * /cassandra/trunk/changes.txt\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/defstable.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/cql/altertablestatement.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/config/ksmetadata.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/migration/updatekeyspace.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/migration/addcolumnfamily.java\\n * /cassandra/trunk/test/unit/org/apache/cassandra/config/cfmetadatatest.java\\n * /cassandra/trunk/test/unit/org/apache/cassandra/db/defstest.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/config/cfmetadata.java\\n * /cassandra/trunk/test/unit/org/apache/cassandra/schemaloader.java\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>request specific column families using streamin streamin.requestranges specifies keyspace , meaning requesting range request column families : large number cfs , cause quite headache . \\n * /cassandra/branches/cassandra-0.8/conf/cassandra.yaml\\n ' `` plan merge fix 0.8.x soon ? checked latest stable build ( https : //builds.apache.org/job/cassandra-0.8/laststablebuild/artifact/cassandra/build/apache-cassandra-2011-08-17_23-00-56-bin.tar.gz ) n't see fix list changes.txt . also checked latest build ( https : //builds.apache.org/job/cassandra-0.8/284/artifact/cassandra/build/apache-cassandra-2011-08-17_23-00-56-bin.tar.gz ) n't see listed changes.txt.\\n\\ni tried https : //builds.apache.org/job/cassandra-0.8/214/ got http 404 . '' 'bq . plan merge fix 0.8.x soon ? \\n\\nno . probably stay 1.0-only . ' 'thanks jonathan.\\n\\nis 1.0 tentative release date still oct 8th ? currently running 0.6.11 would like upgrade 0.8.x issue much show stopper . run repair every weekend 0.6.11 production environment kind load saw testing repair 0.8.4 testing environment suitable production environment.\\n\\n ' 'this present 0.6 sure explain . : ) ' ' ( yes still shooting oct 8 1.0 . ) ' ' noticed attempt port 0.8 comment jonathan ported 0.8.x . \\ni suppose means ported 0.7.x ? \\nis possible many unresolvable conflicts ? ? ' 'as mentioned reverted 0.8.1 cassandra-2818 . we\\ 've committed maintaining drop-in-ability minor releases means can\\'t release protocol changes unless protocol backwards- forwards-compatibility actually works . case 2818 forwards-compatibility bug 0.8.0 0.8.1 means we\\ 'd say `` upgrade 0.8.x x &gt; = 6 must first upgrade 0.8.y 2 &lt; = &lt; 6 . '' super confusing people honestly experience 99 % users don\\'t read news upgrading anyway it\\ 's totally going bite lot them.\\n\\n0.7 affected 2818 it\\ 's past point making protocol changes . change risky bar pretty high make changes `` oldstable `` 0.7 . ' ' 1.0 might turn late . possible patch branch folks need apply patch &gt; 0.8.2 ? also kind tests ( besides repair ) done apply patch custom build ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>away streaming directory muddies approaches use deducing keyspace , cf , etc . paths . [ `` n't adverse affects streaming . '' 'files streamed probably stay marked temporary get cleaned automatically process dies . ' '+1 ' 'r912620 . ' 'integrated cassandra # 364 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/364/ ] ) \\n longer use streaming directory . patch gary dusbabek stu hood reviewed . .\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>scrubtest.testscruboutoforder generate test file fly scrubtest # testscruboutoforder failing trunk due serialization format change pre-generated out-of-order sstable . change generate out-of-order sstable fly n't need bother generating sstable hand . attempted release reference ( org.apache.cassandra.utils.concurrent.ref $ state @ 59662a93 ) already released\\nerror allocate trace org.apache.cassandra.utils.concurrent.ref $ state @ 59662a93 : \\nthread [ main 5 main ] \\n java.lang.thread.getstacktrace ( ) \\n org.apache.cassandra.utils.concurrent.ref $ debug. &lt; init &gt; ( ) \\n org.apache.cassandra.utils.concurrent.ref $ state. &lt; init &gt; ( ) \\n org.apache.cassandra.utils.concurrent.ref. &lt; init &gt; ( ) \\n org.apache.cassandra.io.sstable.format.sstablereader. &lt; init &gt; ( ) \\n org.apache.cassandra.io.sstable.format.big.bigtablereader. &lt; init &gt; ( ) \\n org.apache.cassandra.io.sstable.format.big.bigformat $ readerfactory.open ( ) \\n org.apache.cassandra.io.sstable.format.sstablereader.internalopen ( ) \\n org.apache.cassandra.io.sstable.format.sstablereader.open ( ) \\n org.apache.cassandra.io.sstable.format.sstablereader.open ( ) \\n org.apache.cassandra.io.sstable.format.sstablereader.open ( ) \\n org.apache.cassandra.db.scrubtest.testscruboutoforder ( ) \\n sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) \\n sun.reflect.nativemethodaccessorimpl.invoke ( ) \\n sun.reflect.delegatingmethodaccessorimpl.invoke ( ) \\n java.lang.reflect.method.invoke ( ) \\n org.junit.runners.model.frameworkmethod $ 1.runreflectivecall ( ) \\n org.junit.internal.runners.model.reflectivecallable.run ( ) \\n org.junit.runners.model.frameworkmethod.invokeexplosively ( ) \\n org.junit.internal.runners.statements.invokemethod.evaluate ( ) \\n org.junit.internal.runners.statements.runbefores.evaluate ( ) \\n org.junit.internal.runners.statements.runafters.evaluate ( ) \\n org.junit.runners.blockjunit4classrunner.runchild ( ) \\n org.junit.runners.blockjunit4classrunner.runchild ( ) \\n org.junit.runners.parentrunner.runchildren ( ) \\n org.junit.runners.parentrunner.access $ 000 ( ) \\n org.junit.runners.parentrunner $ 1.evaluate ( ) \\n org.junit.internal.runners.statements.runbefores.evaluate ( ) \\n org.junit.internal.runners.statements.runafters.evaluate ( ) \\n org.junit.runners.parentrunner.run ( ) \\n org.junit.runner.junitcore.run ( ) \\n com.intellij.junit4.junit4ideatestrunner.startrunnerwithargs ( ) \\n com.intellij.rt.execution.junit.junitstarter.preparestreamsandstart ( ) \\n com.intellij.rt.execution.junit.junitstarter.main ( ) \\n sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) \\n sun.reflect.nativemethodaccessorimpl.invoke ( ) \\n sun.reflect.delegatingmethodaccessorimpl.invoke ( ) \\n java.lang.reflect.method.invoke ( ) \\n com.intellij.rt.execution.application.appmain.main ( ) \\n\\nerror deallocate trace org.apache.cassandra.utils.concurrent.ref $ state @ 59662a93 : \\nthread [ main 5 main ] \\n java.lang.thread.getstacktrace ( ) \\n org.apache.cassandra.utils.concurrent.ref $ debug.deallocate ( ) \\n org.apache.cassandra.utils.concurrent.ref $ state.release ( ) \\n org.apache.cassandra.utils.concurrent.ref.release ( ) \\n org.apache.cassandra.io.sstable.format.sstablereader.validate ( ) \\n org.apache.cassandra.io.sstable.format.sstablereader.open ( ) \\n org.apache.cassandra.io.sstable.format.sstablereader.open ( ) \\n org.apache.cassandra.io.sstable.format.sstablereader.open ( ) \\n org.apache.cassandra.db.scrubtest.testscruboutoforder ( ) \\n sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) \\n sun.reflect.nativemethodaccessorimpl.invoke ( ) \\n sun.reflect.delegatingmethodaccessorimpl.invoke ( ) \\n java.lang.reflect.method.invoke ( ) \\n org.junit.runners.model.frameworkmethod $ 1.runreflectivecall ( ) \\n org.junit.internal.runners.model.reflectivecallable.run ( ) \\n org.junit.runners.model.frameworkmethod.invokeexplosively ( ) \\n org.junit.internal.runners.statements.invokemethod.evaluate ( ) \\n org.junit.internal.runners.statements.runbefores.evaluate ( ) \\n org.junit.internal.runners.statements.runafters.evaluate ( ) \\n org.junit.runners.blockjunit4classrunner.runchild ( ) \\n org.junit.runners.blockjunit4classrunner.runchild ( ) \\n org.junit.runners.parentrunner.runchildren ( ) \\n org.junit.runners.parentrunner.access $ 000 ( ) \\n org.junit.runners.parentrunner $ 1.evaluate ( ) \\n org.junit.internal.runners.statements.runbefores.evaluate ( ) \\n org.junit.internal.runners.statements.runafters.evaluate ( ) \\n org.junit.runners.parentrunner.run ( ) \\n org.junit.runner.junitcore.run ( ) \\n com.intellij.junit4.junit4ideatestrunner.startrunnerwithargs ( ) \\n com.intellij.rt.execution.junit.junitstarter.preparestreamsandstart ( ) \\n com.intellij.rt.execution.junit.junitstarter.main ( ) \\n sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) \\n sun.reflect.nativemethodaccessorimpl.invoke ( ) \\n sun.reflect.delegatingmethodaccessorimpl.invoke ( ) \\n java.lang.reflect.method.invoke ( ) \\n com.intellij.rt.execution.application.appmain.main ( ) \\n { code } ' 'thanks checking follow up.\\ncommitted patch .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>storageproxy log correctly schema agreement `` logger.debug ( `` % disagrees ( % ) '' , host , entry.getkey ( ) ) ; '' would literally log : debug [ pool-2-thread-359 ] 2011-10-18 storageproxy.java ( line 821 ) % disagrees ( % ) simple fix : replace % { } ... may want consider logging better comment ? fixed erroneous debug logging statement replacing % { } supported slf4j . also made use { } -notation debug logging statements class . ' 'committed . thanks jackson tommy !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>add cache loading row/key cache tests description [ `` n't use thread.sleep ( ) waiting future done call future.get ( ) \\n\\ni would n't bother reading cache directly disk checking contents rather force save ( way ) force c * completely load cache opening table first time checking size/contents via actual cache use . patch implements end test anyway assert nothing cache create cfs verify cache size/contents . something similar row cache.\\n\\notherwise patch looks good would like see changes 's committed.\\n '' 'attached v3 : \\n\\n1 ) consolidates insert read ( populate cache ) code\\n2 ) compares cached values expected values instead checking key exists\\n3 ) exposes cache cfs directly instead requiring round path reader get reference\\n\\nother changes patch looks good ' '+1 looks good wanted avoid changes main code base v2 thanks ! ' 'committed . ' ' best merge trunk testrowcacheload broken . committed anyway since rest merge bunch work thanks counters -- check current trunk post fix ? ' 'np : ) ' 'committed thanks ! ' 'integrated cassandra-0.7 # 399 ( see [ https : //hudson.apache.org/hudson/job/cassandra-0.7/399/ ] ) \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>allow disabling hotness persistence , tuning rate limiter persisting sstables hotness 10s thousands sstables issues keeping , rate limiter hard coded . another option may nice completely disable feature . sstables super backed ( repairs ) hotness tracking tends cause stcs l0 make poor decisions , always grabbing large sstables skipping tiny sstables would benefit . ||item|link||\\r\\n|pr| [ link|https : //github.com/apache/cassandra/pull/1855 ] |\\r\\n|jdk8 ci| [ link|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/295/workflows/c33ed2b0-5063-4707-ba65-075b38dd9361 ] |\\r\\n|jdk11 ci| [ link|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/295/workflows/274b3677-f709-4e07-be40-a6ad4bc97f59 ] | ' '+1\\r\\n\\r\\n ( dropped couple tiny nits pr ) ' 'the added properties config / dd added cassandra.yaml .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>storageproxy.getrangeslice sometimes returns incorrect number columns deployed single node , number columns correct . deployed cluster , total number returned columns slightly lower desired . attaching patch fixing paged column iteration . ' 'is dtest ? ' `` 's wide row test range slice test combination two . '' 'committed create https : //github.com/riptano/cassandra-dtest/issues/5 follow w/ dtest . ' ' note testing part concern getrangeslice ispaging option ( i.e . thrift get_paged_slice ) currently cql never calls reproducible cql . may end using ispaging thing cql cassandra-4851 however . ' 'just added wide_slice_test putget_test.py dtests .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>fix sstable ordering max timestamp singlepartitionreadcommand test environment drop create keyspaces tables several times within short time frame . since upgrading 3.11.0 3.11.1 , seeing lot create statements failing . see logs : { code : java } warn directory /tmp/ramdisk/commitlog n't exist warn directory /tmp/ramdisk/saved_caches n't exist info initialized prepared statement caches 10 mb ( native ) 10 mb ( thrift ) info initializing system.indexinfo info initializing system.batches info initializing system.paxos info initializing system.local info initializing system.peers info initializing system.peer_events info initializing system.range_xfers info initializing system.compaction_history info initializing system.sstable_activity info initializing system.size_estimates info initializing system.available_ranges info initializing system.transferred_ranges info initializing system.views_builds_in_progress info initializing system.built_views info initializing system.hints info initializing system.batchlog info initializing system.prepared_statements info initializing system.schema_keyspaces info initializing system.schema_columnfamilies info initializing system.schema_columns info initializing system.schema_triggers info initializing system.schema_usertypes info initializing system.schema_functions info initializing system.schema_aggregates info submitting build tasks views keyspace system storage service initialized info scheduling approximate time-check task precision 10 milliseconds info initializing system_schema.keyspaces info initializing system_schema.tables info initializing system_schema.columns info initializing system_schema.triggers info initializing system_schema.dropped_columns info initializing system_schema.views info initializing system_schema.types info initializing system_schema.functions info initializing system_schema.aggregates info initializing system_schema.indexes info submitting build tasks views keyspace system_schema storage service initialized info initializing key cache capacity 25 mbs . info initializing row cache capacity 0 mbs info initializing counter cache capacity 12 mbs info scheduling counter cache save every 7200 seconds ( going save keys ) . info populating token metadata system tables info global buffer pool enabled , pool exhausted ( max 125.000mib ) allocate heap info token metadata : info completed loading ( 14 ms ; 8 keys ) keycache cache info commitlog files found ; skipping replay info populating token metadata system tables info token metadata : info preloaded 0 prepared statements info cassandra version : 3.11.1 info thrift api version : 20.1.0 info cql supported versions : 3.4.4 ( default : 3.4.4 ) info native protocol supported versions : 3/v3 , 4/v4 , 5/v5-beta ( default : 4/v4 ) info initializing index summary manager memory pool size 25 mb resize interval 60 minutes info starting messaging service /172.17.0.2:7000 ( eth0 ) warn host id found , created 62452b7c-33ae-40e6-859c-1d7c803aaea8 ( note : happen exactly per node ) . info loading persisted ring state info starting server gossip info node auto bootstrap configured seed node . info generated random tokens . tokens [ -6736304773851341012 , 3437071596424929702 , 4372058337604769145 , -306854781937968525 , -4419476154597297006 , 4339837665480866486 , 2052026232731139893 , -5761537575805252593 , -4477540978357776290 , 6263754683045286998 , 3670054894619378302 , -4326549778810780939 , 7187409938161102814 , 7030537377703307755 , -2757270254308154659 , -1953637968902719055 , -7235425703069930259 , 7123794193321014835 , 349308827967095711 , 997472983569031481 , 992257140226393205 , -4045122629441468253 , 4149955653388319941 , -3690032393349188278 , 3528068129562283633 , -5057394127379238561 , -4944743272177354946 , 1371473468273321389 , -2771267888257678908 , -2379074055482922854 , 8800628062632970014 , 6016352719444925532 , -6458243637210081043 , -7131512441131507433 , -6135681286390467242 , -7886878247827491401 , -3964432859204941604 , -7124853795154335905 , 4536647221115220987 , 4518363137218750861 , -3945920538919881061 , -8569890499152898728 , -2228677668104169495 , -4004623128783039030 , -6849460601197629451 , -1787645289665343374 , -9004089114738085395 , -8444847561386064840 , -7719025430480017932 , -5020575591450775929 , -3535144847803187721 , 7252524597471726426 , -2582131369519057623 , 3737595811793840609 , -7248797595897252845 , -7065188032269288840 , -6731826791431802176 , -2970075663731571587 , -2619987499373344925 , -2698285069650269138 , -8589822844420136511 , 2658120945314344720 , -3710290429036098141 , 134530136452862749 , 3703742438909992913 , 3460544540911930621 , 8673891706698173777 , 2853177281247015813 , 13977464647778584 , 2404057737490125388 , -6759648287860184451 , 744453319830059045 , -688104893800828924 , 3356383003502762348 , 9054641886966810357 , 2317130729058165506 , -5810663910204725460 , 2577132949237273515 , 6326216055185945365 , 1376570278575995967 , 8758101809469842945 , -2892126907778256351 , -1716283861287440286 , 3040640159143123724 , 4243935966006505554 , -6827972097309863039 , 3055912546894309570 , -3992773844369808712 , -4717007910267923035 , -846198401308205724 , -3924870907185309086 , 1746803312676010060 , 6821355560067598541 , -5786385588878319458 , 3085551110635941848 , 7832310180114101987 , -9149254679798945822 , 3124836728424468300 , -100875121723899324 , -7606007094353527325 , 270256410769436649 , -3016541299722946307 , 6864985654287583845 , 8465468836551135602 , 7372808321676939792 , -2815261206329145311 , -2044219183173664775 , -5342853768228072396 , 3636940711408324184 , -2772742494800447004 , -8420993393273439531 , -1530882172522252534 , 8236427746033013128 , -8939749738449264357 , -571957476330656311 , 6462994120934510138 , -2744633996286755268 , 1001793370994802364 , 6170004027360887596 , 383603396273760626 , 184737756504479596 , -4799447088893889554 , 1038205033737034383 , 2078124248957773983 , -5177819727898656480 , 1588469358432181111 , 2476693400197902714 , 246839957213783595 , -7804622995667946321 , 3516202677463047183 , 7649126752776473673 , -3286662198144050257 , 2592926684883421936 , 6953901594207876325 , 8920684239689152479 , -2427878301857439455 , -6527468054932471540 , -4117125961852289967 , -2833593154725933249 , 2548273043767381234 , -814886098184093796 , -1113961241682560435 , -8364806058670744019 , -86067309810855914 , -7325813350040495905 , -2651532619332818109 , -3028501296208600216 , 2638649530375347897 , -3870517833780069551 , 3770751443844709295 , -7272035856681375921 , -6750394828506790417 , 3368553496734537183 , 8516129492713951191 , 4435960977618718666 , 638690551817702460 , -7462842134093200053 , -7312636473795422279 , 3825550639500258186 , -490674188267611204 , 8488259904981422083 , 4436678791994058329 , 5971819389544487212 , 5777643219857256454 , 6295906877222880293 , -6635403410495817577 , -7125973103119231247 , 2275471188158109929 , -6554337501188391642 , -4759608795508681126 , -7655250005358224912 , 9106670136441382451 , -9080117178764089351 , 5094764588972879219 , -3599769156391426161 , 6116955962236377408 , -1734768840951819839 , 7826627278264825770 , -2624139016757063818 , -4122417151587476614 , -6757251857390630385 , 2099124804383862824 , -3162332634454027278 , 4826222794133551270 , 9122652158513265055 , 1734656138981660315 , 972980826344778639 , -1746779194020635548 , -3426944282250211269 , -3857828063692993065 , 1895243495321867610 , -8828035583443240909 , -4705856469629722102 , -8519546521146945353 , -2150150551733933931 , 8281585304878501119 , -2775028105733898661 , 2087277989579187052 , -4016777313261130077 , 2747128117959922334 , -1398884803916585873 , 7188260080368469340 , -3880993098463994199 , 3574665846011083154 , 5260683239918360122 , 5817587463499837044 , 38978473621576635 , 2680910834841463710 , 6083561971466189055 , 7236937177408808074 , -3600112532662592989 , -4559800196660261967 , 8276688045060113438 , 5496539762676760591 , -2999626688519766687 , 8917068693185637310 , 2348378561310644717 , 7605443413072783308 , 5729359499569394810 , -782345069306605591 , 1165004403533704355 , -8301882560002322767 , 2008499890787626408 , -6211027251975593898 , 7406423735628820605 , -3204398339633370684 , -7917412446164112725 , -106645076087724250 , -1186720400780396653 , -8676089669972641821 , -1970508303671183113 , -7283082875075535628 , -3469652138221449481 , -3310949358194646693 , 6449384223770405185 , -3602652844861890703 , -7845236015467185307 , -4548809972889727666 , -8898627491921139823 , 5187965699546741544 , 295363921125698104 , -8013235493809339368 , -6747271362503076577 , 1102625310233591704 , -2543233385033476145 , -6197912327393001665 , 118165474822979356 , -4838870266722406438 , -5797141823778124932 , -1506683916229985698 , 9139710449103348665 , -1571612701117454805 , 8031141543284728427 , 8472337544063987034 , 3222463867738580103 , 8210687258187437204 ] info create new keyspace : keyspacemetadata { name=system_traces , params=keyspaceparams { durable_writes=true , replication=replicationparams { class=org.apache.cassandra.locator.simplestrategy , replication_factor=2 } } , tables= [ org.apache.cassandra.config.cfmetadata @ 3bc5ed95 [ cfid=c5e99f16-8677-3914-b17e-960613512345 , ksname=system_traces , cfname=sessions , flags= [ compound ] , params=tableparams { comment=tracing sessions , read_repair_chance=0.0 , dclocal_read_repair_chance=0.0 , bloom_filter_fp_chance=0.01 , crc_check_chance=1.0 , gc_grace_seconds=0 , default_time_to_live=0 , memtable_flush_period_in_ms=3600000 , min_index_interval=128 , max_index_interval=2048 , speculative_retry=99percentile , caching= { 'keys ' : 'all ' , 'rows_per_partition ' : 'none ' } , compaction=compactionparams { class=org.apache.cassandra.db.compaction.sizetieredcompactionstrategy , options= { min_threshold=4 , max_threshold=32 } } , compression=org.apache.cassandra.schema.compressionparams @ 4c3448a7 , extensions= { } , cdc=false } , comparator=comparator ( ) , partitioncolumns= [ [ ] | [ client command coordinator duration request started_at parameters ] ] , partitionkeycolumns= [ session_id ] , clusteringcolumns= [ ] , keyvalidator=org.apache.cassandra.db.marshal.uuidtype , columnmetadata= [ client , command , session_id , coordinator , request , started_at , duration , parameters ] , droppedcolumns= { } , triggers= [ ] , indexes= [ ] ] , org.apache.cassandra.config.cfmetadata @ 1a296ffd [ cfid=8826e8e9-e16a-3728-8753-3bc1fc713c25 , ksname=system_traces , cfname=events , flags= [ compound ] , params=tableparams { comment=tracing events , read_repair_chance=0.0 , dclocal_read_repair_chance=0.0 , bloom_filter_fp_chance=0.01 , crc_check_chance=1.0 , gc_grace_seconds=0 , default_time_to_live=0 , memtable_flush_period_in_ms=3600000 , min_index_interval=128 , max_index_interval=2048 , speculative_retry=99percentile , caching= { 'keys ' : 'all ' , 'rows_per_partition ' : 'none ' } , compaction=compactionparams { class=org.apache.cassandra.db.compaction.sizetieredcompactionstrategy , options= { min_threshold=4 , max_threshold=32 } } , compression=org.apache.cassandra.schema.compressionparams @ 4c3448a7 , extensions= { } , cdc=false } , comparator=comparator ( org.apache.cassandra.db.marshal.timeuuidtype ) , partitioncolumns= [ [ ] | [ activity source source_elapsed thread ] ] , partitionkeycolumns= [ session_id ] , clusteringcolumns= [ event_id ] , keyvalidator=org.apache.cassandra.db.marshal.uuidtype , columnmetadata= [ activity , event_id , session_id , source , thread , source_elapsed ] , droppedcolumns= { } , triggers= [ ] , indexes= [ ] ] ] , views= [ ] , functions= [ ] , types= [ ] } info submitting build tasks views keyspace system_traces storage service initialized info initializing system_traces.events info initializing system_traces.sessions info create new keyspace : keyspacemetadata { name=system_distributed , params=keyspaceparams { durable_writes=true , replication=replicationparams { class=org.apache.cassandra.locator.simplestrategy , replication_factor=3 } } , tables= [ org.apache.cassandra.config.cfmetadata @ 2884b38b [ cfid=759fffad-624b-3181-80ee-fa9a52d1f627 , ksname=system_distributed , cfname=repair_history , flags= [ compound ] , params=tableparams { comment=repair history , read_repair_chance=0.0 , dclocal_read_repair_chance=0.0 , bloom_filter_fp_chance=0.01 , crc_check_chance=1.0 , gc_grace_seconds=864000 , default_time_to_live=0 , memtable_flush_period_in_ms=3600000 , min_index_interval=128 , max_index_interval=2048 , speculative_retry=99percentile , caching= { 'keys ' : 'all ' , 'rows_per_partition ' : 'none ' } , compaction=compactionparams { class=org.apache.cassandra.db.compaction.sizetieredcompactionstrategy , options= { min_threshold=4 , max_threshold=32 } } , compression=org.apache.cassandra.schema.compressionparams @ 4c3448a7 , extensions= { } , cdc=false } , comparator=comparator ( org.apache.cassandra.db.marshal.timeuuidtype ) , partitioncolumns= [ [ ] | [ coordinator exception_message exception_stacktrace finished_at parent_id range_begin range_end started_at status participants ] ] , partitionkeycolumns= [ keyspace_name , columnfamily_name ] , clusteringcolumns= [ id ] , keyvalidator=org.apache.cassandra.db.marshal.compositetype ( org.apache.cassandra.db.marshal.utf8type , org.apache.cassandra.db.marshal.utf8type ) , columnmetadata= [ status , id , coordinator , finished_at , participants , exception_stacktrace , parent_id , range_end , range_begin , exception_message , keyspace_name , started_at , columnfamily_name ] , droppedcolumns= { } , triggers= [ ] , indexes= [ ] ] , org.apache.cassandra.config.cfmetadata @ 7fcc80b2 [ cfid=deabd734-b99d-3b9c-92e5-fd92eb5abf14 , ksname=system_distributed , cfname=parent_repair_history , flags= [ compound ] , params=tableparams { comment=repair history , read_repair_chance=0.0 , dclocal_read_repair_chance=0.0 , bloom_filter_fp_chance=0.01 , crc_check_chance=1.0 , gc_grace_seconds=864000 , default_time_to_live=0 , memtable_flush_period_in_ms=3600000 , min_index_interval=128 , max_index_interval=2048 , speculative_retry=99percentile , caching= { 'keys ' : 'all ' , 'rows_per_partition ' : 'none ' } , compaction=compactionparams { class=org.apache.cassandra.db.compaction.sizetieredcompactionstrategy , options= { min_threshold=4 , max_threshold=32 } } , compression=org.apache.cassandra.schema.compressionparams @ 4c3448a7 , extensions= { } , cdc=false } , comparator=comparator ( ) , partitioncolumns= [ [ ] | [ exception_message exception_stacktrace finished_at keyspace_name started_at columnfamily_names options requested_ranges successful_ranges ] ] , partitionkeycolumns= [ parent_id ] , clusteringcolumns= [ ] , keyvalidator=org.apache.cassandra.db.marshal.timeuuidtype , columnmetadata= [ requested_ranges , exception_message , keyspace_name , successful_ranges , started_at , finished_at , options , exception_stacktrace , parent_id , columnfamily_names ] , droppedcolumns= { } , triggers= [ ] , indexes= [ ] ] , org.apache.cassandra.config.cfmetadata @ 7e500004 [ cfid=5582b59f-8e4e-35e1-b913-3acada51eb04 , ksname=system_distributed , cfname=view_build_status , flags= [ compound ] , params=tableparams { comment=materialized view build status , read_repair_chance=0.0 , dclocal_read_repair_chance=0.0 , bloom_filter_fp_chance=0.01 , crc_check_chance=1.0 , gc_grace_seconds=864000 , default_time_to_live=0 , memtable_flush_period_in_ms=3600000 , min_index_interval=128 , max_index_interval=2048 , speculative_retry=99percentile , caching= { 'keys ' : 'all ' , 'rows_per_partition ' : 'none ' } , compaction=compactionparams { class=org.apache.cassandra.db.compaction.sizetieredcompactionstrategy , options= { min_threshold=4 , max_threshold=32 } } , compression=org.apache.cassandra.schema.compressionparams @ 4c3448a7 , extensions= { } , cdc=false } , comparator=comparator ( org.apache.cassandra.db.marshal.uuidtype ) , partitioncolumns= [ [ ] | [ status ] ] , partitionkeycolumns= [ keyspace_name , view_name ] , clusteringcolumns= [ host_id ] , keyvalidator=org.apache.cassandra.db.marshal.compositetype ( org.apache.cassandra.db.marshal.utf8type , org.apache.cassandra.db.marshal.utf8type ) , columnmetadata= [ view_name , status , keyspace_name , host_id ] , droppedcolumns= { } , triggers= [ ] , indexes= [ ] ] ] , views= [ ] , functions= [ ] , types= [ ] } info submitting build tasks views keyspace system_distributed storage service initialized info initializing system_distributed.parent_repair_history info initializing system_distributed.repair_history info initializing system_distributed.view_build_status info joining : finish joining ring info create new keyspace : keyspacemetadata { name=system_auth , params=keyspaceparams { durable_writes=true , replication=replicationparams { class=org.apache.cassandra.locator.simplestrategy , replication_factor=1 } } , tables= [ org.apache.cassandra.config.cfmetadata @ 3c28c0da [ cfid=5bc52802-de25-35ed-aeab-188eecebb090 , ksname=system_auth , cfname=roles , flags= [ compound ] , params=tableparams { comment=role definitions , read_repair_chance=0.0 , dclocal_read_repair_chance=0.0 , bloom_filter_fp_chance=0.01 , crc_check_chance=1.0 , gc_grace_seconds=7776000 , default_time_to_live=0 , memtable_flush_period_in_ms=3600000 , min_index_interval=128 , max_index_interval=2048 , speculative_retry=99percentile , caching= { 'keys ' : 'all ' , 'rows_per_partition ' : 'none ' } , compaction=compactionparams { class=org.apache.cassandra.db.compaction.sizetieredcompactionstrategy , options= { min_threshold=4 , max_threshold=32 } } , compression=org.apache.cassandra.schema.compressionparams @ 4c3448a7 , extensions= { } , cdc=false } , comparator=comparator ( ) , partitioncolumns= [ [ ] | [ can_login is_superuser salted_hash member_of ] ] , partitionkeycolumns= [ role ] , clusteringcolumns= [ ] , keyvalidator=org.apache.cassandra.db.marshal.utf8type , columnmetadata= [ salted_hash , member_of , role , can_login , is_superuser ] , droppedcolumns= { } , triggers= [ ] , indexes= [ ] ] , org.apache.cassandra.config.cfmetadata @ 2e0f771e [ cfid=0ecdaa87-f8fb-3e60-88d1-74fb36fe5c0d , ksname=system_auth , cfname=role_members , flags= [ compound ] , params=tableparams { comment=role memberships lookup table , read_repair_chance=0.0 , dclocal_read_repair_chance=0.0 , bloom_filter_fp_chance=0.01 , crc_check_chance=1.0 , gc_grace_seconds=7776000 , default_time_to_live=0 , memtable_flush_period_in_ms=3600000 , min_index_interval=128 , max_index_interval=2048 , speculative_retry=99percentile , caching= { 'keys ' : 'all ' , 'rows_per_partition ' : 'none ' } , compaction=compactionparams { class=org.apache.cassandra.db.compaction.sizetieredcompactionstrategy , options= { min_threshold=4 , max_threshold=32 } } , compression=org.apache.cassandra.schema.compressionparams @ 4c3448a7 , extensions= { } , cdc=false } , comparator=comparator ( org.apache.cassandra.db.marshal.utf8type ) , partitioncolumns= [ [ ] | [ ] ] , partitionkeycolumns= [ role ] , clusteringcolumns= [ member ] , keyvalidator=org.apache.cassandra.db.marshal.utf8type , columnmetadata= [ role , member ] , droppedcolumns= { } , triggers= [ ] , indexes= [ ] ] , org.apache.cassandra.config.cfmetadata @ 4fabdebb [ cfid=3afbe79f-2194-31a7-add7-f5ab90d8ec9c , ksname=system_auth , cfname=role_permissions , flags= [ compound ] , params=tableparams { comment=permissions granted db roles , read_repair_chance=0.0 , dclocal_read_repair_chance=0.0 , bloom_filter_fp_chance=0.01 , crc_check_chance=1.0 , gc_grace_seconds=7776000 , default_time_to_live=0 , memtable_flush_period_in_ms=3600000 , min_index_interval=128 , max_index_interval=2048 , speculative_retry=99percentile , caching= { 'keys ' : 'all ' , 'rows_per_partition ' : 'none ' } , compaction=compactionparams { class=org.apache.cassandra.db.compaction.sizetieredcompactionstrategy , options= { min_threshold=4 , max_threshold=32 } } , compression=org.apache.cassandra.schema.compressionparams @ 4c3448a7 , extensions= { } , cdc=false } , comparator=comparator ( org.apache.cassandra.db.marshal.utf8type ) , partitioncolumns= [ [ ] | [ permissions ] ] , partitionkeycolumns= [ role ] , clusteringcolumns= [ resource ] , keyvalidator=org.apache.cassandra.db.marshal.utf8type , columnmetadata= [ role , resource , permissions ] , droppedcolumns= { } , triggers= [ ] , indexes= [ ] ] , org.apache.cassandra.config.cfmetadata @ 7103b8de [ cfid=5f2fbdad-91f1-3946-bd25-d5da3a5c35ec , ksname=system_auth , cfname=resource_role_permissons_index , flags= [ compound ] , params=tableparams { comment=index db roles permissions granted resource , read_repair_chance=0.0 , dclocal_read_repair_chance=0.0 , bloom_filter_fp_chance=0.01 , crc_check_chance=1.0 , gc_grace_seconds=7776000 , default_time_to_live=0 , memtable_flush_period_in_ms=3600000 , min_index_interval=128 , max_index_interval=2048 , speculative_retry=99percentile , caching= { 'keys ' : 'all ' , 'rows_per_partition ' : 'none ' } , compaction=compactionparams { class=org.apache.cassandra.db.compaction.sizetieredcompactionstrategy , options= { min_threshold=4 , max_threshold=32 } } , compression=org.apache.cassandra.schema.compressionparams @ 4c3448a7 , extensions= { } , cdc=false } , comparator=comparator ( org.apache.cassandra.db.marshal.utf8type ) , partitioncolumns= [ [ ] | [ ] ] , partitionkeycolumns= [ resource ] , clusteringcolumns= [ role ] , keyvalidator=org.apache.cassandra.db.marshal.utf8type , columnmetadata= [ resource , role ] , droppedcolumns= { } , triggers= [ ] , indexes= [ ] ] ] , views= [ ] , functions= [ ] , types= [ ] } info submitting build tasks views keyspace system_auth storage service initialized info initializing system_auth.resource_role_permissons_index info initializing system_auth.role_members info initializing system_auth.role_permissions info initializing system_auth.roles info netty using native epoll event loop info using netty version : [ netty-buffer=netty-buffer-4.0.44.final.452812a , netty-codec=netty-codec-4.0.44.final.452812a , netty-codec-haproxy=netty-codec-haproxy-4.0.44.final.452812a , netty-codec-http=netty-codec-http-4.0.44.final.452812a , netty-codec-socks=netty-codec-socks-4.0.44.final.452812a , netty-common=netty-common-4.0.44.final.452812a , netty-handler=netty-handler-4.0.44.final.452812a , netty-tcnative=netty-tcnative-1.1.33.fork26.142ecbb , netty-transport=netty-transport-4.0.44.final.452812a , netty-transport-native-epoll=netty-transport-native-epoll-4.0.44.final.452812a , netty-transport-rxtx=netty-transport-rxtx-4.0.44.final.452812a , netty-transport-sctp=netty-transport-sctp-4.0.44.final.452812a , netty-transport-udt=netty-transport-udt-4.0.44.final.452812a ] info starting listening cql clients /0.0.0.0:9042 ( unencrypted ) ... info starting rpc server requested . use jmx ( storageservice- &gt; startrpcserver ( ) ) nodetool ( enablethrift ) start info created default superuser role 'cassandra' info create new keyspace : keyspacemetadata { name=my_keyspace , params=keyspaceparams { durable_writes=true , replication=replicationparams { class=org.apache.cassandra.locator.simplestrategy , replication_factor=1 } } , tables= [ ] , views= [ ] , functions= [ ] , types= [ ] } info create new table : org.apache.cassandra.config.cfmetadata @ c74a94b [ cfid=1572b410-c87f-11e7-9db1-6d2c86545d91 , ksname=my_keyspace , cfname=schema_version , flags= [ compound ] , params=tableparams { comment= , read_repair_chance=0.0 , dclocal_read_repair_chance=0.1 , bloom_filter_fp_chance=0.01 , crc_check_chance=1.0 , gc_grace_seconds=864000 , default_time_to_live=0 , memtable_flush_period_in_ms=0 , min_index_interval=128 , max_index_interval=2048 , speculative_retry=99percentile , caching= { 'keys ' : 'all ' , 'rows_per_partition ' : 'none ' } , compaction=compactionparams { class=org.apache.cassandra.db.compaction.sizetieredcompactionstrategy , options= { min_threshold=4 , max_threshold=32 } } , compression=org.apache.cassandra.schema.compressionparams @ 4c3448a7 , extensions= { } , cdc=false } , comparator=comparator ( ) , partitioncolumns= [ [ ] | [ migration_lock version ] ] , partitionkeycolumns= [ id ] , clusteringcolumns= [ ] , keyvalidator=org.apache.cassandra.db.marshal.int32type , columnmetadata= [ migration_lock , version , id ] , droppedcolumns= { } , triggers= [ ] , indexes= [ ] ] info initializing my_keyspace.schema_version info drop keyspace 'my_keyspace' info create new keyspace : keyspacemetadata { name=my_keyspace , params=keyspaceparams { durable_writes=true , replication=replicationparams { class=org.apache.cassandra.locator.simplestrategy , replication_factor=1 } } , tables= [ ] , views= [ ] , functions= [ ] , types= [ ] } info create new table : org.apache.cassandra.config.cfmetadata @ 1a0616e9 [ cfid=171e8f50-c87f-11e7-9db1-6d2c86545d91 , ksname=my_keyspace , cfname=schema_version , flags= [ compound ] , params=tableparams { comment= , read_repair_chance=0.0 , dclocal_read_repair_chance=0.1 , bloom_filter_fp_chance=0.01 , crc_check_chance=1.0 , gc_grace_seconds=864000 , default_time_to_live=0 , memtable_flush_period_in_ms=0 , min_index_interval=128 , max_index_interval=2048 , speculative_retry=99percentile , caching= { 'keys ' : 'all ' , 'rows_per_partition ' : 'none ' } , compaction=compactionparams { class=org.apache.cassandra.db.compaction.sizetieredcompactionstrategy , options= { min_threshold=4 , max_threshold=32 } } , compression=org.apache.cassandra.schema.compressionparams @ 4c3448a7 , extensions= { } , cdc=false } , comparator=comparator ( ) , partitioncolumns= [ [ ] | [ migration_lock version ] ] , partitionkeycolumns= [ id ] , clusteringcolumns= [ ] , keyvalidator=org.apache.cassandra.db.marshal.int32type , columnmetadata= [ migration_lock , version , id ] , droppedcolumns= { } , triggers= [ ] , indexes= [ ] ] info initializing my_keyspace.schema_version info drop keyspace 'my_keyspace' info create new keyspace : keyspacemetadata { name=my_keyspace , params=keyspaceparams { durable_writes=true , replication=replicationparams { class=org.apache.cassandra.locator.simplestrategy , replication_factor=1 } } , tables= [ ] , views= [ ] , functions= [ ] , types= [ ] } info create new table : org.apache.cassandra.config.cfmetadata @ 7338ccab [ cfid=182996b0-c87f-11e7-9db1-6d2c86545d91 , ksname=my_keyspace , cfname=schema_version , flags= [ compound ] , params=tableparams { comment= , read_repair_chance=0.0 , dclocal_read_repair_chance=0.1 , bloom_filter_fp_chance=0.01 , crc_check_chance=1.0 , gc_grace_seconds=864000 , default_time_to_live=0 , memtable_flush_period_in_ms=0 , min_index_interval=128 , max_index_interval=2048 , speculative_retry=99percentile , caching= { 'keys ' : 'all ' , 'rows_per_partition ' : 'none ' } , compaction=compactionparams { class=org.apache.cassandra.db.compaction.sizetieredcompactionstrategy , options= { min_threshold=4 , max_threshold=32 } } , compression=org.apache.cassandra.schema.compressionparams @ 4c3448a7 , extensions= { } , cdc=false } , comparator=comparator ( ) , partitioncolumns= [ [ ] | [ migration_lock version ] ] , partitionkeycolumns= [ id ] , clusteringcolumns= [ ] , keyvalidator=org.apache.cassandra.db.marshal.int32type , columnmetadata= [ migration_lock , version , id ] , droppedcolumns= { } , triggers= [ ] , indexes= [ ] ] info initializing my_keyspace.schema_version info drop keyspace 'my_keyspace' info create new keyspace : keyspacemetadata { name=my_keyspace , params=keyspaceparams { durable_writes=true , replication=replicationparams { class=org.apache.cassandra.locator.simplestrategy , replication_factor=1 } } , tables= [ ] , views= [ ] , functions= [ ] , types= [ ] } info create new table : org.apache.cassandra.config.cfmetadata @ 229f3694 [ cfid=191b97d0-c87f-11e7-9db1-6d2c86545d91 , ksname=my_keyspace , cfname=schema_version , flags= [ compound ] , params=tableparams { comment= , read_repair_chance=0.0 , dclocal_read_repair_chance=0.1 , bloom_filter_fp_chance=0.01 , crc_check_chance=1.0 , gc_grace_seconds=864000 , default_time_to_live=0 , memtable_flush_period_in_ms=0 , min_index_interval=128 , max_index_interval=2048 , speculative_retry=99percentile , caching= { 'keys ' : 'all ' , 'rows_per_partition ' : 'none ' } , compaction=compactionparams { class=org.apache.cassandra.db.compaction.sizetieredcompactionstrategy , options= { min_threshold=4 , max_threshold=32 } } , compression=org.apache.cassandra.schema.compressionparams @ 4c3448a7 , extensions= { } , cdc=false } , comparator=comparator ( ) , partitioncolumns= [ [ ] | [ migration_lock version ] ] , partitionkeycolumns= [ id ] , clusteringcolumns= [ ] , keyvalidator=org.apache.cassandra.db.marshal.int32type , columnmetadata= [ migration_lock , version , id ] , droppedcolumns= { } , triggers= [ ] , indexes= [ ] ] error unexpected error query java.lang.runtimeexception : java.util.concurrent.executionexception : java.lang.nullpointerexception org.apache.cassandra.utils.fbutilities.waitonfuture ( ) ~ [ ] org.apache.cassandra.service.migrationmanager.announce ( ) ~ [ ] org.apache.cassandra.service.migrationmanager.announcenewcolumnfamily ( ) ~ [ ] org.apache.cassandra.service.migrationmanager.announcenewcolumnfamily ( ) ~ [ ] org.apache.cassandra.service.migrationmanager.announcenewcolumnfamily ( ) ~ [ ] org.apache.cassandra.cql3.statements.createtablestatement.announcemigration ( ) ~ [ ] org.apache.cassandra.cql3.statements.schemaalteringstatement.execute ( ) ~ [ ] org.apache.cassandra.cql3.queryprocessor.processstatement ( ) ~ [ ] org.apache.cassandra.cql3.queryprocessor.process ( ) ~ [ ] org.apache.cassandra.cql3.queryprocessor.process ( ) ~ [ ] org.apache.cassandra.transport.messages.querymessage.execute ( ) ~ [ ] org.apache.cassandra.transport.message $ dispatcher.channelread0 ( ) [ ] org.apache.cassandra.transport.message $ dispatcher.channelread0 ( ) [ ] io.netty.channel.simplechannelinboundhandler . 1 ) ) ; \\r\\n - } \\r\\n - } ; \\r\\n + public static final comparator &lt; sstablereader &gt; maxtimestampcomparator = ( o1 o2 ) - &gt; long.compare ( o1.getmaxtimestamp ( ) o2.getmaxtimestamp ( ) ) ; \\r\\n { code } \\r\\n\\r\\nthis putting back like cassandra-13776 . worked 13776 went . ' 'ok lcs wrong created cassandra-14099 follow there.\\r\\n ' 'thanks review .. fix nits restarted ci . ci looks good . ' 'committed { { a9225f90e205a7c2b24a4ad4a32d0961067005b0 } } cassandra-3.0 merged cassandra-3.11 trunk . thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>modify abstractcassandradaemon.initlog4j ( ) allow hotfixing log level cassandra class customer wants bump log level cassandra class , procedure follow : # add class name log level log4j-server.properties revision identified directory . # new directory symbolically linked location cassandra looks directory . however cassandra 's log4j continue watch old location reason abstractcassandradaemon.initlog4j ( ) uses configfilename = new file ( configlocation.touri ( ) ) .getcanonicalpath ( ) ; customer believes change following , allow symlink work properly . configfilename = new file ( configlocation.touri ( ) ) .getpath ( ) ; possible add configuration would invoke change `` true '' would ideal . find another method allow hotfixing log4j changes . [ `` n't see reason allow symlink trivial patch attached . '' '+1 ' 'committed . trunk assumed logback would work .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>range query support scan keys range ( x ) prefix ( starting p ) . create collatediterator return unique keys different sources already sorted . need iterators different key sources . sstables means adding seekto iterator interface filestruct . memtable means adding destructivepqiterator since unlike sstable keys inherently ordered already . means log n work sorting memtable keys number keys actually read n total number keys naive sort-everything-first iterator would n log n.\\n\\nthis yet implement range queries spanning multiple nodes.\\n\\npatch jbellis ; reviewed jun rao \\n ' 'the new patch close ( ) looks good me.\\n ' 'committed -close patch . ' 'integrated cassandra # 55 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/55/ ] ) \\n close filestructs range query . patch jbellis ; reviewed jun rao \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>merge get_indexed_slices get_range_slices comment 1157 : { quote } indexclause start key get_indexed_slices , would seem reasoning behind using 'keyrange ' get_range_slices applies well , since know range care primary index , n't want continue scanning exhaust 'count ' ( cluster ) . since would appear get_indexed_slices would benefit keyrange , smash get_ ( range|indexed ) _slices together , make indexclause optional field keyrange ? { quote } current code call cfs.getrangeslice ( ) directly case ( bypassing read stage protections coming ) . clearly fix it\\ 's neither detail related patch suggest spawning another ticket instead . ' 'bq . node range like ( 2^127-1 2^126 ] really hold ranges ( 0 2^126 ] ( 2^127 2^127-1 ] non contiguous far ordering disk concerned.\\n\\nfair enough.\\n\\nbq . problem introduced patch\\n\\ncreated cassandra-3687.\\n\\n+1 v4 . ' 'committed thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>regular startup log confusing `` bootstrap/replace/move completed ! '' without boostrap , replace , move regular startup completes successfully , confusing message end startup : '' info bootstrap/replace/move completed ! serving reads . '' happens despite bootstrap , replace , move . purely cosmetic , makes wonder node - bootstrap ? ! simply read something like `` startup completed ! serving reads '' unless actually done one actions error message . complete log end : info log replay complete , 6274 replayed mutations info cassandra version : 1.0.12 info thrift api version : 19.20.0 info loading persisted ring state info starting server gossip info enqueuing flush memtable-locationinfo @ 1828864224 ( 29/36 serialized/live bytes , 1 ops ) info writing memtable-locationinfo @ 1828864224 ( 29/36 serialized/live bytes , 1 ops ) info completed flushing /data2/data-cassandra/system/locationinfo-hd-274-data.db ( 80 bytes ) info starting messaging service port 7000 info using saved token 31901471898837980949691369446728269823 info enqueuing flush memtable-locationinfo @ 294410307 ( 53/66 serialized/live bytes , 2 ops ) info writing memtable-locationinfo @ 294410307 ( 53/66 serialized/live bytes , 2 ops ) info completed flushing /data2/data-cassandra/system/locationinfo-hd-275-data.db ( 163 bytes ) info node kaos-cass02.xxxxxxx/1.2.3.4 state jump normal info bootstrap/replace/move completed ! serving reads . info load mx4j , mx4j-tools.jar classpath startup completed ? \\n ( still looking abstract word : ) ) ' 'bootstrap means something specifically cassandra think data streamed in.\\n\\ni think `` startup completed '' would great.\\n\\nif bootstrap/replace/move think message ought specify happened it\\ 's ready ( it\\ 's easy ) : ) ' 'committed https : //git-wip-us.apache.org/repos/asf ? p=cassandra.git ; a=blobdiff ; f=src/java/org/apache/cassandra/service/storageservice.java ; h=8de0bd24632c89ea1b41c952ee6ec2db58808894 ; hp=7d92fbe0ff15c8c686a93425f4fccca49b921c0b ; hb=d525cf969c042b21a9375446f5449ee82d7d1484 ; hpb=7e937b3d1308c0774e4b0366b6e66b14af1dd5f6\\n\\nlet know need info reopen ticket . ' `` n't quite mind . 's semantic issue 's logical issue . clearly indicate operation actually performed quick glance code means need store state somewhere . '' 'had discussion brandon offline \\nthere enough information logs show operation bootstrap vs repair vs startup closing ticket .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>cas always correctly replay inprogress rounds paxos says receiving result prepare quorum acceptors , proposer propose value higher-number proposal accepted amongst ones returned acceptors , propose value acceptor send us back previously accepted value . preparecallback keep recent inprogress commit regardless whether update . means could ignore value already accepted acceptors acceptor send us recent ballot acceptor values . net effect mistakenly accept two different values round . far checking finish inprogress round need keep recent inprogress commit value . break optimization cassandra-5667 patch also keep recent inprogress regardless whether value not.\\n ' '+1 ' 'committed thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>mvs built unnecessarily bootstrap bootstrap mvs enqueued built already created bootstrap . simply adding system.built_views successful bootstrap fix issue . https : //github.com/jaumo/cassandra/commits/12984-3.0\\n+ \\nhttps : //github.com/jaumo/cassandra/commits/12984-3.x ' 'code looks good -- running ci : \\n\\n||3.0| [ utest|https : //cassci.datastax.com/view/dev/view/carlyeks/job/carlyeks-review-12984-3.0-testall/ ] | [ dtest|https : //cassci.datastax.com/view/dev/view/carlyeks/job/carlyeks-review-12984-3.0-dtest/ ] |\\n||3.11| [ utest|https : //cassci.datastax.com/view/dev/view/carlyeks/job/carlyeks-review-12984-3.11-testall/ ] | [ dtest|https : //cassci.datastax.com/view/dev/view/carlyeks/job/carlyeks-review-12984-3.11-dtest/ ] |\\n||3.x| [ utest|https : //cassci.datastax.com/view/dev/view/carlyeks/job/carlyeks-review-12984-3.x-testall/ ] | [ dtest|https : //cassci.datastax.com/view/dev/view/carlyeks/job/carlyeks-review-12984-3.x-dtest/ ] |\\n||trunk| [ utest|https : //cassci.datastax.com/view/dev/view/carlyeks/job/carlyeks-review-12984-trunk-testall/ ] | [ dtest|https : //cassci.datastax.com/view/dev/view/carlyeks/job/carlyeks-review-12984-trunk-dtest/ ] | ' 'thanks patch [ ~brstgt ] ! tests looked reasonable committed patch [ e9b7a0f|https : //git1-us-west.apache.org/repos/asf/cassandra/ ? p=cassandra.git ; a=commit ; h=e9b7a0f2546579244ffc167c56122b0a47d4b4b0 ] .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>classcastexception messagingservice got following exception running repair 3 node ccm cluster { code } error [ ] 2014-07-03 - exception thread thread [ , main ] java.lang.classcastexception : org.apache.cassandra.net.callbackinfo cast org.apache.cassandra.net.writecallbackinfo org.apache.cassandra.net.messagingservice $ 5.apply ( ) ~ [ main/ : na ] org.apache.cassandra.net.messagingservice $ 5.apply ( ) ~ [ main/ : na ] org.apache.cassandra.utils.expiringmap $ 1.run ( ) ~ [ main/ : na ] org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor $ uncomplainingrunnable.run ( ) ~ [ main/ : na ] java.util.concurrent.executors $ runnableadapter.call ( ) [ ] java.util.concurrent.futuretask.runandreset ( ) [ ] java.util.concurrent.scheduledthreadpoolexecutor $ scheduledfuturetask.access $ 301 ( ) [ ] java.util.concurrent.scheduledthreadpoolexecutor $ scheduledfuturetask.run ( ) [ ] java.util.concurrent.threadpoolexecutor.runworker ( ) [ ] java.util.concurrent.threadpoolexecutor $ worker.run ( ) [ ] java.lang.thread.run ( ) [ ] { code } looks like block ( messagingservice , li . 352 ) , changed cassandra-7245 . produced trunk , compared ms code trunk 2.1.0 . change 7245 introduced : pre-7245 { code } ( expiredcallbackinfo.shouldhint ( ) ) { mutation mutation = ( mutation ) ( ( writecallbackinfo ) expiredcallbackinfo ) .sentmessage.payload ; return storageproxy.submithint ( mutation , expiredcallbackinfo.target , null ) ; } { code } 7245 : { code } mutation mutation = ( mutation ) ( ( writecallbackinfo ) expiredcallbackinfo ) .sentmessage.payload ; try { ( expiredcallbackinfo.shouldhint ( ) ) { return storageproxy.submithint ( mutation , expiredcallbackinfo.target , null ) ; } } finally { //we serialized hint n't need mutation anymore mutation.release ( ) ; } { code } [ `` looks like cast wci moved back shouldhint ( ) check callbackinfo.shouldhint ( ) hardcoded return false thus could never downcast anyway.\\n\\nso probably fix ( move cast shouldhint ( ) check ) : \\n { code } \\n ( expiredcallbackinfo.shouldhint ( ) ) \\n { \\n try\\n { \\n mutation mutation = ( mutation ) ( ( writecallbackinfo ) expiredcallbackinfo ) .sentmessage.payload ; \\n return storageproxy.submithint ( mutation expiredcallbackinfo.target null ) ; \\n } \\n finally\\n { \\n //we serialized hint n't need mutation anymore\\n mutation.release ( ) ; \\n } \\n } \\n { code } \\n '' `` patch fix attached . describe ccm scenario caused ? 'd like add dtest . '' `` committed roll rc3.\\n\\nif want follow test looks like happen 100 % time callback non-write expired message . ( actually sure includes -- lots non-write messages callbacks n't allowed expire . clearly must . ) '' `` [ ~tjake ] produced building 3 node cluster ccm . ran 'stress write ' get data cluster ( rf=2 ) calling flush every tens seconds could monitor amount data per node ( n't need much data something play ) . ran \\n { code } ccm node1 nodetool repair keyspace1 standard1 { code } \\n node1 would get exception 30 seconds ( usually occurring times span repair ) . \\n\\nas testing changes 'm working n't believe cause messages timing repair problem 2.1 ( bugs work branch ) . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>gossipers notion schema differs reality reported nodes question 1.1 cluster happened notice { { nodetool gossipinfo | grep schema } } reported disagreement : { code } schema : b0d7bab7-c13c-37d9-9adb-8ab8a5b7215d schema : bcdbd318-82df-3518-89e3-6b72227b3f66 schema : bcdbd318-82df-3518-89e3-6b72227b3f66 { code } however , result thrift { { describe_ring } } cluster claims agree { { b0d7bab7-c13c-37d9-9adb-8ab8a5b7215d } } schema . schemas seem `` actually '' propagate ; e.g . dropping keyspace actually drops keyspace . change migration unnecessary passiveannounce get 's called part migration.announce ( ) routine n't need change apply ( ) behavior . '' 'committed w/migration.apply change removed . ' `` @ brandon record hh turned n't know . '' 'fyi fixes issue introduced issue cassandra-5025 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>uses dataoutputbuffer.recycler possible use cases { { dataoutputbuffer.recycler } } , prevents couple ( larger ) allocations . ( provide patch soon ) patch uses recycled { { dataoutputbuffer } } instead allocating new ones.\\nalso introduces { { dataoutputbuffer.asnewbuffer ( ) } } replace { { bytebuffer.wrap ( out.getdata ( ) 0 out.getlength ( ) ) } } .\\n\\n||trunk| [ branch|https : //github.com/apache/cassandra/compare/trunk ... ] | [ testall|http : //cassci.datastax.com/view/dev/view/snazy/job/snazy-11971-more-recycler-trunk-testall/lastsuccessfulbuild/ ] | [ dtest|http : //cassci.datastax.com/view/dev/view/snazy/job/snazy-11971-more-recycler-trunk-dtest/lastsuccessfulbuild/ ] ' '+1 ' 'thanks ! \\ncommitted [ 063e91754b22a28a43efccb0c238c577a6bd0b8a|https : //github.com/apache/cassandra/commit/063e91754b22a28a43efccb0c238c577a6bd0b8a ] [ trunk|https : //github.com/apache/cassandra/tree/trunk ] \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>streaming commitlog backup problems current sst backups 1 ) current backup n't allow us restore point time ( within sst ) 2 ) current sst implementation needs backup read filesystem hence additional io normal operational disks 3 ) 1.0 removed flush interval size flush triggered per cf , use cases less writes becomes increasingly difficult time right . 4 ) use cases needs bi external ( non cassandra ) , needs data regular intervals waiting longer unpredictable intervals . disadvantages new solution 1 ) head processing mutations recover phase . 2 ) complicated solution copying file archive . additional advantages : online offline restore . close live incremental backup . note : listener agent gets restarted , agents responsibility stream files missed incomplete . 3 options initial implementation : 1 ) backup - &gt; socket connected switch commit log send new updates via socket . 2 ) stream - &gt; take absolute path file read file send updates via socket . 3 ) restore - &gt; get serialized bytes apply 's mutation . side note : ( related patch ) agent take incremental backup planned open sourced soon ( name : priam ) . way 'll get null future back archive tack submitted ; errors 'll get executionexception call get ( ) never null.\\n\\nupdated getarchivingsegmentnames javadoc emphasize include failed archive attempts . sure intended . '' '+1 ' 'committed w/ final improvements yaml comments</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>upgrade thrift 0.5.0 's finally new thrift release . gives us chance standardize release instead revision thrift trunk . http : //www.apache.org/dist/incubator/thrift/0.4.0-incubating/thrift-0.4.0.tar.gz 're right need using position / limit / remaining . brain-damaged api . may mistake thrift using bytebuffer instead rolling obviously 're stuck . '' 'bq . rowhash class \\n\\nis jdk retarded ? instances rowhash serialize either . ' `` ah sorry 's inner class . token member '' 'can fix overriding inner serialization instead writeobject etc ? ' 'bq . stu mentioned bbutil equals hashcode unnecessary\\n\\nwe also get rid getlong using bb absolute methods instead ' 're imo treat position/limit immutable could even check assertions enabled wrap bb methods throw errors non-absolute methods although might paranoid necessary ' 'bq . fix overriding inner serialization instead writeobject etc ? \\n\\ni suppose could make token serializable instead way . ' ' ( murmurhash `` right '' api take byte [ ] offset length used bytebuffer callers byte [ ] ones ) ' 'this patch removes many butebufferutil calls removes bytearraytoken fixes murmur hash ' 'committed . impressive effort ! ' 'integrated cassandra # 573 ( see [ https : //hudson.apache.org/hudson/job/cassandra/573/ ] ) \\n upgrade thrift 0.5\\npatch jake luciani ; reviewed jbellis cassandra-1367\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>easy oom log replay since memtable limits ignored description low load won\\'t much exceed single memtable\\ 's worth inserts since memtable flushes marks commitlog header `` start replay . '' high load significant amount inserts done flushes queued sort + write executors run trouble replay . ' `` recovery used rely cfs.switchmemtable call onmemtableflush ( ctx ) discard commit log files . patch onmemtableflush ( ctx ) wo n't called recovery . commit log files deleted ? \\n '' ' thinking final table.flush `` normal '' flush take care . thinking corner case `` last write log happened exactly hit threshold triggered no-discard flush final flush saw clean memtable nothing . `` \\n\\npatch 2 ( applies top first ) addresses . ( patch converting cl true singleton replay access normal context operations w/o ugly hacks . ) ' `` bad . got confused log files deleted recovery . turns log files deleted explicitly recoverymanager.dorecovery ( ) . deletion n't rely flushing logic . even v1 patch fine . '' `` n't know either . think 's bug -- something goes wrong recovery definitely n't deleting data n't replayed . 'd feel safer taking think ? '' `` currently cl.discardcompletedsegments ( ) n't really discard log files properly recovery . problem clheaders_ never populated recovery.\\n\\ndeleting log files explicitly recovery may bad . anything goes wrong recovery get either ioexception runtimeexception . either case log files wo n't deleted . '' `` 're right assumes cl instantiated 's starting fresh . 'll commit patch 1 . '' 'integrated cassandra # 282 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/282/ ] ) \\n respect memtable thresholds replaying commit log\\npatch jbellis ; reviewed jun rao \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>possible authorizaton handling impovements 'll create another issue suggestions fixing/improving iauthority interfaces . one lists possible improvements n't related grant/revoke methods . inconsistencies : - create columnfamily : p.create ks cql2 vs. p.create cf cql3 thrift - batch : p.update p.delete cf cql2 vs. p.update cql3 thrift ( despite remove * thrift asking p.delete ) - delete : p.delete cql2 thrift vs. p.update cql3 - drop index : checks cql2 vs. p.alter cf cql3 issues/suggestions - cql2 drop index require authorization - current permission checks inconsistent since performed separately cql2 query processor , thrift cassandraserver cql3 statement classes . move one place . someclasswithabettername.authorize ( operation , ks , cf , user ) , operation would enum ( alter_keyspace , alter_table , create_table , create , use , update etc . ) , cf nullable . - n't respect hierarchy checking permissions , , specific , wrong . take cql3 insert example : require p.update cf full_access either ks cf . however , p.update ks wo n't allow perform statement , full_access . doubt intentional , , say 's wrong . p.update ks allow updates ks 's cfs . examples http : //www.datastax.com/dev/blog/dynamic-permission-allocation-in-cassandra-1-1 point bug , since revoke update ks omega . - currently lack way set permission cassandra/keyspaces resource . think able . see following point . - currently create keyspace must p.create permission keyspace n't even exist yet . superuser create keyspace , superuser must first grant permission create . n't look right . p.create cassandra/keyspaces allow create new keyspaces without explicit permission . - goes create table . need p.create not-yet-existing cf full_access whole ks . p.create ks wo n't . wrong . - since permissions n't map directly statements , describe clearly documentation permissions required cql statement/thrift method . full list current permission requirements : https : //gist.github.com/3978182 one authentication rewrite complete make sure : \\n1 ) validate user existence grant/revoke/list permissions\\n2 ) call iauthority # revokeall ( string droppeduser ) user dropped ' `` attached 'warn-authority.txt ' patch that\\n1 . prints warning startup detects 'authority ' param\\n2 . throws configurationexception 'authority ' n't set 'allowallauthority'\\n\\nthis way upgrading users n't care authorization first place wo n't update anything.\\nthis friendlier exiting 'm 100 % sure friendly . 100 % pretty sure still . '' 'looks good overall . couple comments : \\n\\n- prefer returning permission.none|all instead copyof ; copying makes think original ( possibly copy ) mutable case here\\n- rewrite dropindexstatement ? \\n ' 'do want support `` show permissions granted object x ? '' ' `` attached 'move-resource-on-to-iauthorizer.txt ' moves filtering resource iauthorizer away listpermissionsstatement # execute . '' 'bq . prefer returning permission.none|all instead copyof ; copying makes think original ( possibly copy ) mutable case here\\nthis case simpleauthorizer . old iauthority authorize ( ) return enumset permission.all|none don\\'t return enumset anymore ( enumsset mutable ) . copyof convert set &lt; permission &gt; enumset &lt; permission &gt; .\\n\\nbq . rewrite dropindexstatement ? \\nthere used way get cf dropindexstatement needed one permission check drop index ( alter parent cf ) . way required rewrite.\\n\\nbq . want support `` show permissions granted object x ? `` \\nwe cource . latest ( last ? ) patch makes explicit . ' '+1 ' 'committed thanks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>move bloom_filter_fp_chance compaction options setting n't take affect data recompacted , moved compaction options . alternatively , could index_interval , rebuild startup changed . bq . alternatively could index_interval rebuild startup changed.\\n\\ncan elaborate ? ' `` believe jonathan 's referring sstablereader.load recompute index summary ca n't read disk . fact sstablereader.load already takes boolean decide rebuild bloom filter seems need pass true detect bloom_filter_fp_chance changed . though later detection may require save bffc sstable metadata. `` 'https : //github.com/iamaleksey/cassandra/compare/5015 ' `` probably assume bffpc metadata 's correct ( unless 's ancient strings-in-bloom-filter file ) .\\n\\notherwise lgtm . '' `` bq . probably assume bffpc metadata 's correct ( unless 's ancient strings-in-bloom-filter file ) .\\n\\ncommitted change . thanks . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>cl replayed custom 2i exception node shutdown / drain custom ( non-cf ) 2i throws exception , commitlog get correctly preserved ( segments wo n't get discarded segment tracking correct ) . however , gets replayed node startup , 're making decision whether replay commit log . cl segment starts getting replayed , since non-discarded segments process 're checking whether every [ individual mutation|https : //github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/commitlogreplayer.java # l215 ] commit log already committed . information sstables taken [ live sstables disk|https : //github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/commitlogreplayer.java # l250-l256 ] . flush running ( memtable queried ) flush\\nsuccessfull ( sstable queried ) flush unsuccessful ( memtable queried ) flush unsuccessful + node restarted\\n ( cl replay data it\\ 'll available memtable ) . 3.0 patch relies behaviour.\\n\\n| [ 3.0|https : //github.com/ifesdjeen/cassandra/tree/12956-3.0-v2 ] | [ utest|https : //cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-12956-3.0-v2-testall/ ] | [ dtest|https : //cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-12956-3.0-v2-dtest/ ] |\\n| [ 3.x|https : //github.com/ifesdjeen/cassandra/tree/12956-3.x ] | [ utest|https : //cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-12956-3.x-testall/ ] | [ dtest|https : //cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-12956-3.x-dtest/ ] |\\n| [ trunk|https : //github.com/ifesdjeen/cassandra/tree/12956-trunk ] | [ utest|https : //cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-12956-trunk-testall/ ] | [ dtest|https : //cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-12956-trunk-dtest/ ] |\\n ' 'thanks much cleaner safer indeed . one small issue nit : \\n- [ 2i flush|https : //github.com/apache/cassandra/compare/trunk ... # diff-98f5acb96aa6d684781936c141132e2ar1082 ] done { { truncate } } false.\\n- [ barrier await|https : //github.com/apache/cassandra/compare/trunk ... # diff-98f5acb96aa6d684781936c141132e2ar1130 ] necessary flush commence [ happened|https : //github.com/apache/cassandra/compare/trunk ... # diff-98f5acb96aa6d684781936c141132e2ar1071 ] .\\n ' `` great thank you.\\ni 've removed duplicate { { await } } thanks catching . { { truncate } } check { { false } } done [ flush memtable|https : //github.com/apache/cassandra/compare/trunk ... # diff-98f5acb96aa6d684781936c141132e2ar1097 ] .\\n\\ni 've applied change branches ci . '' 'thanks committed 6f90e55e7e23cbe814a3232c8d1ec67f2ff2a537 . ' 'thank ! ' 'what fix versions ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>memtable flushed without hostid version `` '' newer commitlogreplay ticket cassandra-16619 files changed allow cassandra store hostid new `` '' sstable version . sstables flushed commitlogreplay miss hostid info . next cassandra startup , sstables still present , system.log show : { { warn origin 3 sstables unknown n't match local node ; commitlogintervals ignored } } { { warn } } { { { } origin 3 sstables unknown n't match local node ; commitlogintervals ignored { } } } { { { } { } } } { { } } debug.log show list sstables , witch include `` md '' `` '' version ( upgradesstables ) : { { ignored commitlogintervals following sstables : [ /var/lib/cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/me-3-big-data.db , /var/lib/cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/md-1-big-data.db , /var/lib/cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/md-2-big-data.db ] } } https : //issues.apache.org/jira/browse/cassandra-16619 https : //github.com/apache/cassandra/commit/4effa4efbe456d2f5335ef4b6db39804eab21042\\r\\n\\r\\nalso two `` merge branch \\'cassandra-4.0\\ ' cassandra-4.1 '' commits ( 14936d0bd3716ed251e799a264f5ab16d51b893b [ |https : //github.com/apache/cassandra/commit/14936d0bd3716ed251e799a264f5ab16d51b893b ] [ f8dd1931eeacb290af9e246376468f3704891cc4|https : //github.com/apache/cassandra/commit/f8dd1931eeacb290af9e246376468f3704891cc4 ] ) ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>additional compaction logging currently , viewing results past compactions requires parsing log looking compaction history system table , n't information , example , flushed sstables previously compacted . proposal extend information captured compaction . initially , would done jmx call , proves useful much overhead , might feature could enabled compaction strategy time . initial log information would include : - compaction strategy type controlling column family - set sstables included compaction strategy - information flushes compactions , including times involved sstables - information sstables , including generation , size , tokens - additional metadata strategy wishes add compaction sstable , like level sstable type compaction performed [ `` 've pushed [ branch|https : //github.com/carlyeks/cassandra/tree/ticket/10805 ] . adds compactionlogger class compaction strategies use . currently work happens either flush runnable compactiontask statements could added strategies use.\\n\\nthere added table parameter { { log_all } } set true start logging right away . otherwise jmx operation start compaction logging cf . '' ' * use logback logging file ? possible create special logger goes separate file . feels wrong implement log rotation etc this\\n * enable/disable change log level logger ? \\n * would nice logging could bit self-describing human readable - json ? ' `` * initially using logback changed getting incomplete files ( since n't know new file created ) . looking [ logback docs|http : //logback.qos.ch/manual/appenders.html # rollingfileappender ] seems like probably need implement two classes make sure logs complete\\n * work well ; 'll make sure logger name table assigned order capture output one table\\n * good point ; could also simplify multiple-line events '' `` idea jira months ago busy/distracted anything . really glad see 's added others 's also actively addressed . fantastic.\\n\\nwe 're comparative studies stcs dtcs huge c * user enhancements really provide actionable metrics people desiring tune compaction procedures. `` `` 've looking logback might able use logback directly . biggest problem n't notification log file changes . need know logfile changing log sstables already disk logfile independent ( old ones deleted ) .\\nwe wo n't able use logback loggers logback.xml wo n't able use loggers currently defined since additional information needs passed creation logger know tables ' files log new file.\\nwe still use infrastructure logback able execute mechanics log rotations n't responsible ; wo n't seamless updating logback.xml files reread . '' `` 've pushed new version branch updated cassandra-6696 changes outputs json objects . n't looked use parts logback would want yet current approach looks ok except reimplementing log rolling 'll take look early next week . '' 'it looks good comments ; \\n\\n * timestamps log entries\\n * could perhaps useful log compaction strategy handles repaired unrepaired data data directory handling instead strategy id ? perhaps log mapping startup figure ? \\n\\nand nit - remove redundant `` . '' compactionlogger.\\n\\nand idea - feel free ignore could make serialization \\'pluggable\\ ' compactionlogger ? could example nodes cluster write socket somewhere don\\'t ship log files visualize ? could followup ticket when/if anyone needs though ' `` looks really promising . 've played around branch minor changes [ pr|https : //github.com/carlyeks/cassandra/pull/1/files ] .\\n\\nhowever 'm still sure plan implement file rolling logic . getting files rolled logback archive manually afterwards would work perfectly fine . '' `` [ ~spodxx @ gmail.com ] n't possible use logback everywhere else need know log file rotated add current sstables . n't log file wo n't represent state compaction strategy time start . 'll end replacing custom logging logic logback right 'm using something really simple focus rest logging system . '' 'setting patch available ' `` 've pushed new version addresses comments . started { { compactionlogger.writer } } interface ; wanted know made sense change objects could serialized json serialized differently different interfaces.\\n\\ni 've kicked new utests/dtests 'll see looks . '' 'wanted run tests logging default get startup : \\n { code } \\njava.lang.nullpointerexception\\n org.apache.cassandra.db.compaction.compactionstrategymanager.getstrategyfolders ( ) \\n org.apache.cassandra.db.compaction.compactionlogger.startstrategy ( ) \\n org.apache.cassandra.db.compaction.compactionlogger.lambda $ compactionstrategymap $ 1 ( ) \\n java.util.arraylist.foreach ( ) \\n org.apache.cassandra.db.compaction.compactionlogger.lambda $ foreach $ 0 ( ) \\n java.util.arrays $ arraylist.foreach ( ) \\n org.apache.cassandra.db.compaction.compactionlogger.foreach ( ) \\n org.apache.cassandra.db.compaction.compactionlogger.compactionstrategymap ( ) \\n org.apache.cassandra.db.compaction.compactionlogger.startstrategies ( ) \\n org.apache.cassandra.db.compaction.compactionlogger.enable ( ) \\n org.apache.cassandra.db.compaction.compactionstrategymanager.startup ( ) \\n org.apache.cassandra.db.compaction.compactionstrategymanager.reload ( ) \\n org.apache.cassandra.db.compaction.compactionstrategymanager. &lt; init &gt; ( ) \\n org.apache.cassandra.db.columnfamilystore. &lt; init &gt; ( ) \\n org.apache.cassandra.db.columnfamilystore. &lt; init &gt; ( ) \\n org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore ( ) \\n org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore ( ) \\n org.apache.cassandra.db.keyspace.initcf ( ) \\n org.apache.cassandra.db.keyspace. &lt; init &gt; ( ) \\n org.apache.cassandra.db.keyspace.open ( ) \\n org.apache.cassandra.db.keyspace.open ( ) \\n org.apache.cassandra.db.systemkeyspace.checkhealth ( ) \\n org.apache.cassandra.service.startupchecks $ 8.execute ( ) \\n org.apache.cassandra.service.startupchecks.verify ( ) \\n org.apache.cassandra.service.cassandradaemon.setup ( ) \\n org.apache.cassandra.service.cassandradaemon.activate ( ) \\n org.apache.cassandra.service.cassandradaemon.main ( ) \\n { code } \\n\\nother code lgtm ' ' pushed new update includes fix also pushed test make sure activate compaction logger column families . [ utest|http : //cassci.datastax.com/job/carlyeks-ticket-10805-logall-testall/ ] [ dtest|http : //cassci.datastax.com/job/carlyeks-ticket-10805-logall-dtest/ ] \\n\\ni need dig dtest results figure whether caused new logging . ' ' [ ~krummas ] reran logall branch dtests look much better . ' 'nice +1 ' 'thanks [ ~krummas ] . commited [ e16d8a7|https : //git-wip-us.apache.org/repos/asf/cassandra/ ? p=cassandra.git ; a=commit ; h=e16d8a7a667d50271a183a95be894126cb2a5414 ] .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>incorrect output `` nodetool status -r '' nodetool status -r working well c * 4 , version : { code : java } [ root @ foo001 ~ ] # nodetool version releaseversion : 4.0-beta3 { code } without resolving : { code : java } [ root @ foo001 ~ ] # nodetool status datacenter : v4ch ================ status=up/down |/ state=normal/leaving/joining/moving -- address load tokens owns ( effective ) host id rack un 1.2.3.4 363.68 kib 128 ? 92ae4c39-edb3-4e67-8623-b49fd8301b66 rac1 un 1.2.3.5 109.71 kib 128 ? d80647a8-32b2-4a8f-8022-f5ae3ce8fbb2 rac1 { code } resolving : { code : java } [ root @ foo001 ~ ] # nodetool status -r datacenter : v4ch ================ status=up/down |/ state=normal/leaving/joining/moving -- address load tokens owns ( effective ) host id rack ? n foo001.tab.com ? 128 ? rac1 ? n foo002.tab.com ? 128 ? rac1 { code } changed ips hostnames . see { { toolrunner } } test tooling # collaborating ' 'fixed via pr https : //github.com/apache/cassandra/pull/845 ' 'thank [ ~wolfenhaut ] : ) ' 'thanks [ ~wolfenhaut ] . would consider adding test moving `` ready revidew '' ? ' 'sure thanks ! ' 'thanks taking look patch . reviewed left comments pr.\\r\\n\\r\\nwhile looking think noticed one potential flaw around looking things endpoint ( unrelated resolved host lookup logic ) . seems like tool never showing anything definitive `` owns ( effective ) '' column . resolved table always produces \\ ' ? \\ ' . ' `` review . mostly aligned adam mentioned + minor . real concern whether tackle 'owns ' issue new ticket . '' `` personally n't see reason split new ticket n't feel strongly . fits description 's closely related code . take look revising patch [ ~wolfenhaut ] n't care . '' `` [ ~wolfenhaut ] checking see 're going time get back review ? problem carrying work forward . wanted check first . '' ' [ ~aholmber ] \\r\\n\\r\\nbeen little busy feel free run want ... \\r\\n\\r\\nthanks letting help ! \\r\\n\\r\\n -- scott\\xa0 ' 'will . thanks getting back matter thanks getting far . ' 'updated branch pr : \\r\\nhttps : //github.com/aholmberg/cassandra/pull/38\\r\\n\\r\\ntl ; dr several maps status broken depending whether host resolved ( -r ) port printing enabled ( -pp ) using different forms string conversion . everything normalized always using form `` &lt; ip &gt; : &lt; port &gt; '' command internal maps . port printing dns resolution apply printing host string table output.\\r\\n\\r\\nci running : \\r\\nhttps : //app.circleci.com/pipelines/github/aholmberg/cassandra ? branch=cassandra-16283 ' `` see port fixed unit tests jvm-dtest codified question marks . 'll address test issues tomorrow . '' 'thank [ ~aholmber ] . please let know done happy review patch . ' `` branch updated . full ci run finished revealing in-jvm test overlooked . another limited run still running tweak think 's ready review . thanks advance . '' 'jenkins run pushed [ | [ https : //jenkins-cm4.apache.org/job/cassandra-devbranch/333/ # showfailureslink ] . ] \\r\\n\\r\\nreview progress thanks : ) \\xa0 ' 'the patch looks good left small comments [ here|https : //github.com/ekaterinadimitrova2/cassandra/commit/9e79a336bf5348ac6fae59dac7ffa60eb4c29bae ] \\xa0- created new branch squashed commits reviewing.\\r\\n\\r\\ni believe two main things changes.txt entry missing last in-jvm test [ failing|https : //jenkins-cm4.apache.org/job/cassandra-devbranch/333/ ] \\xa0to fixed.\\xa0 ' 'thanks review . pushed updates . think ci looks good well . ' `` thanks [ ~wolfenhaut ] [ ~aholmber ] patch ! \\r\\nlgtm +1\\r\\ncircleci unrelated failures\\r\\n [ ~brandon.williams ] review second committer please ? \\r\\ni believe berenguer 's concerns also addressed . need move changes.txt update top commit . : - ) `` `` +1\\r\\n\\r\\nbq . need move changes.txt update top commit.\\r\\n\\r\\nhistorically 've left patch 's committer handle often ends conflicting patch . '' 'patch rebased squashed [ here|https : //github.com/ekaterinadimitrova2/cassandra/commit/f99faca3e62dad3c748673e306433ce242a31b92 ] \\r\\ncommit pending final jenkins run progress [ here|https : //jenkins-cm4.apache.org/job/cassandra-devbranch/352/ ] ' 'patch committed [ here| https : //github.com/apache/cassandra/commit/b61860c76e9cf1eebfb7d29dc4f4420955f62bb4 ] \\r\\n\\r\\nthank !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>general minor tidying collationcontroller path lot unnecessary boiler plate grabbing iterator in-memory column family . patch : * removes fakecellname * avoids wrapping non-ondiskatomiterator ondiskatomiterator except wrapping useful * removes columnslice.navigablesetiterator creates simpler direct equivalent abtc * construct sliceiterator either absc abtc one slice requested ( returns slice iterator ) * construct multiple list indirections absc constructing slice * shares forward/reverse iterators absc slices full-iteration * avoids ( n ) comparisons collation results absc , using knowledge columns provided insertion order merge iterator specific case think may ? '' `` 'invert ' case although still seem fine . n't trust much ¯\\\\_ ( ツ ) _/¯ '' 'committed thanks . ' `` well sure : \\n\\n * translate slice range \\\\ [ lb .. ub\\\\ ) - i.e . lb inclusive ; \\n * search range [ 0 .. lb ) translate next slice - lb exclusive here\\n\\nso 'm pretty sure 's fine - lb always gets returned first iterator never searched building following iterator exact boundary start search . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>n't tie client side use abstracttype jdbc currently expose abstracttype java clients want reuse though cql.jdbc . * classes . think n't tied jdbc standard . jdbc make sql db , cassandra ( cql sql never ) . typically , fair amount jdbc standard implemented c * , number specificity c * jdbc ( typically set maps collections ) . propose extract simple type classes compose decompose method ( without ties jdbc , would allow jdbc specific method types ) purpose exporting separate jar clients ( could put org.apache.cassandra.type package instance ) . could deprecate jdbc classes basically schedule cql2 . let note * * saying n't jdbc driver cassandra . bq . propose extract simple type classes compose decompose method\\n\\nwhy expose abstracttype classes point ? ' 'bq . expose abstracttype classes point ? \\n\\ni though think least currently means pull pretty mull cassandra ( goal allow clients pull minimum useful ) . ' ' would suggest adding { { getstring ( bytebuffer bytes ) } } { { gettype ( ) } } well . jdbc specific stuff like { { iscurrency ( ) } } { { issigned ( ) } } course easily moved client side . ' `` [ ~ardot ] [ ~mfiguiere ] [ ~urandom ] n't suppose interest one ? '' `` first cut : \\n\\nhttps : //github.com/carlyeks/cassandra/tree/4495\\n\\ni n't like name ( composer ) using abstracttype et al caused lot conflicts . 'm open better name abstracttype really best name 'll fully qualify names . '' 'could solution enhanced address collection types ? ( list set map ) . really awkward client side . \\n\\n { { gettype ( ) } } class would also handy passed abstracttype.\\n\\nis reason remove { { o.a.c.cql.jdbc } } package build ? \\n\\ngreat job btw ... ' 'just pushed couple new commits.\\n\\n- removed o.a.c.cql.jdbc namespace\\n- added gettype\\n- added list set map implementations\\n - haven\\'t figured merge { list set map } type\\ 's compose decompose usage validate\\n- added `` ascompose ( ) '' call abstracttype returns abstractcomposer type ' `` bq . n't figured merge { list set map } \\n\\ncan elaborate ? '' 'just meant collection-types use collection-composers depend validation composers . sure worth adding happy without . ' `` ca n't move { { validate ( ) } } method classes { { o.a.c.types } } ones { { o.a.c.db.marshal } } ? collections classes { { o.a.c.types } } access . '' `` adds validate composer.\\n\\na couple things aware : \\n- replaced o.a.c.db.marshal.marshalexception o.a.c.types.marshalexception may thrown client\\n- collection composer validate ops reasoning below\\n\\ni 'm sure reason validate collections . seems previous validation would fail example map &lt; map &lt; timeuuid string &gt; timeuuid &gt; validation map actually validating using value type map rather iterating values making sure entry valid . entry-wise validation happens call compose . '' `` n't really look detail patch 's worth 've somehow never fan compose/decompose terminology . 'd prefer say encode/decode serialize/deserialize . booleancodec booleanserializer sounds better hear booleancomposer . feel free discard opinion 's french composer sounds perfectly fine guys . '' ' like name lot . attached updated version renames * serializer renames methods ( de ) serialize . ' 'wdyt rick ? ' 'lgtm . problem incorporate client side work . thanks enhancements ! ' 'alright committed . ' `` info took liberty following renames : \\n * renamed package type serializers since 's classes called.\\n * made abstractserializer interface since n't see good reason abstract class . renamed typeserializer too.\\n * renamed ascomposer ( ) method abstracttype getserializer ( ) ( suspect left-over initial patch ) '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>warning message aggregation queries n't specify table name aggregate type existing aggregation query warning messages cassandra.log selectstatement.java : _ ' [ warn ] aggregation query used without partition key ' _ { _ } ' { _ } { _ } [ warn ] { _ } _aggregation query used multiple partition keys ( restriction ) ' _ missing helpful details would assist users quickly identifying offending query . table name type aggregation would useful additions warn message . addition , following example slow query logger printing query string debug level would especially helpful . tiny patch sure building tests make sense . built j8 j11 4.0 branch here\\r\\n\\r\\ndo want run 5h ci ? seems like overkill . compiled . ' `` let 's see circle complies one time.\\r\\n\\r\\n||branch||ci||\\r\\n| [ 4.0|https : //github.com/driftx/cassandra/tree/cassandra-18219-4.0 ] | [ j8|https : //app.circleci.com/pipelines/github/driftx/cassandra/855/workflows/ef784b41-4d20-426c-892f-fd7f48137394 ] [ j11|https : //app.circleci.com/pipelines/github/driftx/cassandra/855/workflows/d575b5f3-d99b-48eb-a012-f73e0c263cd7 ] |\\r\\n| [ 4.1|https : //github.com/driftx/cassandra/tree/cassandra-18219-4.1 ] | [ j8|https : //app.circleci.com/pipelines/github/driftx/cassandra/856/workflows/ab2c31f2-d80a-47ce-9645-7d16bad8b4e1 ] [ j11|https : //app.circleci.com/pipelines/github/driftx/cassandra/856/workflows/75338b20-1ad7-479d-bbee-59f674b0cbb8 ] |\\r\\n| [ trunk|https : //github.com/driftx/cassandra/tree/cassandra-18219-trunk ] | [ j8|https : //app.circleci.com/pipelines/github/driftx/cassandra/858/workflows/c402f0c3-f010-443e-8cd3-b0181487cc37 ] [ j11|https : //app.circleci.com/pipelines/github/driftx/cassandra/858/workflows/8e85b273-c2b4-4508-9a3c-7147938b1b0f ] |\\r\\n\\r\\n '' `` yeah run ci afraid . dtests parse log files could break iirc i.e . case thx [ ~brandon.williams ] . 'm +1 unless sbdy else objects . '' 'everything looks good +1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>messagedigests created several places , centralize creation error handling messagedigest.getinstance ( `` somealg '' ) throws nosuchalgorithm exception ( checked exception ) . annoying causes everyone uses standard algs like md5 surround code try/catch . concentrate creation one method n't raise exception ( i.e . catches nosuchalgorithm raises runtimeexception ) clean code little . attached patch puts messagedigest creation util class . ' `` let 's use http : //guava-libraries.googlecode.com/svn-history/r2/trunk/javadoc/com/google/common/io/messagedigestalgorithm.html '' 'the link svn revision 2 sep 15 2009 `` initial code dump '' . doesn\\'t look like messagedigestalgorithm ( equivalent ) exists guava ( even r2 despite fact javadoc ) . ' 'rebased ' `` bq . n't look like messagedigestalgorithm ( equivalent ) exists guava\\n\\nbummer.\\n\\nfbutilities fine let 's refactor use threadlocal instead going back using synchronized md5 directly . '' `` thought could n't convince safe without looking invocations closely . particular thread inadvertently passed result thread local another thread ( e.g . via putting wrappedrunnable passing another stage ) . \\n\\nthe subtle difference : \\n\\n { code } \\nx = getthreadlocal ( ) ; \\nstage.submit ( new runnable ( ) { x.dosomething ( ) } ) ; \\n { code } \\n\\nand \\n\\n { code } \\nrunnable r = new runnable ( ) { getthreadlocal ( ) .dosomething ( ) } \\nstage.submit ( r ) ; \\n { code } \\n\\nworried would exceptions errors incorrect results . also concerned implementations digests might return underlying byte array ( opposed copy ) used compute hash .digest ( ) called.\\n\\nthat said attached patch uses thread local instance . '' 'rebased + updated guidgenerator use fbutilities threadlocal + committed ' 'integrated cassandra-0.7 # 296 ( see [ https : //hudson.apache.org/hudson/job/cassandra-0.7/296/ ] ) \\n centralize messagedigest creation use threadlocals md5s\\npatch mdennis ; reviewed jbellis cassandra-2107\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>jvm exit jmx fails bind already running cassandra instance , reason try start another one , happens : { noformat } info jna mlockall successful warn jmx enabled receive remote connections . please see cassandra-env.sh info . error error starting local jmx server : java.rmi.server.exportexception : port already use : 7199 ; nested exception : java.net.bindexception : address already use sun.rmi.transport.tcp.tcptransport.listen ( ) ~ [ ] sun.rmi.transport.tcp.tcptransport.exportobject ( ) ~ [ ] sun.rmi.transport.tcp.tcpendpoint.exportobject ( ) ~ [ ] sun.rmi.transport.liveref.exportobject ( ) ~ [ ] sun.rmi.server.unicastserverref.exportobject ( ) ~ [ ] sun.rmi.registry.registryimpl.setup ( ) ~ [ ] sun.rmi.registry.registryimpl. &lt; init &gt; ( ) ~ [ ] java.rmi.registry.locateregistry.createregistry ( ) ~ [ ] org.apache.cassandra.service.cassandradaemon.maybeinitjmx ( ) [ main/ : na ] org.apache.cassandra.service.cassandradaemon.setup ( ) [ main/ : na ] org.apache.cassandra.service.cassandradaemon.activate ( ) [ main/ : na ] org.apache.cassandra.service.cassandradaemon.main ( ) [ main/ : na ] caused : java.net.bindexception : address already use java.net.plainsocketimpl.socketbind ( native method ) ~ [ ] java.net.abstractplainsocketimpl.bind ( ) ~ [ ] java.net.serversocket.bind ( ) ~ [ ] java.net.serversocket. &lt; init &gt; ( ) ~ [ ] javax.net.defaultserversocketfactory.createserversocket ( ) ~ [ ] org.apache.cassandra.utils.rmiserversocketfactoryimpl.createserversocket ( ) ~ [ main/ : na ] sun.rmi.transport.tcp.tcpendpoint.newserversocket ( ) ~ [ ] sun.rmi.transport.tcp.tcptransport.listen ( ) ~ [ ] ... 11 common frames omitted { noformat } however startup continues , ends replaying commitlogs , probably good thing . address already use\\n java.net.plainsocketimpl.socketbind ( native method ) ~ [ ] \\n java.net.abstractplainsocketimpl.bind ( ) ~ [ ] \\n java.net.serversocket.bind ( ) ~ [ ] \\n java.net.serversocket. &lt; init &gt; ( ) ~ [ ] \\n javax.net.defaultserversocketfactory.createserversocket ( ) ~ [ ] \\n org.apache.cassandra.utils.rmiserversocketfactoryimpl.createserversocket ( ) ~ [ main/ : na ] \\n sun.rmi.transport.tcp.tcpendpoint.newserversocket ( ) ~ [ ] \\n sun.rmi.transport.tcp.tcptransport.listen ( ) ~ [ ] \\n sun.rmi.transport.tcp.tcptransport.exportobject ( ) ~ [ ] \\n sun.rmi.transport.tcp.tcpendpoint.exportobject ( ) ~ [ ] \\n sun.rmi.transport.liveref.exportobject ( ) ~ [ ] \\n sun.rmi.server.unicastserverref.exportobject ( ) ~ [ ] \\n sun.rmi.registry.registryimpl.setup ( ) ~ [ ] \\n sun.rmi.registry.registryimpl. &lt; init &gt; ( ) ~ [ ] \\n java.rmi.registry.locateregistry.createregistry ( ) ~ [ ] \\n org.apache.cassandra.utils.jmxserverutils.createjmxserver ( ) ~ [ main/ : na ] \\n org.apache.cassandra.service.cassandradaemon.maybeinitjmx ( ) [ main/ : na ] \\n org.apache.cassandra.service.cassandradaemon.setup ( ) [ main/ : na ] \\n org.apache.cassandra.service.cassandradaemon.activate ( ) [ main/ : na ] \\n org.apache.cassandra.service.cassandradaemon.main ( ) [ main/ : na ] \\n ( trunk ) mshuler @ mana : ~/git/cassandra $ \\n { noformat } ' ' found config change helps env situation : \\n { { -dcassandra.jmx.local.port=7199 } } instead { { -dcom.sun.management.jmxremote.port=7199 } } ' `` { { com.sun.management.jmxremote.port } } property set jvm automatically uses inbuilt agent initialize jmx connector server . 'expected ' way configure jmx server modify { { cassandra-env.sh } } desired port/ssl/auth settings fact setting remote connections actually sets properties . see though passing directly via { { jvm_args } } also worked without error ( bypassing setup { { cassandra-env.sh } } ) c * would interpret presence property indicate jmx server already setup would n't attempt initialize ; 'm afraid overlooked cassandra-10091 . \\n\\nsince cassandra-10091 c * expects always control jmx configuration . { { cassandra-env.sh } } modified { { com.sun.management.jmxremote.port } } never set ( 's replaced { { cassandra.jmx.remote.port } } ) . top cassandra-11540 makes c * setup error face jmx bind errors whereas previously would . \\n\\nbq . found config change helps env situation\\n\\nthat create slightly different behaviour jmx server listen loopback address previous setup would bind { { 0.0.0.0 } } . want still support remote jmx set { { cassandra.jmx.remote.port } } instead . \\n\\ni 've added cassandra-11725 provide clearer message kind misconfiguration occurs.\\n '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>make sure sstables left compaction get deleted logged opening columnfamily , cassandra checks sstable files ' ancestors skips loading already compacted ones . files expected deleted , currently never happens . also , indication skipping loading file log , confusing especially upgradesstables . sstable ancestor x x safe delete.\\n\\nspecifically lcs create multiple sstables given set ancestors unless know finished compaction ( finished writing resulting descendant sstables ) could lose data delete ancestors themselves.\\n\\none possible fix : \\n\\n # add flag sstm `` final sstable compaction '' \\n # scan sstables delete ancestors find marker descendants\\n # otherwise delete * descendants * leave ancestors alone ( don\\'t doublecount data counters ) ' `` think 're right.\\n\\na fourth pseudo-solution could wait end compaction rename newly created writers ( i.e remove tmp markers ) . 's bulletproof though n't think rename multiple files atomically wanted mention it.\\n\\nmaybe least 1.1 3. best/simplest option . longer term maybe 1. better . '' 'you\\ 're right don\\'t actually need marker since compaction completes next step remove ancestors . think `` ancestors still alive assume compaction didn\\'t finish delete descendants '' good enough . ' 'bq . think `` ancestors still alive assume compaction didn\\'t finish delete descendants '' good enough.\\n\\nyeah agreed . ' `` hum wait works ancestors though . one ancestor n't get deleted reason ( sstabledeletingtask executed crash ) ? 's easy enough check * * ancestors n't ? 're back square one : ( `` `` 're right . guess need compaction-finished flag all.\\n\\ninstead storing sstable metadata maybe could store system.local way truncation information . unfortunately 1.1 n't support maps 'd two separate implementations 1.1 1.2.\\n\\nshould say 1.1 'll retain sstables ( counter users get overcounts everyone else gets extra compaction work ) fix better 1.2 ? '' 'something like ... \\n\\n { code } \\ncreate table compaction_log ( \\n id uuid primary key \\n inputs set &lt; int &gt; \\n outputs set &lt; int &gt; \\n ) ; \\n { code } \\n\\nwhen start compaction add log . finish remove . restart compaction_log empty remove sstables outputs set . ' `` bq . say 1.1 'll retain sstables\\n\\nfor 1.1 'd suggest fourth pseudo-solution i.e . moving renaming newly created writes end compaction ( 's trivial ) . startup could indeed retain sstables normal cf counter would keep removing predecessors . would n't totally fix risk losing counters would make unlikely ( 'd need fail exactly middle bulk renaming newly create sstable writers ) retaining sstables would make easy overcounts.\\n\\nfor 1.2 compaction_log solution seem reasonable . '' `` v2 implements sylvain 's idea renames written sstables end compaction.\\n\\nfor 1.2 let 's open different issue jonathan 's suggestion . '' `` code v2 looks alright let 's also disable filtering columnfamilystore.ctor non-counter cfs take zero chance losing data ( since reusing already compacted sstable bit inefficient harmless ) .\\n\\nbq . 1.2 let 's open different issue jonathan 's suggestion\\n\\nagreed . '' 'attached v3 also changes filtering part counter cf . ' `` +1 ( though commit v1 along way way keeping sstable 're going use even 's counters ) . '' 'committed v1 + v3 opened cassandra-5151 better solution .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>regression prevents recognizing local reads description +1 ' 'committed ' 'integrated cassandra # 237 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/237/ ] ) \\n fix regression recognizing local reads 828148.\\npatch jbellis ; reviewed brandon williams \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>autosaving keycache system load time improvements . cassandra-2392 saves index summary disk ... saved cache still scan index get data . might able separate sstr.load let load index summary , sst 's loaded might able check bloomfilter random io fewer index 's populate keycache . addition minor fix npe compactioninfo.getid ( ) .tostring ( ) ' 'lgtm +1\\n\\nnit : \\n\\n { { public compactioninfo ( uuid id operationtype tasktype long bytescomplete long totalbytes string unit ) } } \\n\\nwould better w/ parameters renamed to\\n\\n { { public compactioninfo ( uuid id operationtype tasktype long complete long total string unit ) } } ' '+1 jonathan comment also think would great add compactioninfo ( operationtype long complete long total string unit ) constructor don\\'t pass `` null '' uuid argument ( hide implementation detail ) . ' 'committed fix ! thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>add convenient way reset node 's schema people often encounter schema disagreement one node sync . get back sync , shutdown node , move schema * migration * files system ks , start back . rather go process , would nice could tell node reset schema . truncate schema migration cfs . local truncate ( columnfamilystore.truncate ) rather pushing cluster.\\n\\nschema management done systemtable.java . ' `` first try apache cassandra patch possible problems way futures handled systemtable.java.\\ni also n't know write proper unit test type functionnality . '' `` thanks patch ! n't think thrift-level call though since clients n't care machine 're connected many client libs abstract away . jmx call nodetool command would appropriate . '' `` mean installed thrift nothing ! : - ) \\nalright 'll keep systemtable code push call jmx . '' 'pierre-yves still planning v2 ? ' `` patch ( trunk ) attached add { { nodetool resetlocalschema } } . truncate part basically pierre-yves 's patch . '' 'what patch addresses part problem even schema migrations gets truncated still old data version schema . suggest reset schema version initial_vesion announce ring right truncate done request migration push node ( behavior announce changed cassandra-1391 ) . also note node process migration reads writes could served . ' 'now cassandra-1391 committed reset schema need truncate schema_ { keyspaces columnfamilies columns } re-set schema.instance initial ( blank ) state . ' `` patch add way reset schema truncating schema_ * cf send migration request live node.\\ni 'm sure right way last part ( migration request ) please let know wrong.\\n '' `` almost last things : \\n\\n - n't want remove data like schema.init ( ) metadata keyspaces/columnfamilies ; \\n - better name schema.init ( ) would schema.clear ( ) ; \\n - migrationmanager.resetlocalschema propagate ioexception '' `` v2 attached.\\n\\nbq . n't want remove data like schema.init ( ) metadata keyspaces/columnfamilies ; \\n\\nin v2 schema.clear removes metadata . causes modify table.initcf reload metadata . think way fine please check.\\n\\nbq . better name schema.init ( ) would schema.clear ( ) ; \\n\\ndone.\\n\\nbq . migrationmanager.resetlocalschema propagate ioexception\\n\\ndone . '' ' made styling improvements changed migrationmanager.resetlocalschema ( ) reset local schema even nodes around ( new nodes come request schema anyway ) + made sure nodes schema requested first node version &gt; = 1.1 . ' 'lgtm . ' 'committed ' `` n't ninja pushing 1.1.0 bit loose definition code freeze ? 'm asking revert least form justification something tagged 'new feature ' ends 1.1.0 way freeze would nice . '' `` 'm okay since pierre yuki started work well freeze addresses * * common pain point users 's minimal interplay existing code risk causing regressions elsewhere low . '' `` yeah point clearly n't respect freeze ( 's 'new feature ' ticket ) least comment kind justification * * committing would nice otherwise freeze become far west . communication transparency important . '' `` 'm little confused new logic initcf ( see cassandra-4402 ) . idea clear schema added back gossip n't want clear table.columnfamilystores ? ? '' `` looking back think due nature clear operation receive data remove node ( keyspace + nested cfs ) initialize keyspace would also initcf cf impicitly new table ( ... ) 's nature merge operation n't really know cfs already initialized ( re-creating keyspace example ) would call initcf new cfs avoid order preserving checking cf already initialized try reload attributes already present table.columnfamilystores . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>make startup checks configurable ticket created needs discovered cassandra-17180 . want able configure startup check figured necessary treat startup checks - able configure . ticket making startup checks configurable . ticket done , continue implementation cassandra-17180 implementation gc grace check done . identified one check currently place needs changed reflect configuration implementation filesystemownershipcheck . startup checks configurable via means configuration file , configurable via system properties . ticket aim get rid system properties configuration mechanism , system properties precedence settings configuration file . , next release , aiming get rid system properties configuration mechanism . true # ( overriden cassandra.ignore_rack system property ) \\r\\n { code } \\r\\n\\r\\nwhat think [ ~smiklosovic ] \\xa0 [ ~dcapwell ] ? ' 'we probably need create documentation ticket explain startup check documentation . ' 'thanks mate simple configs obvious check . yes config ticket follow . ' ' { quote } \\xa0thanks mate simple configs obvious check.\\r\\n { quote } \\r\\nok even though still prefer { { check_dc } } check_rack ; ) \\r\\n\\r\\n\\r\\n\\r\\n\\xa0can attach ci results ? ' 'once settable via system property be\\r\\n\\r\\n-dstartup_checks.dc.enabled=true\\r\\n\\r\\ninstead \\r\\n\\r\\n-dstartup_checks.check_dc.enabled=true\\r\\n\\r\\nthe former makes sense . ' 'https : //ci-cassandra.apache.org/view/patches/job/cassandra-devbranch/1457/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>indexsummarybuilder construct offheap , share memory result build ( ) invocation description waiting ( parking ) \\n\\tat sun.misc.unsafe.park ( native method ) \\n\\t- parking wait &lt; 0x00000000ecb941f8 &gt; ( java.util.concurrent.futuretask ) \\n\\tat java.util.concurrent.locks.locksupport.park ( ) \\n\\tat java.util.concurrent.futuretask.awaitdone ( ) \\n\\tat java.util.concurrent.futuretask.get ( ) \\n\\tat org.apache.cassandra.utils.fbutilities.waitonfuture ( ) \\n\\tat org.apache.cassandra.service.migrationmanager.announce ( ) \\n\\tat org.apache.cassandra.service.migrationmanager.announcenewkeyspace ( ) \\n\\tat org.apache.cassandra.service.migrationmanager.announcenewkeyspace ( ) \\n\\tat org.apache.cassandra.service.migrationmanager.announcenewkeyspace ( ) \\n\\tat org.apache.cassandra.schemaloader.loadschema ( ) \\n\\tat sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) \\n\\tat sun.reflect.nativemethodaccessorimpl.invoke ( ) \\n\\tat sun.reflect.delegatingmethodaccessorimpl.invoke ( ) \\n\\tat java.lang.reflect.method.invoke ( ) \\n\\tat org.junit.runners.model.frameworkmethod $ 1.runreflectivecall ( ) \\n\\tat org.junit.internal.runners.model.reflectivecallable.run ( ) \\n\\tat org.junit.runners.model.frameworkmethod.invokeexplosively ( ) \\n\\tat org.junit.internal.runners.statements.runbefores.evaluate ( ) \\n\\tat org.junit.internal.runners.statements.runafters.evaluate ( ) \\n\\tat org.junit.runners.parentrunner.run ( ) \\n\\tat junit.framework.junit4testadapter.run ( ) \\n\\tat org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.run ( ) \\n\\tat org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.launch ( ) \\n\\tat org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.main ( ) \\n { code } ' `` broken rebase since 8792 's behaviour changed . indexsummarybuilder calculates maxexpectedentries count zero perfectly safe accepted zero length allocations . new version 8792 support ensure maxexpectedentries least 1 allocate zero length region memory . pushed update . '' ' didn\\'t understand offset business comment probably doesn\\'t provide right context . agonizing figured meant . could lack sleep.\\n\\nit might clearer described mismatch memory disk ( way grok ) . in-memory representation set offsets separate zero indexed array disk based representation set offsets entries appended offsets section every offset needs recalculated.\\n\\ni think `` serialization point '' didn\\'t parse point file offsets.\\n { quote } \\nbecause serialize/deserialize native\\n+ // int/long format \\n { quote } \\nand doesn\\'t seem cause mess . it\\ 's native int/long formatness . it\\ 's offsets array two flattened one file.\\n\\nsstablereader line 747 random semi-colon indexsummarybuilder line 216 extra semi-colon.\\n\\nsafememorywriter unit test.\\n\\notherwise +1 ' `` ok 've pushed new version repository improves comments integrates safememorywriter dataoutputtest ( also slightly changing behaviour safememorywriter support way probably generally sensible anyway ) '' `` got ahead thought 'd final +1 've already committed . could still check final changes confirm 're ok n't need rollback would appreciated . '' '+1 new comment makes sense although might already know going .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>disable deprecated keyspace/table thresholds convert guardrails non-guardrail thresholds 'keyspace_count_warn_threshold ' 'table_count_warn_threshold ' configuration settings first added cassandra-16309 4.0-beta4 subsequently deprecated since 4.1-alpha cassandra-17195 replaced/migrated guardrails part cep-3 ( guardrails ) . thresholds removed cassandra.yaml , still allowed existing yaml files . old thresholds disabled removing default values config.java , existing values thresholds converted new guardrails using ' @ replaces ' tag corresponding guardrail values . since old thresholds considered number system keyspace/tables values , ' @ replaces ' conversion subtract current number system tables old value log descriptive message . see dev list discussion : https : //lists.apache.org/thread/0zjg08hrd6xv7lhvo96frz456b2rvr8b \\xa0 [ https : //github.com/apache/cassandra/pull/2467 ] '' 'the patch looks good . running ci rebasing conflicts : \\r\\n\\r\\n||pr||ci||\\r\\n| [ trunk|https : //github.com/apache/cassandra/pull/2467 ] | [ j8|https : //app.circleci.com/pipelines/github/adelapena/cassandra/3022/workflows/477197d6-3881-4e06-840e-ae442bad521a ] [ j11|https : //app.circleci.com/pipelines/github/adelapena/cassandra/3022/workflows/7b762c95-75c3-45e3-b077-f05e1ad10fad ] | ' `` seems repeated tests failed branch missing [ ninja fix|https : //github.com/apache/cassandra/commit/f038059e89b089a19c36b3be58a443b0586fef5c ] . pass rebasing : \\r\\n\\r\\n||pr||ci||\\r\\n| [ trunk|https : //github.com/apache/cassandra/pull/2467 ] | [ j8|https : //app.circleci.com/pipelines/github/adelapena/cassandra/3025/workflows/a7ada882-3622-4d23-8336-d764484caa53 ] [ j11|https : //app.circleci.com/pipelines/github/adelapena/cassandra/3025/workflows/75bf6211-bee7-4fd4-8548-ced6b0a3c4df ] |\\r\\n\\r\\nnow 'll need approval second committer . '' 'this looks good keeping jmx api stable right call . +1 ' ' [ ~brandon.williams ] thanks review : ) \\r\\n\\r\\ncommitted to\\xa0 { { trunk } } [ aac070681bd01ba796814692c3fba04de103d8b4|https : //github.com/apache/cassandra/commit/aac070681bd01ba796814692c3fba04de103d8b4 ] .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>compactions n't work node bootstrapping seems race condition storageservice prevents compactions completing node bootstrap state . able reproduce multiple times throttling streaming throughput extend bootstrap time simultaneously inserting data cluster . problems lies synchronization initserver ( int delay ) reportseverity ( double incr ) methods try acquire instance lock storageservice use synchronized keyword . initserver return bootstrap completed , calls reportseverity block . however , reportseverity called starting compactions compactioninfo thus compactions block bootstrap completes . might severely degrade node 's performance bootstrap might lots compactions pending simultaneously starting serve reads . able solve issue adding separate lock reportseverity removing class level synchronization . course valid approach must assume gossiper 's iendpointstatechangesubscribers could potentially end calling back storageservice 's synchronized methods . however , least moment , seem case . maybe somebody experience codebase comes better solution ? ( might affect dynamicendpointsnitch well , also calls reportseverity setseverity method ) blocked ( object monitor ) \\n org.apache.cassandra.service.storageservice.reportseverity ( ) \\n - waiting lock &lt; 0x00000000ca576ac8 &gt; ( org.apache.cassandra.service.storageservice ) \\n org.apache.cassandra.db.compaction.compactioninfo $ holder.started ( ) \\n org.apache.cassandra.metrics.compactionmetrics.begincompaction ( ) \\n org.apache.cassandra.db.compaction.compactionmanager $ 9.run ( ) \\n java.util.concurrent.executors $ runnableadapter.call ( ) \\n java.util.concurrent.futuretask $ sync.innerrun ( ) \\n java.util.concurrent.futuretask.run ( ) \\n java.util.concurrent.threadpoolexecutor $ worker.runtask ( ) \\n java.util.concurrent.threadpoolexecutor $ worker.run ( ) \\n java.lang.thread.run ( ) \\n { noformat } ' `` seems reason 're synchronizing increment n't need get severity gossip track local atomicdouble instead . '' '+1 ' 'committed . ' 'this looks good .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>add getpendingtasks cfsmbean need add atomicint inc/decr whenever acquire memtablelock rebased patch 0001-cassandra-173-added-cfs-pending-tasks-jmx-attr.txt ' 'patch 001-cassandra-173-added-cfs-pending-tasks-jmx-attr.txt looks good me\\n\\n-arin ' 'committed ' 'integrated cassandra # 132 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/132/ ] ) \\n added cfs pending tasks jmx attribute\\n\\npatch eevans ; reviewed arin sarkissian \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>cfs readstats_ diskreadstats_ missing description [ `` n't also get rid getreaddiskhits mbean + implementation per irc discussion ? '' 'removed diskreadstats.\\n ' 'committed ' 'integrated cassandra # 165 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/165/ ] ) \\n add back read latency stats cfs.getcolumnfamily . patch sammy yu ; reviewed jbellis \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>nullpointerexception consistency manager failed node rejoins error [ ] 2009-04-30 debuggablethreadpoolexecutor.java ( line 89 ) error threadpoolexecutor java.util.concurrent.executionexception : java.lang.nullpointerexception java.util.concurrent.futuretask $ sync.innerget ( ) java.util.concurrent.futuretask.get ( ) org.apache.cassandra.concurrent.debuggablethreadpoolexecutor.afterexecute ( ) java.util.concurrent.threadpoolexecutor $ worker.runtask ( ) java.util.concurrent.threadpoolexecutor $ worker.run ( ) java.lang.thread.run ( ) caused : java.lang.nullpointerexception org.apache.cassandra.service.consistencymanager.run ( ) java.util.concurrent.executors $ runnableadapter.call ( ) java.util.concurrent.futuretask $ sync.innerrun ( ) java.util.concurrent.futuretask.run ( ) java.util.concurrent.threadpoolexecutor $ worker.runtask ( ) ... 2 plus similar ones . config : &lt; replicationfactor &gt; 2 &lt; /replicationfactor &gt; &lt; tables &gt; &lt; table name= '' messages '' &gt; &lt; columnfamily columnsort= '' name '' name= '' base '' / &gt; &lt; columnfamily columnsort= '' name '' name= '' extra '' / &gt; &lt; columnfamily columnsort= '' time '' name= '' standardbytime1 '' / &gt; &lt; columnfamily columnsort= '' time '' name= '' standardbytime2 '' / &gt; &lt; columnfamily columntype= '' super '' columnsort= '' name '' name= '' super1 '' / &gt; &lt; columnfamily columntype= '' super '' columnsort= '' name '' name= '' super2 '' / &gt; &lt; /table &gt; &lt; /tables &gt; inserted data using insert method another node one node failed ( three ) , brought failed node back [ `` n't consistencymanager ( ) constructor contain following line ? \\nthis.replicas_ = replicas_ ; '' `` fwiw regression 's showing logs added change logs exceptions executors instead ignoring . '' `` oops take back . 's regression cassandra-95 . nk11 right constructor got broken . '' 'integrated cassandra # 59 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/59/ ] ) \\n leave variables uninitialized consistencymanager constructor . fixes regression # 95. patch jbellis \\n ' 'looks fixed testing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sstablesinbounds might actually give sstables within bounds due start positions moved sstables problem cassandra-11886 - try fetch sstablesinbounds canonical_sstables , miss actually overlapping sstables . 3.0+ state sstableset want calling method . looks like issue could cause include many sstables compactions think contain droppable tombstones https : //github.com/krummas/cassandra/commits/marcuse/intervaltreesstableset\\nhttp : //cassci.datastax.com/view/dev/view/krummas/job/krummas-marcuse-intervaltreesstableset-testall/\\nhttp : //cassci.datastax.com/view/dev/view/krummas/job/krummas-marcuse-intervaltreesstableset-dtest/\\n\\npatch remove option pick sstableset want returned live sstables supported . want canonical sstables within bounds provide intervaltree built sstables.\\n\\nalso includes cassandra-11886 part might change depending review ticket ' ' [ ~benedict ] - bandwidth review well along w/cassandra-11886 ? ' 'life busy right sure ... ' 'no doubt - hope context would similar enough 11886 delta would pretty small add top . looking make habit . : ) ' 'habits imply future date life hopefully hectic . currently mid-demolition rebuild home eating free non-free time alike . ' `` 'm little confused patch jira comment - n't see ( branch ) removal option provide sstableset ... '' 'maybe looking wrong commit ? rebased squashed [ here|https : //github.com/krummas/cassandra/commits/marcuse/intervaltreesstableset ] \\n { code } \\n- public collection &lt; sstablereader &gt; getoverlappingsstables ( sstableset sstableset iterable &lt; sstablereader &gt; sstables ) \\n+ public collection &lt; sstablereader &gt; getoverlappingsstables ( iterable &lt; sstablereader &gt; sstables ) \\n { code } ' `` probably - earlier commits seemed ticket . thanks.\\n\\ni 'll proper read later would suggest renaming methods include implicit sstableset.live 's still minimally front-and-centre functionality used perhaps renaming select selectlive sstablesinbounds perhaps inboundslive ( sstables ) ? '' 'pushed new commit method renames branch triggered new cassci builds ' 'ping [ ~benedict ] ' '+1 ' 'committed thanks !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>make consistency level user-level auth reads writes configurable reads auth-related tables execute { { local_one } } . 'd like make configurable , default still { { local_one } } . [ link|https : //app.circleci.com/pipelines/github/josh-mckenzie/cassandra/87/workflows/12f26230-abb0-4a67-947b-63f5ff18020c ] '' '+1 ' `` one minor change commit - 'd originally\\xa0performed { { cassandrarolemanager.getallroles ( ) } } call cl.quorum deliberately pr flipped authproperties configured level . reverted cl.quorum javadocced method 's clear future cleared w/benjamin slack.\\r\\n\\r\\ngoing push dtest pr shortly . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anti-compaction briefly corrupts sstable state reads since use multiple sstable rewriters anticompaction , first call preparetocommit remove original sstables tracker view rewriters add sstables . creates brief window reads miss data . sure going dtests though probably need restart ' 'nice catch looks like good fix me.\\r\\n\\r\\n ( +1 ) ' `` blake realised issue patch posted put together alternative patch input [ ~krummas ] .\\r\\n\\r\\n [ 3.0|https : //github.com/belliottsmith/cassandra/tree/15004-3.0 ] [ 3.11|https : //github.com/belliottsmith/cassandra/tree/15004-3.11 ] [ 4.0|https : //github.com/belliottsmith/cassandra/tree/15004-4.0 ] \\r\\n\\r\\nthese patches extract interface { { lifecycletransaction } } no-op relevant calls ( { { preparetocommit } } { { obsoleteoriginals } } ) { { sstablerewriter.preparetocommit } } update tracker - invoked directly rewriter finished preparatory work.\\r\\n\\r\\nit 's bit ugly still finicky probably better/safer invasive surgery point time . '' 'updated unit tests [ 3.0|https : //github.com/krummas/cassandra/tree/15004-3.0 ] [ 3.11|https : //github.com/krummas/cassandra/tree/15004-3.11 ] [ trunk|https : //github.com/krummas/cassandra/tree/15004-trunk ] also adds checks files disk expect ' 'lgtm need comments explaining going comment mentioning { { permitredundanttransitions } } needs removed/updated ' `` thanks . 've pushed branches updated comments . '' '+1 ' 'thanks committed [ 3.0|https : //github.com/apache/cassandra/commit/44785dd2eec5697eec7e496ed3a73d2573f4fe6a ] [ 3.11|https : //github.com/apache/cassandra/commit/9199e591c6148d14f3d12784af8ce5342f118161 ] [ 4.0|https : //github.com/apache/cassandra/commit/df62169d1b6a5bfff2bc678ffbeb0883a3a576b5 ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>index summary redistribution start even compactions paused pause autocompaction upgradesstables/scrub/cleanup etc pause compaction strategies make sure grab sstables , index summary redistribution pause cause us fail operation . [ 3.0|https : //circleci.com/workflow-run/8882a8a6-8593-4d3e-8ec1-05bcab855a44 ] [ 3.11|https : //circleci.com/workflow-run/6b057c7e-1b4a-4f11-9af8-eb3ec2dd8cc9 ] [ trunk|https : //circleci.com/workflow-run/457f8304-c477-45e7-b195-06cf67c22450 ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fix starting paxos auto repair test run ci name ( ending test ) went undetected . fails locally well ( least ) . repaired } } rely running regular/incremental/paxos repair clearing { { system.paxos } } important job auto repair means coordinated paxos repairs finish bit quicker.\\r\\n ' `` given explanation think ticket rc blocker given progress fix two flaky tests . ( still try get 's one two tickets currently waiting on… ) '' `` ticket n't fixing tests starting paxos auto repair done think ready commit ? go rc create tickets flaky tests . '' `` even ticket included running tests erroneously running flaky would entirely weird refuse merge view flaky tests . tests _should running_ failing either way . failing test metric gamed 's encourage good practices . '' `` n't disagree fix naming test runs . n't think block rc though unless time fix . '' `` agree n't block rc . '' 'what got go extra line cassandradeamon renamed test class fixed tests even flaky block rc create new tickets make stable . assign run builds trunk well . ' ' 4.1 j11 pre-commit [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/1573/workflows/84394d1a-13a7-4de3-955c-a0d7cfed2681 ] \\r\\n4.1 j8 pre-commit [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/1561/workflows/c8fae944-d876-4c2a-a8fd-05bb0b8a693b ] \\r\\ntrunk j11 pre-commit [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/1572/workflows/e5b6b663-68c7-46dc-82df-206d907da949 ] \\r\\ntrunk j8 pre-commit [ https : //app.circleci.com/pipelines/github/instaclustr/cassandra/1572/workflows/c324f4ef-c9d1-4456-9f0d-9d4b4e2524c6 ] \\r\\n\\r\\ntrunk pr https : //github.com/apache/cassandra/pull/2007\\r\\n4.1 pr https : //github.com/apache/cassandra/pull/1994\\r\\n ' ' moving `` needs commiter '' afaik needs go 4.1 trunk . nothing 4.0 like . two tests flaky expected . ' ' think [ ~benedict ] may +1 already ? ' 'yep lgtm +1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>in-memory index query path in-memory index using in-memory trie structure introduced cassandra-17240 along query path implementation perform index queries in-memory index . [ https : //app.circleci.com/pipelines/github/adelapena/cassandra/2564/workflows/d8521983-64bf-4447-9af9-612ef2f09327 ] '' `` ci looks good . [ failure|https : //app.circleci.com/pipelines/github/adelapena/cassandra/2564/workflows/5b074e0f-6328-4397-8a2a-53dc58ebd0ff/jobs/25525 ] seems known flaky cassandra-17708 . [ ~maedhroz ] n't anything else add think ready merge . '' 'merged [ feature branch| [ https : //github.com/maedhroz/cassandra/tree/cassandra-16052 ] ] ! ' ' [ ~adelapena ] thanks information ci setup . update cassandra-18062 branch use settings .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>disable deprecated keyspace/table thresholds convert guardrails non-guardrail thresholds 'keyspace_count_warn_threshold ' 'table_count_warn_threshold ' configuration settings first added cassandra-16309 4.0-beta4 subsequently deprecated since 4.1-alpha cassandra-17195 replaced/migrated guardrails part cep-3 ( guardrails ) . thresholds removed cassandra.yaml , still allowed existing yaml files . old thresholds disabled removing default values config.java , existing values thresholds converted new guardrails using ' @ replaces ' tag corresponding guardrail values . since old thresholds considered number system keyspace/tables values , ' @ replaces ' conversion subtract current number system tables old value log descriptive message . see dev list discussion : https : //lists.apache.org/thread/0zjg08hrd6xv7lhvo96frz456b2rvr8b \\xa0 [ https : //github.com/apache/cassandra/pull/2467 ] '' 'the patch looks good . running ci rebasing conflicts : \\r\\n\\r\\n||pr||ci||\\r\\n| [ trunk|https : //github.com/apache/cassandra/pull/2467 ] | [ j8|https : //app.circleci.com/pipelines/github/adelapena/cassandra/3022/workflows/477197d6-3881-4e06-840e-ae442bad521a ] [ j11|https : //app.circleci.com/pipelines/github/adelapena/cassandra/3022/workflows/7b762c95-75c3-45e3-b077-f05e1ad10fad ] | ' `` seems repeated tests failed branch missing [ ninja fix|https : //github.com/apache/cassandra/commit/f038059e89b089a19c36b3be58a443b0586fef5c ] . pass rebasing : \\r\\n\\r\\n||pr||ci||\\r\\n| [ trunk|https : //github.com/apache/cassandra/pull/2467 ] | [ j8|https : //app.circleci.com/pipelines/github/adelapena/cassandra/3025/workflows/a7ada882-3622-4d23-8336-d764484caa53 ] [ j11|https : //app.circleci.com/pipelines/github/adelapena/cassandra/3025/workflows/75bf6211-bee7-4fd4-8548-ced6b0a3c4df ] |\\r\\n\\r\\nnow 'll need approval second committer . '' 'this looks good keeping jmx api stable right call . +1 ' ' [ ~brandon.williams ] thanks review : ) \\r\\n\\r\\ncommitted to\\xa0 { { trunk } } [ aac070681bd01ba796814692c3fba04de103d8b4|https : //github.com/apache/cassandra/commit/aac070681bd01ba796814692c3fba04de103d8b4 ] .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>add shutdownhook flush commitlog replaces periodic_with_flush approach cassandra-1780 / cassandra-1917 could create serversocket address /127.0.0.1:9170.\\n [ junit ] \\tat org.apache.thrift.transport.tserversocket. &lt; init &gt; ( ) \\n [ junit ] \\tat org.apache.thrift.transport.tserversocket. &lt; init &gt; ( ) \\n [ junit ] \\tat org.apache.cassandra.thrift.tcustomserversocket. &lt; init &gt; ( ) \\n [ junit ] \\tat org.apache.cassandra.thrift.cassandradaemon.setup ( ) \\n { noformat } \\n\\nfirst test runs fine second errors ( 'm guessing error message ) first jvm still hanging around third test n't start even long junit timeout . '' 'aha batch cl executor shutdown thread created started . v2 attached.\\n\\nclitest still failing though error along embeddedcassandraservicetest . sure tell junit `` wait previous jvm exit completely starting next test . '' ' `` +1 . \\n\\ni 'm beginning think something introduced ssl patch altered behavior sockets . 've seen odd socket errors twice last days running unit tests think removetest . fwiw n't see errors running tests patch . '' `` bq . 'm beginning think something introduced ssl patch altered behavior sockets\\n\\nnow 'm getting clitest failures w/o patch . think might something . '' `` hmm n't quite right drain similar want ( flushing every cf could take ) . '' 'v3 shuts mutation stage + commitlog hook ' '+1 ' 'committed ' 'integrated cassandra-0.7 # 216 ( see [ https : //hudson.apache.org/hudson/job/cassandra-0.7/216/ ] ) \\n add jvm shutdownhook sync commitlog\\npatch jbellis ; reviewed gdusbabek cassandra-1919\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>remove sstablelock description cleanup happen . sstr enqueued then\\n\\n r = ( filedeletingreference ) finalizerqueue.remove ( ) ; \\n\\nwould generate classcastexception nothing would get cleaned ( server restart ) . '' 'integrated cassandra # 194 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/194/ ] ) \\n replace sstablelock sstabletracker performs updates sstable list atomically\\nwithout readers ever block . ( readers always either see old list new . ) \\nwe avoid race delete old sstable files on-disk using referencequeue : \\nwhen last reference gone phantomreference added queue cleanup.\\nin case cassandra killed compaction cleanup -compacted empty file\\nis written disk ; cassandra removes files thus tagged startup.\\n\\npatch jbellis ; reviewed chris goffinet \\nconvert sstables_ set since filename encapsulated sstr object now\\npatch jbellis ; reviewed chris goffinet \\ncombine addtolist storelocation ; rename addsstable\\npatch jbellis ; reviewed chris goffinet \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>redesign repair messages many people reporting 'repair hang ' something goes wrong . two major causes hang 1 ) validation failure 2 ) streaming failure . currently , failures happen , failed node would respond back repair initiator . goal ticket redesign message flows around repair repair never hang . https : //github.com/yukim/cassandra/commits/5426-3\\n\\nremoved classes kept backward compatibility.\\n\\nbq . one thing i\\ 'm sure seems get error log doesn\\'t error repair session . maybe otherwise fear people won\\'t notice something went wrong.\\nbq . also fail maybe could send error message ( typically exception message ) easier debugging/reporting.\\n\\nthe latest version notifies user throwing exception filled ( repairsession # exception ) error occurred . sending exception back coordinator useful i\\ 'd rather take different approach use tracing cf ( cassandra-5483 ) .\\n\\nbq . also wonder maybe fail-fast policy errors . instance one node fail it\\ 's validation phase maybe might worth failing right away let user re-trigger repair fixed whatever source error rather still differencing/syncing nodes ( admit solutions possible ) .\\n\\ni changed let repair session fail error occurred think better repair option ( something like -k -- keep-going ) keep repair running report failed session/job end . +1 separate ticket.\\n\\nbq . going bit think add 2 messages interrupt validation sync phase . could useful users need stop repair reason also get error validation one node could use interrupt nodes thus fail fast minimizing amount work done uselessly . anyway guess part done follow ticket.\\n\\n+1 separate ticket . also need add way abort streaming interrupt syncing.\\n\\nbq . repairmessagetype gossip proof could wise add `` future '' type say 4 5 `` case '' .\\nbq . really need repairmessageheader ? making repairmessage repairjobdesc repairmessagetype body rather creating yet another class ? \\n\\nfor messages mimicked way o.a.c.transport.messages does.\\n\\nbq . hashcode methods ( differencer nodepair repairjobdesc ... ) i\\ 'd prefer using guava\\ 's objects.hashcode ( ) ( objects.equal ( ) equals ( ) null ) .\\n\\ndone didn\\'t miss anything.\\n\\nbq . would move gossiper/failure registration ars.addtoactivesessions.\\n\\ndone.\\n\\nbq . i\\ 'd remove validator.rangetovalidate inline desc.range.\\n\\ndone.\\n\\nbq . curiosity mean todo comment validator.add ( ) .\\n\\nthat comment ancient version . removed since longer applicable.\\n\\nbq . merkletree.fullrange maybe it\\ 's time add mt serializer rather restoring manually ugly error prone . aslo partitioner let\\ 's maybe mt uses databasedescriptor.getpartitioner ( ) directly rather restoring manually differencer.run ( ) .\\n\\nyup good time finally cleanup merkletree serialization . done.\\n ' `` streamingrepairtask.initiatestreaming ( ) 's block\\n\\n { code } try\\n { \\n ... \\n streamout.transfersstables ( outsession sstables request.ranges operationtype.aes ) ; \\n // request ranges remote node\\n streamin.requestranges ( request.dst desc.keyspace collections.singleton ( cfstore ) request.ranges operationtype.aes ) ; \\n } \\ncatch ( exception e ) ... { code } \\n\\nis value putting streamin.requestranges ( ) separate try block ( immediately ) fail streamout problem ? could potentially make forward progress ( stream streamin ) even streamout fails ? 'll note 1.2 try/catch yuki 's new work changed regard.\\n\\n\\n\\n '' `` [ ~jasobrown ] actually think try catch block redundant . streaming run thread streamingrepairtask exception handled istreamcallback 's onerror method ( empty current 1.2 ) .\\ni 'm trying overhaul streaming api 2.0 ( cassandra-5286 ) fine grained control streaming . '' 'yuki confirms https : //github.com/yukim/cassandra/commits/5426-3 ready review . ' `` alright v3 lgtm +1.\\n\\ni 've committed though 'll note currently repair tends get stuck due cassandra-5699 ( 've checked ok patch cassandra-5699 ) . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>jvm exit jmx fails bind already running cassandra instance , reason try start another one , happens : { noformat } info jna mlockall successful warn jmx enabled receive remote connections . please see cassandra-env.sh info . error error starting local jmx server : java.rmi.server.exportexception : port already use : 7199 ; nested exception : java.net.bindexception : address already use sun.rmi.transport.tcp.tcptransport.listen ( ) ~ [ ] sun.rmi.transport.tcp.tcptransport.exportobject ( ) ~ [ ] sun.rmi.transport.tcp.tcpendpoint.exportobject ( ) ~ [ ] sun.rmi.transport.liveref.exportobject ( ) ~ [ ] sun.rmi.server.unicastserverref.exportobject ( ) ~ [ ] sun.rmi.registry.registryimpl.setup ( ) ~ [ ] sun.rmi.registry.registryimpl. &lt; init &gt; ( ) ~ [ ] java.rmi.registry.locateregistry.createregistry ( ) ~ [ ] org.apache.cassandra.service.cassandradaemon.maybeinitjmx ( ) [ main/ : na ] org.apache.cassandra.service.cassandradaemon.setup ( ) [ main/ : na ] org.apache.cassandra.service.cassandradaemon.activate ( ) [ main/ : na ] org.apache.cassandra.service.cassandradaemon.main ( ) [ main/ : na ] caused : java.net.bindexception : address already use java.net.plainsocketimpl.socketbind ( native method ) ~ [ ] java.net.abstractplainsocketimpl.bind ( ) ~ [ ] java.net.serversocket.bind ( ) ~ [ ] java.net.serversocket. &lt; init &gt; ( ) ~ [ ] javax.net.defaultserversocketfactory.createserversocket ( ) ~ [ ] org.apache.cassandra.utils.rmiserversocketfactoryimpl.createserversocket ( ) ~ [ main/ : na ] sun.rmi.transport.tcp.tcpendpoint.newserversocket ( ) ~ [ ] sun.rmi.transport.tcp.tcptransport.listen ( ) ~ [ ] ... 11 common frames omitted { noformat } however startup continues , ends replaying commitlogs , probably good thing . address already use\\n java.net.plainsocketimpl.socketbind ( native method ) ~ [ ] \\n java.net.abstractplainsocketimpl.bind ( ) ~ [ ] \\n java.net.serversocket.bind ( ) ~ [ ] \\n java.net.serversocket. &lt; init &gt; ( ) ~ [ ] \\n javax.net.defaultserversocketfactory.createserversocket ( ) ~ [ ] \\n org.apache.cassandra.utils.rmiserversocketfactoryimpl.createserversocket ( ) ~ [ main/ : na ] \\n sun.rmi.transport.tcp.tcpendpoint.newserversocket ( ) ~ [ ] \\n sun.rmi.transport.tcp.tcptransport.listen ( ) ~ [ ] \\n sun.rmi.transport.tcp.tcptransport.exportobject ( ) ~ [ ] \\n sun.rmi.transport.tcp.tcpendpoint.exportobject ( ) ~ [ ] \\n sun.rmi.transport.liveref.exportobject ( ) ~ [ ] \\n sun.rmi.server.unicastserverref.exportobject ( ) ~ [ ] \\n sun.rmi.registry.registryimpl.setup ( ) ~ [ ] \\n sun.rmi.registry.registryimpl. &lt; init &gt; ( ) ~ [ ] \\n java.rmi.registry.locateregistry.createregistry ( ) ~ [ ] \\n org.apache.cassandra.utils.jmxserverutils.createjmxserver ( ) ~ [ main/ : na ] \\n org.apache.cassandra.service.cassandradaemon.maybeinitjmx ( ) [ main/ : na ] \\n org.apache.cassandra.service.cassandradaemon.setup ( ) [ main/ : na ] \\n org.apache.cassandra.service.cassandradaemon.activate ( ) [ main/ : na ] \\n org.apache.cassandra.service.cassandradaemon.main ( ) [ main/ : na ] \\n ( trunk ) mshuler @ mana : ~/git/cassandra $ \\n { noformat } ' ' found config change helps env situation : \\n { { -dcassandra.jmx.local.port=7199 } } instead { { -dcom.sun.management.jmxremote.port=7199 } } ' `` { { com.sun.management.jmxremote.port } } property set jvm automatically uses inbuilt agent initialize jmx connector server . 'expected ' way configure jmx server modify { { cassandra-env.sh } } desired port/ssl/auth settings fact setting remote connections actually sets properties . see though passing directly via { { jvm_args } } also worked without error ( bypassing setup { { cassandra-env.sh } } ) c * would interpret presence property indicate jmx server already setup would n't attempt initialize ; 'm afraid overlooked cassandra-10091 . \\n\\nsince cassandra-10091 c * expects always control jmx configuration . { { cassandra-env.sh } } modified { { com.sun.management.jmxremote.port } } never set ( 's replaced { { cassandra.jmx.remote.port } } ) . top cassandra-11540 makes c * setup error face jmx bind errors whereas previously would . \\n\\nbq . found config change helps env situation\\n\\nthat create slightly different behaviour jmx server listen loopback address previous setup would bind { { 0.0.0.0 } } . want still support remote jmx set { { cassandra.jmx.remote.port } } instead . \\n\\ni 've added cassandra-11725 provide clearer message kind misconfiguration occurs.\\n '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>cas may return false still commit insert paxos proposer proposes value/update propose fail , guarantee whether value accepted ultimately . paxos guarantees 'll agree `` '' value ( given round case ) , guarantee proposer agreed upon value know . particular , given proposal least one accepter accepted quorum , value might ( 's guaranteed either ) replayed ( committed ) another proposer . currently , proposer proposes update u rejected , sleep bit retry u. u accepted least one acceptor , proposer b might replay u , succeed commit . retry happens , prepare , check condition , probably find conditions n't apply anymore since u committed already . thus return false , even though u fact committed . unfortunately 'm sure easy way proposer whose propose fails know update prevail eventually . mean acceptable solution see would return user `` n't know '' ( exception instance ) . annoying proposal rejected wo n't extremely rare occurrence , even relatively light contention , returning `` n't know '' often bit unfriendly . attaching v4 version ( equivalent v2 updated comment v3 ) .\\n '' '+1 ' 'committed thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>hsha fails default rpc_max_threads setting hsha server fails 'out heap space ' error rpc_max_threads left default setting ( unlimited ) cassandra.yaml . 'm proposing code change submitted patch comment change cassandra.yaml indicate rpc_max_threads needs changed use hsha . committed . ' `` guaranteed oom ? ca n't check combination provide sensible error instead ooming letting user figure ? '' `` 's pretty much guaranteed oom number handlers per selectorthread based max pool size default integer.max_value . change happened part cassandra-7594.\\n\\ni suppose could check throw value integer.max_value n't going able check every value . also happens node started pretty immediate suggested doc change rather code change . '' 'just fyi - bisected dtest thrift_hsha_test.thrifthshatest.test_6285 failures cassandra-7594 commit increase ccm_max_heap_size 4g ccm nodes would start successfully without heap oom . ' 'setting rpc_max_threads=20 thrift_hsha_test test_6285 appear keep dtest running default ccm heap . ' 'bq . suppose could check throw value integer.max_value aren\\'t going able check every value.\\n\\n '' unlimited '' default value think there\\ 's still quite bit value checking . i\\ 'll put together patch . ' '8116-throw-exc-2.0.txt throws configurationexception hsha used unlimited rpc_max_threads . ' 'is reason default unlimited ? seems like would simple change reasonable default . ' '+1 latest patch far clearer user oom get moment . ' 'thanks committed 8116-throw-exc-2.0.txt 1b332bc1c02786623e2baf773e9f46af9c04f21f . ' `` bq . reason default unlimited ? seems like would simple change reasonable default.\\n\\ni 'm sure rationale default . would mind opening new ticket discuss better default ? 'm sure 's something would want change 2.0 2.1 . '' 'the latest 2.0.x release cassandra using hsha default settings either stalls minutes operation crashes.\\n\\nthis seem like priority `` minor '' . major problem . longer 2.0.11 `` latest '' version bigger problem becomes new users existing users automation high levels trust minor version upgrades.\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>re-apply mv updates commitlog replay node crashes commit log update local memtable update materialized view node replica could lose mv data . really issue rf=1 since replicas likely apply successfully . case fix mv updates always applied even commit log replay ( care re-add mutations commit log ) . [ `` [ patch|https : //github.com/tjake/cassandra/tree/10164 ] \\n [ tests|http : //cassci.datastax.com/job/tjake-10164-testall/1/ ] \\n [ dtests|http : //cassci.datastax.com/job/tjake-10164-dtest/1/ ] \\n\\n * cleaned areas trying catch write timeouts submitmv since n't possible 's done locally actual view updates async.\\n\\n * added logic make mv updates cl replay write commit log ( since always flush cl replay anyway ) \\n\\n * added logic avoid cl updating mutations streamed sstable ( flush transaction complete ) \\n\\n * found/fixed little bug builder would build &gt; 128 rows partition . 'll add test ... '' 'added wide partition builder test ' '+1 ' 'committed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>uses dataoutputbuffer.recycler possible use cases { { dataoutputbuffer.recycler } } , prevents couple ( larger ) allocations . ( provide patch soon ) patch uses recycled { { dataoutputbuffer } } instead allocating new ones.\\nalso introduces { { dataoutputbuffer.asnewbuffer ( ) } } replace { { bytebuffer.wrap ( out.getdata ( ) 0 out.getlength ( ) ) } } .\\n\\n||trunk| [ branch|https : //github.com/apache/cassandra/compare/trunk ... ] | [ testall|http : //cassci.datastax.com/view/dev/view/snazy/job/snazy-11971-more-recycler-trunk-testall/lastsuccessfulbuild/ ] | [ dtest|http : //cassci.datastax.com/view/dev/view/snazy/job/snazy-11971-more-recycler-trunk-dtest/lastsuccessfulbuild/ ] ' '+1 ' 'thanks ! \\ncommitted [ 063e91754b22a28a43efccb0c238c577a6bd0b8a|https : //github.com/apache/cassandra/commit/063e91754b22a28a43efccb0c238c577a6bd0b8a ] [ trunk|https : //github.com/apache/cassandra/tree/trunk ] \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>illegalargumentexception compactiontask ran largepartitionstest.test_11_1g trunk , found test fails due java.lang.illegalargumentexception compaction . exception apparently happens compaction merges large ( &gt; 2gb ) partition . { noformat } debug [ commit-log-allocator ] 2016-09-28 ? : ? - segments reserve ; creating fresh one debug [ commit-log-allocator ] 2016-09-28 ? : ? - segments reserve ; creating fresh one warn [ ] 2016-09-28 ? : ? - writing large partition cql_test_keyspace/table_4:1000000000000000000000 ( 1.004gib ) error [ ] 2016-09-28 ? : ? - fatal exception thread thread [ , main ] java.lang.illegalargumentexception : range : 2234434614 com.google.common.primitives.ints.checkedcast ( ) ~ [ guava-18.0.jar : na ] org.apache.cassandra.db.compaction.compactiontask.runmaythrow ( ) ~ [ main/ : na ] org.apache.cassandra.utils.wrappedrunnable.run ( ) ~ [ main/ : na ] org.apache.cassandra.db.compaction.compactiontask.executeinternal ( ) ~ [ main/ : na ] org.apache.cassandra.db.compaction.abstractcompactiontask.execute ( ) ~ [ main/ : na ] org.apache.cassandra.db.compaction.compactionmanager $ backgroundcompactioncandidate.run ( ) ~ [ main/ : na ] java.util.concurrent.executors $ runnableadapter.call ( ) ~ [ ] java.util.concurrent.futuretask.run ( ) ~ [ ] java.util.concurrent.threadpoolexecutor.runworker ( ) ~ [ ] java.util.concurrent.threadpoolexecutor $ worker.run ( ) [ ] java.lang.thread.run ( ) [ ] debug [ commit-log-allocator ] 2016-09-28 ? : ? - segments reserve ; creating fresh one debug [ commit-log-allocator ] 2016-09-28 ? : ? - segments reserve ; creating fresh one debug [ commit-log-allocator ] 2016-09-28 ? : ? - segments reserve ; creating fresh one debug [ commit-log-allocator ] 2016-09-28 ? : ? - segments reserve ; creating fresh one { noformat } { noformat } java.lang.runtimeexception : java.util.concurrent.executionexception : java.lang.illegalargumentexception : range : 2540348821 org.apache.cassandra.utils.throwables.maybefail ( ) org.apache.cassandra.utils.fbutilities.waitonfutures ( ) org.apache.cassandra.db.compaction.compactionmanager.performmaximal ( ) org.apache.cassandra.db.columnfamilystore.forcemajorcompaction ( ) org.apache.cassandra.db.columnfamilystore.forcemajorcompaction ( ) org.apache.cassandra.cql3.cqltester.compact ( ) org.apache.cassandra.io.sstable.largepartitionstest.lambda $ withpartitionsize $ 2 ( ) org.apache.cassandra.io.sstable.largepartitionstest.measured ( ) org.apache.cassandra.io.sstable.largepartitionstest.withpartitionsize ( ) org.apache.cassandra.io.sstable.largepartitionstest.test_11_1g ( ) sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) sun.reflect.nativemethodaccessorimpl.invoke ( ) sun.reflect.delegatingmethodaccessorimpl.invoke ( ) java.lang.reflect.method.invoke ( ) org.junit.runners.model.frameworkmethod $ 1.runreflectivecall ( ) org.junit.internal.runners.model.reflectivecallable.run ( ) org.junit.runners.model.frameworkmethod.invokeexplosively ( ) org.junit.internal.runners.statements.invokemethod.evaluate ( ) org.junit.internal.runners.statements.runbefores.evaluate ( ) org.junit.internal.runners.statements.runafters.evaluate ( ) org.junit.runners.blockjunit4classrunner.runchild ( ) com.intellij.junit4.junit4testrunnerutil $ ignoreignoredtestjunit4classrunner.runchild ( ) org.junit.runners.blockjunit4classrunner.runchild ( ) org.junit.runners.parentrunner.runchildren ( ) org.junit.runners.parentrunner.access $ 000 ( ) org.junit.runners.parentrunner $ 1.evaluate ( ) org.junit.internal.runners.statements.runbefores.evaluate ( ) org.junit.internal.runners.statements.runafters.evaluate ( ) org.junit.runners.parentrunner.run ( ) org.junit.runner.junitcore.run ( ) com.intellij.junit4.junit4ideatestrunner.startrunnerwithargs ( ) com.intellij.junit4.junit4ideatestrunner.startrunnerwithargs ( ) com.intellij.rt.execution.junit.junitstarter.preparestreamsandstart ( ) com.intellij.rt.execution.junit.junitstarter.main ( ) sun.reflect.nativemethodaccessorimpl.invoke0 ( native method ) sun.reflect.nativemethodaccessorimpl.invoke ( ) sun.reflect.delegatingmethodaccessorimpl.invoke ( ) java.lang.reflect.method.invoke ( ) com.intellij.rt.execution.application.appmain.main ( ) caused : java.util.concurrent.executionexception : java.lang.illegalargumentexception : range : 2540348821 java.util.concurrent.futuretask.report ( ) java.util.concurrent.futuretask.get ( ) org.apache.cassandra.utils.fbutilities.waitonfutures ( ) ... 37 caused : java.lang.illegalargumentexception : range : 2540348821 com.google.common.primitives.ints.checkedcast ( ) org.apache.cassandra.db.compaction.compactiontask.runmaythrow ( ) org.apache.cassandra.utils.wrappedrunnable.run ( ) org.apache.cassandra.db.compaction.compactiontask.executeinternal ( ) org.apache.cassandra.db.compaction.abstractcompactiontask.execute ( ) org.apache.cassandra.db.compaction.compactionmanager $ 10.runmaythrow ( ) org.apache.cassandra.utils.wrappedrunnable.run ( ) java.util.concurrent.executors $ runnableadapter.call ( ) java.util.concurrent.futuretask.run ( ) java.util.concurrent.threadpoolexecutor.runworker ( ) java.util.concurrent.threadpoolexecutor $ worker.run ( ) java.lang.thread.run ( ) { noformat } patch . could please review ? \\n\\nfix illegalargumentexception compactiontask\\nhttps : //github.com/matope/cassandra/commit/d6c40dd3d4d95dba8b9c3f88de1015315e45990d ' 'took patch added fix cleanup compaction . ci triggered.\\n\\n||cassandra-3.x| [ branch|https : //github.com/apache/cassandra/compare/cassandra-3.x ... ] | [ testall|http : //cassci.datastax.com/view/dev/view/snazy/job/snazy-12717-3.x-testall/lastsuccessfulbuild/ ] | [ dtest|http : //cassci.datastax.com/view/dev/view/snazy/job/snazy-12717-3.x-dtest/lastsuccessfulbuild/ ] \\n ' 'thanks patch ! ci looks good.\\n\\ncommitted [ 433dd1c0ab77d296dafcc6c2079aa9445a6c1b2a|https : //github.com/apache/cassandra/commit/433dd1c0ab77d296dafcc6c2079aa9445a6c1b2a ] [ cassandra-3.x|https : //github.com/apache/cassandra/tree/cassandra-3.x ] \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>nodetool cleanup ks replicas remove old data , silently complete user list : https : //lists.apache.org/thread.html/5d49cc6bbc6fd2e5f8b12f2308a3e24212a55afbb441af5cb8cd4167 @ % 3cuser.cassandra.apache.org % 3e multi-dc cluster , keyspaces replicated given dc , 'll unable run cleanup keyspaces dc , [ cleanup code see ranges exit early|https : //github.com/apache/cassandra/blob/4cfaf85/src/java/org/apache/cassandra/db/compaction/compactionmanager.java # l427-l441 ] nodetool cleanup ks replicas remove old data silently complete\\n\\n -- -- \\n ' `` hi [ ~jasonstack ] really appreciate patience time 's taken back . hope review weekend.\\r\\n\\r\\n [ ~krummas ] / [ ~iamaleksey ] - folks think versions ? 2.2 3.0 ? \\r\\n\\r\\n '' `` 'd probably go 3.0+ 2.2 acceptable . '' `` 've rebased patch 'm re-running ci took long review patch.\\r\\n\\r\\ngenerally patches look fine n't understand 're running method twice [ here|https : //github.com/jasonstack/cassandra/commit/b51c46565adf0d765ac6ded831469a2eca2939d8 # diff-ba6d3d8e296151fc283ef11ac4594e62r211 ] ( similar helper ) ? \\r\\n\\r\\ni 'm inclined remove one calls . marking ready-to-commit 'll merge ci finishes.\\r\\n '' `` [ ~jjirsa ] 's mistake 3.11 pr .. thanks fix . '' 'thank much patch patience . committed 3.0 { { 090f418831be4e4dace861fda380ee4ec27cec35 } } merged fixing 3.11 test way.\\r\\n\\r\\n ' 'github user asfgit closed pull request : \\n\\n https : //github.com/apache/cassandra-dtest/pull/1\\n ' 'thanks reviewing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>expose number rpc timeouts individual hosts metric via jmx total number timeouts node . 's better monitoring break total number number timeouts per host node tried connect . \\n * /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/messagingservice.java\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>clean ksmetadata , cfmetadata many conversion methods thrift avro native , potential source bugs . \\n * /cassandra/trunk/test/unit/org/apache/cassandra/config/databasedescriptortest.java\\n * /cassandra/trunk/test/unit/org/apache/cassandra/config/columndefinitiontest.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/config/columndefinition.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/cql/dropindexstatement.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/migration/updatecolumnfamily.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/migration/migration.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/cql/queryprocessor.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/migration/addkeyspace.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/thrift/cassandraserver.java\\n * /cassandra/trunk/test/unit/org/apache/cassandra/thrift/thriftvalidationtest.java\\n * /cassandra/trunk/changes.txt\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/defstable.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/cql/altertablestatement.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/config/ksmetadata.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/migration/updatekeyspace.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/db/migration/addcolumnfamily.java\\n * /cassandra/trunk/test/unit/org/apache/cassandra/config/cfmetadatatest.java\\n * /cassandra/trunk/test/unit/org/apache/cassandra/db/defstest.java\\n * /cassandra/trunk/src/java/org/apache/cassandra/config/cfmetadata.java\\n * /cassandra/trunk/test/unit/org/apache/cassandra/schemaloader.java\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>cfs readstats_ diskreadstats_ missing description [ `` n't also get rid getreaddiskhits mbean + implementation per irc discussion ? '' 'removed diskreadstats.\\n ' 'committed ' 'integrated cassandra # 165 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/165/ ] ) \\n add back read latency stats cfs.getcolumnfamily . patch sammy yu ; reviewed jbellis \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>modify abstractcassandradaemon.initlog4j ( ) allow hotfixing log level cassandra class customer wants bump log level cassandra class , procedure follow : # add class name log level log4j-server.properties revision identified directory . # new directory symbolically linked location cassandra looks directory . however cassandra 's log4j continue watch old location reason abstractcassandradaemon.initlog4j ( ) uses configfilename = new file ( configlocation.touri ( ) ) .getcanonicalpath ( ) ; customer believes change following , allow symlink work properly . configfilename = new file ( configlocation.touri ( ) ) .getpath ( ) ; possible add configuration would invoke change `` true '' would ideal . find another method allow hotfixing log4j changes . [ `` n't see reason allow symlink trivial patch attached . '' '+1 ' 'committed . trunk assumed logback would work .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>duplicate rows returned clause repeated values value repeated within clause repeated rows returned repeats : cqlsh &gt; create table t1 ( c1 text primary key ) ; cqlsh &gt; insert t1 ( c1 ) values ( ' ' ) ; cqlsh &gt; select * t1 ; c1 -- -- cqlsh &gt; select * t1 c1 = ' ' ; c1 -- -- cqlsh &gt; select * t1 c1 ( ' ' ) ; c1 -- -- cqlsh : dslog &gt; select * t1 c1 ( ' ' , ' ' ) ; c1 -- -- [ `` kind intended behavior . best behavior ? n't know though 'm sure matters much practice tbh . order resulting rows following order values ( unless explicit ordering takes precedence course ) kind suggest consider values list rather set perspective 's probably entirely crazy return duplicate results case . particular use prepared marker server expect list set values ( changing would really break users ) . 's easy enough avoid duplication client side n't want duplicates.\\n\\ndo n't get wrong 'm saying returning duplicate case would inferior rather n't see big problem current behavior 'd rather introduce breaking change even small one good reason . '' 'if performing calculation summing results answer would wrong . \\ni suppose arguable know better put duplicates clause bit generating query aggregating selection parameters separate sources.\\nmy background much extensive relational databases postgres ( take preferred example ) would get back one row query terms correctness absolutely would expect . ' `` [ ~slebresne ] closed wo n't fix problem left open ? '' 'irc discussion shows desire fix 3.0 warn 2.1 . ' 'no dba developer familiar sql expect behavior . 100 % find unexpected many conclude bug.\\n\\ni understand cql sql people expect similar semantics . ' 'this behavior apparently already changed trunk { { selectstatement } } refactoring cassandra-7981 . ' 'the patch 2.1 make sure cassandra log warning first time user execute query containing restriction duplicate values primary key.\\n\\nthe patch trunk add unit tests check behavior fix { { changes.txt } } files . ' ' [ ~snazy ] review ? ' 'sure : ) ' '+1 2.1 patch . triggers warning example ticket description.\\n\\nthe trunk patch tests duplicate values partition key.\\nit checks duplicate values clustering key ( even 2.1 return duplicates ck duplicates ) .\\nyou simply add assertions like { { select * % k1 ( ? ? ) k2 = ? } } { { select * % k1 ( ? ? ) k2 ( ? ? ) } } . ' 'adds missing tests . ' '+1\\n\\ncommitted 0c2eaa9 ( 2.1 ) + 732986b ( trunk merge-commit )</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>examine shortening path length post-5202 cassandra-5202 discussion : { quote } give ? could clean redundancy little moving id directory name ? e.g. , ks/cf-uuid/version-generation-component.db 'm worried path length , limited windows . edit : give specific example , ks foo table bar /var/lib/cassandra/flush/foo/bar-2fbb89709a6911e3b7dc4d7d4e3ca4b4/foo-bar-ka-1-data.db 'm proposing /var/lib/cassandra/flush/foo/bar-2fbb89709a6911e3b7dc4d7d4e3ca4b4/ka-1-data.db { quote } feels pretty error prone . keeping keyspace/table name ( sake making easy mix sstables mistake ) limit say 10 characters ( file name ) truncating name necessary ? '' 'bq . limit say 10 characters ( file name ) truncating name necessary ? \\n\\nwe truncate name fit within os path limit adaptively calculation.\\n\\nhow completely omit keyspace name keep columnfamily name adaptively adjust ( truncate ) name ? ' 'this turns bit complex first thought secondary index cfs flushing directory . : ( \\nany ideas ? ' `` 'm inclined say leave alone 2.1. windows users start yelling 3.0 address . lots stuff work meantime . '' `` patch attached unit test.\\ni ended following naming convention/directory structure : \\n\\n { code } \\n/var/lib/cassandra/data/ks/cf-a85fc210cb1011e3a15f9d25721dbb44\\n /.idx\\n ka-1-data.db\\n ... \\n /snapshots\\n /my_snapshot\\n /.idx\\n ka-1-data.db\\n ... \\n ka-1-data.db\\n ... \\n /backups\\n /.idx\\n ka-1-data.db\\n ... \\n ka-1-data.db\\n ... \\n ka-1-data.db\\n ... \\n ks-cf-jb-123-data.db ( older version co exist ) \\n ks-cf.idx-jb-2-data.db\\n { code } \\n\\nhighlight : \\n\\n * keyspace/columnfamily name omitted filename.\\n * 'temporary ' file file name would 'tmp-ka-1-data.db'.\\n * secondary index sstable directory whose name starts '. ' . distinguish snapshots/backup directories.\\n * older version sstable file co-exist along new directory structure . '' `` still fan removing keyspace/table name filename . imo go jonathan 's suggestion leave alone people actually start yelling cause n't remember single user reporting path lengths blocker 'd rather change directory layout ( might break user scripts ) practical evidence 's problem .. '' 'wdyt [ ~joshuamckenzie ] ? likely problem windows users ? ' 'cassandra-4110 limitations schema.java provide us protection there\\ 's really nothing stop users nesting cassandra data 250 characters deep path things blow regardless length limit to.\\n\\non snapshots we\\ 'll using 204 chars worst-case ( 48 ks 48 cf * 2 9 `` snapshots '' 3 slashes ) doesn\\'t leave us lot breathing room path data_file_directories . maybe lowering name_length schema.java would appropriate given cassandra-7136 ? lot users rolling 40+ char ks cf names general much less windows ? ' 'we lot people hit issues first time lowered max name lengths schemas ' `` looks like lowered across board per-platform basis . see file-path limitation linux surprise 's part ecosystem people used windows . '' 'bq . looks like lowered across board per-platform basis.\\n\\nyeah nobody * really * needs keyspace names long ; worth explaining linux snapshot broke moved windows avoid it.\\n\\nbq . snapshots we\\ 'll using 204 chars worst-case ( 48 ks 48 cf * 2 9 `` snapshots '' 3 slashes ) doesn\\'t leave us lot breathing room path data_file_directories.\\n\\nso ... review 3.0 ? it\\ 's ton code . ' `` 3.0 seems like right time frame changes going . n't huge undertaking . '' 'officially tagging [ ~joshuamckenzie ] reviewer ' `` [ ~yukim ] - could get rebase trunk instead 2.1-beta2 ? 's variety file access issues beta2 ( windows ) making testing bit headache . 'll continue testing linux want verify platforms though n't expect platform-specific differences . '' ' [ ~joshuamckenzie ] attaching patch trunk . ' ' w/len 48 cf ks long username ( 30 chars ) windows path length still allows bit 50 chars secondary index name - plenty.\\nfile names look good tmp names snapshots secondary indexes check linux windows.\\n\\n+1 ' 'committed . thanks review . ' `` end removing ks cf name sstable filename right ? \\n\\nbecause breaks least cqlsstablewriter ( precisely abstractsstablesimplewriter ) n't force sstable directory name ks cf breaks code [ here|https : //github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/abstractsstablesimplewriter.java # l66-l88 ] ( precisely { { desc.cfname.equals ( columnfamily ) } } test fails { { desc.cfname } } wrong ) .\\n\\ni suppose could fix code removing test question would n't rally break code say 'm rather uncomfortable fact filename nothing identifying table anymore . feels way easy mistakenly copy sstable different tables directory ending overwriting stuffs n't . also feels like way find ks table name without relying sstable file 's rather fragile imo . maybe record metadata something . '' ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>throw error auto_bootstrap : true bootstrapping node listed seeds obviously condition exists node bootstrap . obvious logs bootstrapping . throwing error would make obvious therefore faster correct . false seed configs . ' 'yes right fix log explicitly info . added back cassandra-746 got undone point . ' 'committed basically thing ticket . circle life complete .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>support wrapped range queries want support scanning keyx keya x &gt; . ( thus alphabet would include x z . ) important allow hadoop scan key ring exactly . add wrapped range support + test ' '+1 looks good . ' 'committed ' 'integrated cassandra # 357 ( see [ http : //hudson.zones.apache.org/hudson/job/cassandra/357/ ] ) \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>support total/recent latency histogram metrics range slices metrics histogram pretty bad non-normal data like latencies ( empirically tested theoretically ) untrustworthy 99th percentile . applications care percentiles statistically accurate version beneficial . adding deprecated methods like latency histograms cassandra-7338 temporarily would help . 2.1 branch . cassandra-5657 solves everything 3.0. committed thanks .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 357
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1ZnB9RbE6s33",
        "outputId": "c39b966c-1760-4d81-d9ac-9fe4b830c015"
      },
      "execution_count": 358,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      AI_No\n",
              "1      AI_No\n",
              "2     AI_Yes\n",
              "3      AI_No\n",
              "4     AI_Yes\n",
              "       ...  \n",
              "21     AI_No\n",
              "22     AI_No\n",
              "23     AI_No\n",
              "24     AI_No\n",
              "25     AI_No\n",
              "Name: Textual_Type, Length: 226, dtype: object"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Textual_Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>AI_Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>AI_No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 358
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skf.split(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ6ZyMMZ6xx0",
        "outputId": "f67cf9cc-3160-4f9d-8e47-200653b97633"
      },
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object _BaseKFold.split at 0x791d4c98b760>"
            ]
          },
          "metadata": {},
          "execution_count": 359
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Running in {device_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvVVVRnw61dC",
        "outputId": "06ecfcc5-5354-416a-bd73-ea2b187eab34"
      },
      "execution_count": 360,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treina e Avalia o Modelo"
      ],
      "metadata": {
        "id": "odsJJzmm-Krd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_my_metrics = list()"
      ],
      "metadata": {
        "id": "UGdlMCekXdmb"
      },
      "execution_count": 361,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    print(f\"Fold {i+1}: Train Size {len(train_index)} | Test Size {len(test_index)}\")\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    train_labels_encoded = [float(label2id[yi]) for yi in y_train]\n",
        "    test_labels_encoded  = [float(label2id[yi]) for yi in y_test]\n",
        "\n",
        "    X_train = [str(i) for i in X_train]\n",
        "    X_test = [str(i) for i in X_test]\n",
        "\n",
        "    unique_labels = set(label for label in y_train)\n",
        "    label2id = {label: id for id, label in enumerate(unique_labels)}\n",
        "    id2label = {id: label for label, id in label2id.items()}\n",
        "\n",
        "    train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=max_length)\n",
        "    test_encodings  = tokenizer(X_test, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "    train_labels_encoded = [float(label2id[yi]) for yi in y_train]\n",
        "    test_labels_encoded  = [float(label2id[yi]) for yi in y_test]\n",
        "\n",
        "    train_dataset = MyDataset(train_encodings, train_labels_encoded)\n",
        "    test_dataset = MyDataset(test_encodings, test_labels_encoded)\n",
        "\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=1).to(device_name)\n",
        "    trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=test_dataset,\n",
        "      compute_metrics=compute_metrics\n",
        "    )\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n",
        "    predicted_results = trainer.predict(test_dataset)\n",
        "    outputs = predicted_results.predictions.flatten().tolist()\n",
        "    probas = [cap_number(x) for x in outputs]\n",
        "    preds = np.array(np.array(probas) > 0.5, dtype=int)\n",
        "\n",
        "    # roc_auc_score(test_labels_encoded, probas)\n",
        "    folds[i] = {}\n",
        "    folds[i]['pre'] = precision_score(test_labels_encoded, preds)\n",
        "    folds[i]['rec'] = recall_score(test_labels_encoded, preds)\n",
        "    folds[i]['acc'] = accuracy_score(test_labels_encoded, preds)\n",
        "    folds[i]['auc'] = roc_auc_score(test_labels_encoded, probas)\n",
        "    folds[i]['f1'] = f1_score(test_labels_encoded, preds)\n",
        "\n",
        "    print(f\"Fold {i+1}=> PRE: {folds[i]['pre']}; REC: {folds[i]['rec']}; ACC: {folds[i]['acc']}; F1S: {folds[i]['f1']}; AUC: {folds[i]['auc']}\")\n",
        "\n",
        "    item_metric = {'Fold':i+1, 'PRE':folds[i]['pre'], 'REC':folds[i]['rec'], 'ACC':folds[i]['acc'], 'F1S':folds[i]['f1'], 'AUC':folds[i]['auc']}\n",
        "    list_my_metrics.append(item_metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "Jwc__lW368wt",
        "outputId": "d872c462-4a09-4bcb-f69c-6b155f558c8e"
      },
      "execution_count": 362,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: Train Size 180 | Test Size 46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36/36 00:37, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1=> PRE: 0.6739130434782609; REC: 1.0; ACC: 0.6739130434782609; F1S: 0.8051948051948052; AUC: 0.7655913978494624\n",
            "Fold 2: Train Size 181 | Test Size 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36/36 00:28, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2=> PRE: 0.6888888888888889; REC: 1.0; ACC: 0.6888888888888889; F1S: 0.8157894736842105; AUC: 0.7373271889400921\n",
            "Fold 3: Train Size 181 | Test Size 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36/36 00:35, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3=> PRE: 0.6666666666666666; REC: 1.0; ACC: 0.6666666666666666; F1S: 0.8; AUC: 0.6599999999999999\n",
            "Fold 4: Train Size 181 | Test Size 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36/36 00:36, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4=> PRE: 0.6818181818181818; REC: 1.0; ACC: 0.6888888888888889; F1S: 0.8108108108108109; AUC: 0.7333333333333334\n",
            "Fold 5: Train Size 181 | Test Size 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36/36 00:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5=> PRE: 0.6590909090909091; REC: 0.9666666666666667; ACC: 0.6444444444444445; F1S: 0.7837837837837838; AUC: 0.6511111111111112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_my_metrics = pd.DataFrame(list_my_metrics)\n",
        "df_my_metrics"
      ],
      "metadata": {
        "id": "1M8jIzX9X6d5",
        "outputId": "0c2c60f9-5cb4-48e2-aef8-ece4ca0027a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Fold       PRE       REC       ACC       F1S       AUC\n",
              "0     1  0.673913  1.000000  0.673913  0.805195  0.765591\n",
              "1     2  0.688889  1.000000  0.688889  0.815789  0.737327\n",
              "2     3  0.666667  1.000000  0.666667  0.800000  0.660000\n",
              "3     4  0.681818  1.000000  0.688889  0.810811  0.733333\n",
              "4     5  0.659091  0.966667  0.644444  0.783784  0.651111"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc23aac8-2279-496c-9961-a5729ca024bd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fold</th>\n",
              "      <th>PRE</th>\n",
              "      <th>REC</th>\n",
              "      <th>ACC</th>\n",
              "      <th>F1S</th>\n",
              "      <th>AUC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.673913</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.673913</td>\n",
              "      <td>0.805195</td>\n",
              "      <td>0.765591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.688889</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.688889</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.737327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.660000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.688889</td>\n",
              "      <td>0.810811</td>\n",
              "      <td>0.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.659091</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.644444</td>\n",
              "      <td>0.783784</td>\n",
              "      <td>0.651111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc23aac8-2279-496c-9961-a5729ca024bd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cc23aac8-2279-496c-9961-a5729ca024bd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cc23aac8-2279-496c-9961-a5729ca024bd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-85233fb1-9737-4ba6-9746-cdcda10a59f7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-85233fb1-9737-4ba6-9746-cdcda10a59f7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-85233fb1-9737-4ba6-9746-cdcda10a59f7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_ca373eb9-2b7c-4395-9306-75d616ad68f1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_my_metrics')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ca373eb9-2b7c-4395-9306-75d616ad68f1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_my_metrics');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_my_metrics",
              "summary": "{\n  \"name\": \"df_my_metrics\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Fold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PRE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.011819906728684682,\n        \"min\": 0.6590909090909091,\n        \"max\": 0.6888888888888889,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.6888888888888889,\n          0.6590909090909091,\n          0.6666666666666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"REC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.014907119849998594,\n        \"min\": 0.9666666666666667,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.9666666666666667,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ACC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.018441202459261705,\n        \"min\": 0.6444444444444445,\n        \"max\": 0.6888888888888889,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.6888888888888889,\n          0.6444444444444445\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1S\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012324695931880209,\n        \"min\": 0.7837837837837838,\n        \"max\": 0.8157894736842105,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.8157894736842105,\n          0.7837837837837838\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AUC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05086288855438333,\n        \"min\": 0.6511111111111112,\n        \"max\": 0.7655913978494624,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.7373271889400921,\n          0.6511111111111112\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 363
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B) Create the Model (based on example from Hugging face DestilBert)"
      ],
      "metadata": {
        "id": "Jf8FUSpaTCwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "task = \"issue-analysis\"\n",
        "MY_HUGGING_FACE_DATASET = \"armandoufpi/cassandraissuesgroundtruth\""
      ],
      "metadata": {
        "id": "eQzOCrhj6OCO"
      },
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained DistilBERT model and tokenizer\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "16FxUwnv6QC0"
      },
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset da minha conta Hugging Fase\n",
        "\n",
        "https://huggingface.co/datasets/armandoufpi/cassandraissuesgroundtruth"
      ],
      "metadata": {
        "id": "KEQ30tEHk3im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset da minha conta Hugging Fase\n",
        "splits = {'train': 'train.jsonl', 'test': 'test.jsonl'}\n",
        "df_treino = pd.read_json(\"hf://datasets/armandoufpi/cassandraissuesgroundtruth/\" + splits[\"train\"])\n",
        "df_teste = pd.read_json(\"hf://datasets/armandoufpi/cassandraissuesgroundtruth/\" + splits[\"test\"])"
      ],
      "metadata": {
        "id": "6tmhgLBvY0xn"
      },
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_treino"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "8Jhpvs-EkvYW",
        "outputId": "c14d7682-3905-4e54-96c2-a6408fea2dc2"
      },
      "execution_count": 367,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           issue_key                                            summary  \\\n",
              "0     CASSANDRA-3489           EncryptionOptions should be instantiated   \n",
              "1    CASSANDRA-16780    Log when writing many tombstones to a partition   \n",
              "2     CASSANDRA-5426                           Redesign repair messages   \n",
              "3     CASSANDRA-5121    system.peers.tokens is empty after node restart   \n",
              "4    CASSANDRA-11944  sstablesInBounds might not actually give all s...   \n",
              "..               ...                                                ...   \n",
              "195  CASSANDRA-18617  Disable the deprecated keyspace/table threshol...   \n",
              "196   CASSANDRA-5244  Compactions don't work while node is bootstrap...   \n",
              "197    CASSANDRA-173                    add getPendingTasks to CFSMBean   \n",
              "198    CASSANDRA-359      CFS readStats_ and diskReadStats_ are missing   \n",
              "199    CASSANDRA-124  NullPointerException in consistency manager af...   \n",
              "\n",
              "      issue_type issue_status issue_priority  \\\n",
              "0            Bug     Resolved            Low   \n",
              "1    Improvement     Resolved         Normal   \n",
              "2    Improvement     Resolved            Low   \n",
              "3            Bug     Resolved            Low   \n",
              "4            Bug     Resolved         Normal   \n",
              "..           ...          ...            ...   \n",
              "195  Improvement     Resolved         Normal   \n",
              "196          Bug     Resolved         Urgent   \n",
              "197  Improvement     Resolved            Low   \n",
              "198          Bug     Resolved         Normal   \n",
              "199          Bug     Resolved         Urgent   \n",
              "\n",
              "                                           description  \\\n",
              "0    As the title says, otherwise you get an NPE wh...   \n",
              "1    Log when writing many tombstones to a partitio...   \n",
              "2    Many people have been reporting 'repair hang' ...   \n",
              "3    Using a 2 nodes fresh cluster (127.0.0.1 & 127...   \n",
              "4    Same problem as with CASSANDRA-11886 - if we t...   \n",
              "..                                                 ...   \n",
              "195  The non-guardrail thresholds 'keyspace_count_w...   \n",
              "196  It seems that there is a race condition in Sto...   \n",
              "197  need to add an atomicint and inc/decr it whene...   \n",
              "198                            There is no description   \n",
              "199  ERROR [CONSISTENCY-MANAGER:2] 2009-04-30 18:22...   \n",
              "\n",
              "                                              comments architectural_impact  \\\n",
              "0    ['There\\'s a bunch of \"if encryption options i...                   NO   \n",
              "1    ['https://github.com/krummas/cassandra/commits...                   NO   \n",
              "2    ['Work in progress is pushed to: https://githu...                  YES   \n",
              "3    ['In StorageService.handleStateNormal, when we...                   NO   \n",
              "4    ['https://github.com/krummas/cassandra/commits...                  YES   \n",
              "..                                                 ...                  ...   \n",
              "195  [\"Part of this change is to add converters tha...                  YES   \n",
              "196  [\"Thanks for the detective work, Jouni.  I'll ...                   NO   \n",
              "197  ['rebased patch as 0001-CASSANDRA-173-added-CF...                   NO   \n",
              "198  [\"shouldn't we also get rid of getReadDiskHits...                   NO   \n",
              "199  [\"Shouldn't ConsistencyManager() constructor c...                   NO   \n",
              "\n",
              "                                         comments_text  label label_text  \n",
              "0    There\\'s a bunch of \"if encryption options is ...      0   negative  \n",
              "1    https://github.com/krummas/cassandra/commits/m...      0   negative  \n",
              "2    https://github.com/yukim/cassandra/commits/542...      1   positive  \n",
              "3    removeEndpoint should be used instead\\n    [ju...      0   negative  \n",
              "4    https://github.com/krummas/cassandra/commits/m...      1   positive  \n",
              "..                                                 ...    ...        ...  \n",
              "195  \\xa0[https://github.com/apache/cassandra/pull/...      1   positive  \n",
              "196  BLOCKED (on object monitor)\\n    at org.apache...      0   negative  \n",
              "197  rebased patch as 0001-CASSANDRA-173-added-CFS-...      0   negative  \n",
              "198  [\"shouldn't we also get rid of getReadDiskHits...      0   negative  \n",
              "199  [\"Shouldn't ConsistencyManager() constructor c...      0   negative  \n",
              "\n",
              "[200 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e931bad-2937-40e7-8451-04b3e7586a23\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_key</th>\n",
              "      <th>summary</th>\n",
              "      <th>issue_type</th>\n",
              "      <th>issue_status</th>\n",
              "      <th>issue_priority</th>\n",
              "      <th>description</th>\n",
              "      <th>comments</th>\n",
              "      <th>architectural_impact</th>\n",
              "      <th>comments_text</th>\n",
              "      <th>label</th>\n",
              "      <th>label_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CASSANDRA-3489</td>\n",
              "      <td>EncryptionOptions should be instantiated</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>As the title says, otherwise you get an NPE wh...</td>\n",
              "      <td>['There\\'s a bunch of \"if encryption options i...</td>\n",
              "      <td>NO</td>\n",
              "      <td>There\\'s a bunch of \"if encryption options is ...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CASSANDRA-16780</td>\n",
              "      <td>Log when writing many tombstones to a partition</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Log when writing many tombstones to a partitio...</td>\n",
              "      <td>['https://github.com/krummas/cassandra/commits...</td>\n",
              "      <td>NO</td>\n",
              "      <td>https://github.com/krummas/cassandra/commits/m...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CASSANDRA-5426</td>\n",
              "      <td>Redesign repair messages</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>Many people have been reporting 'repair hang' ...</td>\n",
              "      <td>['Work in progress is pushed to: https://githu...</td>\n",
              "      <td>YES</td>\n",
              "      <td>https://github.com/yukim/cassandra/commits/542...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CASSANDRA-5121</td>\n",
              "      <td>system.peers.tokens is empty after node restart</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>Using a 2 nodes fresh cluster (127.0.0.1 &amp; 127...</td>\n",
              "      <td>['In StorageService.handleStateNormal, when we...</td>\n",
              "      <td>NO</td>\n",
              "      <td>removeEndpoint should be used instead\\n    [ju...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CASSANDRA-11944</td>\n",
              "      <td>sstablesInBounds might not actually give all s...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Same problem as with CASSANDRA-11886 - if we t...</td>\n",
              "      <td>['https://github.com/krummas/cassandra/commits...</td>\n",
              "      <td>YES</td>\n",
              "      <td>https://github.com/krummas/cassandra/commits/m...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>CASSANDRA-18617</td>\n",
              "      <td>Disable the deprecated keyspace/table threshol...</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>The non-guardrail thresholds 'keyspace_count_w...</td>\n",
              "      <td>[\"Part of this change is to add converters tha...</td>\n",
              "      <td>YES</td>\n",
              "      <td>\\xa0[https://github.com/apache/cassandra/pull/...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>CASSANDRA-5244</td>\n",
              "      <td>Compactions don't work while node is bootstrap...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Urgent</td>\n",
              "      <td>It seems that there is a race condition in Sto...</td>\n",
              "      <td>[\"Thanks for the detective work, Jouni.  I'll ...</td>\n",
              "      <td>NO</td>\n",
              "      <td>BLOCKED (on object monitor)\\n    at org.apache...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>CASSANDRA-173</td>\n",
              "      <td>add getPendingTasks to CFSMBean</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>need to add an atomicint and inc/decr it whene...</td>\n",
              "      <td>['rebased patch as 0001-CASSANDRA-173-added-CF...</td>\n",
              "      <td>NO</td>\n",
              "      <td>rebased patch as 0001-CASSANDRA-173-added-CFS-...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>CASSANDRA-359</td>\n",
              "      <td>CFS readStats_ and diskReadStats_ are missing</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>There is no description</td>\n",
              "      <td>[\"shouldn't we also get rid of getReadDiskHits...</td>\n",
              "      <td>NO</td>\n",
              "      <td>[\"shouldn't we also get rid of getReadDiskHits...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>CASSANDRA-124</td>\n",
              "      <td>NullPointerException in consistency manager af...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Urgent</td>\n",
              "      <td>ERROR [CONSISTENCY-MANAGER:2] 2009-04-30 18:22...</td>\n",
              "      <td>[\"Shouldn't ConsistencyManager() constructor c...</td>\n",
              "      <td>NO</td>\n",
              "      <td>[\"Shouldn't ConsistencyManager() constructor c...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 11 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e931bad-2937-40e7-8451-04b3e7586a23')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0e931bad-2937-40e7-8451-04b3e7586a23 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0e931bad-2937-40e7-8451-04b3e7586a23');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b50c7d76-e3e3-47fe-9f66-e6b46b17213d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b50c7d76-e3e3-47fe-9f66-e6b46b17213d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b50c7d76-e3e3-47fe-9f66-e6b46b17213d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c534ae7f-eaa8-4305-96d8-ac6abf1552a6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_treino')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c534ae7f-eaa8-4305-96d8-ac6abf1552a6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_treino');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_treino",
              "summary": "{\n  \"name\": \"df_treino\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"issue_key\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"CASSANDRA-2950\",\n          \"CASSANDRA-18803\",\n          \"CASSANDRA-17509\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"Data from truncated CF reappears after server restart\",\n          \"Refactor validation logic in StorageService.rebuild\",\n          \"Add Guardrail to disable GROUP BY functionality\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Improvement\",\n          \"Task\",\n          \"New Feature\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Resolved\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_priority\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Low\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 189,\n        \"samples\": [\n          \"This ticket was created from needs discovered in CASSANDRA-17180. We want to be able to configure a startup check so we figured out that it is necessary to treat all startup checks same - to be able to configure them. This ticket is about making startup checks configurable.\\n\\n\\n\\nOnce this ticket is done, we can continue with the implementation of CASSANDRA-17180 where the implementation of gc grace check will be done.\\n\\n\\n\\nWe have identified that there is one check currently in place which needs to be changed to reflect this configuration implementation and that is FileSystemOwnershipCheck.\\n\\n\\n\\nBecause startup checks were not configurable before via means of a configuration file, they were configurable via system properties. This ticket does not aim to get rid system properties configuration mechanism, system properties will have precedence over settings in configuration file. Then, in the next release, I am aiming to get rid of system properties configuration mechanism.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"[\\\"This is a general issue with all CF's. updating bug.\\\", 'The other permutation of this bug looked like, assuming write with CL.Q:\\\\n* Insert 50 (3 nodes up)\\\\n* truncate CF (3 nodes up)\\\\n* Insert 1 (3 nodes up)\\\\n* Bring node3 down\\\\n* Delete 1  (2 nodes up)\\\\n* Bring up node3 and run repair\\\\n* Take down node1 and node2.\\\\n* Query node3 with CL.ONE: list Standard1;  --- 30 rows returned\\\\n\\\\nNot sure, but this looked suspicious in my logs:\\\\n{code}\\\\n INFO 01:19:45,616 Streaming to /50.57.114.45\\\\n INFO 01:19:45,689 Finished streaming session 698609583499991 from /50.57.107.176\\\\n INFO 01:19:45,690 Finished streaming session 698609609994154 from /50.57.114.45\\\\n INFO 01:19:46,501 Finished streaming repair with /50.57.114.45 for (0,56713727820156410577229101238628035242]: 0 oustanding to complete session\\\\n INFO 01:19:46,531 Compacted to /var/lib/cassandra/data/Keyspace1/Standard1-tmp-g-106-Data.db.  16,646,523 to 16,646,352 (~99% of original) bytes for 30 keys.  Time: 1,509ms.\\\\n INFO 01:19:46,930 Finished streaming repair with /50.57.107.176 for (113427455640312821154458202477256070484,0]: 1 oustanding to complete session\\\\n INFO 01:19:47,619 Finished streaming repair with /50.57.114.45 for (113427455640312821154458202477256070484,0]: 0 oustanding to complete session\\\\n INFO 01:19:48,232 Finished streaming repair with /50.57.107.176 for (56713727820156410577229101238628035242,113427455640312821154458202477256070484]: 1 oustanding to complete session\\\\n INFO 01:19:48,856 Finished streaming repair with /50.57.114.45 for (56713727820156410577229101238628035242,113427455640312821154458202477256070484]: 0 oustanding to complete session\\\\n{code}', \\\"Currently, truncate does:\\\\n* force a flush\\\\n* record the time\\\\n* delete any sstables older than the time\\\\n\\\\nThis isn't quite enough if the machine crashes shortly afterward, however, since there can be mutations present in the commitlog that were previously truncated and are now resurrected by CL replay.\\\\n\\\\nOne thing we could do is record the truncate time for the CF in the system ks and then ignore mutations older than that, however this would require time synchronization between the client and the server to be accurate.\\\\n\\\", 'but we record CL \\\"context\\\" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.\\\\n\\\\nchecked and we do wait for flush to complete in truncate.', 'bq. but we record CL \\\"context\\\" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.\\\\n\\\\nI think there\\\\'s something wrong with that, then:\\\\n\\\\n{noformat}\\\\n INFO 21:25:15,274 Replaying /var/lib/cassandra/commitlog/CommitLog-1312924388053.log\\\\nDEBUG 21:25:15,290 Replaying /var/lib/cassandra/commitlog/CommitLog-1312924388053.log starting at 0\\\\nDEBUG 21:25:15,291 Reading mutation at 0\\\\nDEBUG 21:25:15,295 replaying mutation for system.4c: {ColumnFamily(LocationInfo [47656e65726174696f6e:false:4@1312924388140000,])}\\\\nDEBUG 21:25:15,321 Reading mutation at 89\\\\nDEBUG 21:25:15,322 replaying mutation for system.426f6f747374726170: {ColumnFamily(LocationInfo [42:false:1@1312924388203,])}\\\\nDEBUG 21:25:15,322 Reading mutation at 174\\\\nDEBUG 21:25:15,322 replaying mutation for system.4c: {ColumnFamily(LocationInfo [546f6b656e:false:16@1312924388204,])}\\\\nDEBUG 21:25:15,322 Reading mutation at 270\\\\nDEBUG 21:25:15,324 replaying mutation for Keyspace1.3030: {ColumnFamily(Standard1 [C0:false:34@1312924813259,C1:false:34@1312924813260,C2:false:34@1312924813260,C3:false:34@1312924813260,C4:false:34@1312924813260,])}\\\\n{noformat}\\\\n\\\\nThe last entry there is the first of many errant mutations.', 'Ah, CASSANDRA-2419 keeps on giving...\\\\n\\\\nbq. but we record CL \\\"context\\\" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.\\\\n\\\\nThe obvious problem with this is that the point of truncate is to blow away such sstables...  Patch attached.  Comment explains the core fix:\\\\n\\\\n{noformat}\\\\n// Bonus complication: since we store replay position in sstable metadata,\\\\n// truncating those sstables means we will replay any CL segments from the\\\\n// beginning if we restart before they are discarded for normal reasons\\\\n// post-truncate.  So we need to (a) force a new segment so the currently\\\\n// active one can be discarded, and (b) flush *all* CFs so that unflushed\\\\n// data in others don\\\\'t keep any pre-truncate CL segments alive.\\\\n{noformat}\\\\n\\\\nPatch also fixes the bug in ReplayManagerTruncateTest that made it miss this.\\\\n', 'I think the forceFlush of all the CF is not safe, because if for a given column family the memtable is clean, forceFlush will return immediately, even though there could be a memtable being flush at the same time (or pending flush). So we cannot be sure all the old segment are clean after the waitFutures (I know, it took me some time to figure out some problem with repair for this very reason when the repairs were not properly synchronized).\\\\n\\\\nWhat we would need is to add to the future we wait on the futures of all the flush being processed at that time. Sounds annoying though. ', '+1, though this patch is against trunk, not 0.8.  Also mistakenly bumps the log4j level to debug.', 'v2:\\\\n\\\\n{noformat}\\\\n// Bonus bonus: simply forceFlush of all the CF is not enough, because if\\\\n// for a given column family the memtable is clean, forceFlush will return\\\\n// immediately, even though there could be a memtable being flush at the same\\\\n// time.  So to guarantee that all segments can be cleaned out, we need\\\\n// \\\"waitForActiveFlushes\\\" after the new segment has been created.\\\\n{noformat}', \\\"Attaching a v3 that is rebased against 0.8. I've also slightly change the logic in Truncate to submit all the flushes and then call waitForActiveFlushes, as this is slightly simpler and should work equally well as far as I can tell.\\\\nApart from that, this lgtm.\\\", 'committed', 'Integrated in Cassandra-0.8 #272 (See [https://builds.apache.org/job/Cassandra-0.8/272/])\\\\n    make sure truncate clears out the commitlog\\\\npatch by jbellis; reviewed by slebresne for CASSANDRA-2950\\\\n\\\\njbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156763\\\\nFiles : \\\\n* /cassandra/branches/cassandra-0.8/CHANGES.txt\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java\\\\n* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java\\\\n']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"architectural_impact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"YES\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"\\\\n* /cassandra/branches/cassandra-0.8/CHANGES.txt\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java\\\\n* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java\\\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java\\\\n \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 367
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U2F5ZFSykzML",
        "outputId": "9e807a8c-ed9b-45df-ad27-eab2dfef0116"
      },
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          issue_key                                            summary  \\\n",
              "0   CASSANDRA-11944  sstablesInBounds might not actually give all s...   \n",
              "1   CASSANDRA-12988  make the consistency level for user-level auth...   \n",
              "2   CASSANDRA-15004  Anti-compaction briefly corrupts sstable state...   \n",
              "3   CASSANDRA-15265  Index summary redistribution can start even wh...   \n",
              "4   CASSANDRA-18029                     fix starting Paxos auto repair   \n",
              "5   CASSANDRA-18058                     In-memory index and query path   \n",
              "6   CASSANDRA-18617  Disable the deprecated keyspace/table threshol...   \n",
              "7    CASSANDRA-1919                Add shutdownhook to flush commitlog   \n",
              "8     CASSANDRA-414                                 remove sstableLock   \n",
              "9    CASSANDRA-5426                           Redesign repair messages   \n",
              "10  CASSANDRA-11540           The JVM should exit if jmx fails to bind   \n",
              "11   CASSANDRA-6013   CAS may return false but still commit the insert   \n",
              "12   CASSANDRA-8116    HSHA fails with default rpc_max_threads setting   \n",
              "13  CASSANDRA-10164            Re-apply MV updates on commitlog replay   \n",
              "14  CASSANDRA-11971             More uses of DataOutputBuffer.RECYCLER   \n",
              "15  CASSANDRA-12717         IllegalArgumentException in CompactionTask   \n",
              "16  CASSANDRA-13526  nodetool cleanup on KS with no replicas should...   \n",
              "17   CASSANDRA-2941  Expose number of rpc timeouts for individual h...   \n",
              "18   CASSANDRA-3032                    clean up KSMetadata, CFMetadata   \n",
              "19    CASSANDRA-359      CFS readStats_ and diskReadStats_ are missing   \n",
              "20   CASSANDRA-6170  Modify AbstractCassandraDaemon.initLog4j() to ...   \n",
              "21   CASSANDRA-6706  Duplicate rows returned when in clause has rep...   \n",
              "22   CASSANDRA-6962           examine shortening path length post-5202   \n",
              "23   CASSANDRA-6972  Throw an ERROR when auto_bootstrap: true and b...   \n",
              "24    CASSANDRA-758                      support wrapped range queries   \n",
              "25   CASSANDRA-8627  Support Total/Recent latency histogram metrics...   \n",
              "\n",
              "     issue_type issue_status issue_priority  \\\n",
              "0           Bug     Resolved         Normal   \n",
              "1   Improvement     Resolved            Low   \n",
              "2           Bug     Resolved         Urgent   \n",
              "3           Bug     Resolved         Normal   \n",
              "4           Bug     Resolved         Normal   \n",
              "5   New Feature     Resolved         Normal   \n",
              "6   Improvement     Resolved         Normal   \n",
              "7   Improvement     Resolved            Low   \n",
              "8   Improvement     Resolved         Normal   \n",
              "9   Improvement     Resolved            Low   \n",
              "10          Bug     Resolved         Normal   \n",
              "11          Bug     Resolved         Normal   \n",
              "12          Bug     Resolved            Low   \n",
              "13          Bug     Resolved         Normal   \n",
              "14  Improvement     Resolved            Low   \n",
              "15          Bug     Resolved         Normal   \n",
              "16          Bug     Resolved         Normal   \n",
              "17  Improvement     Resolved            Low   \n",
              "18         Task     Resolved            Low   \n",
              "19          Bug     Resolved         Normal   \n",
              "20  New Feature     Resolved         Normal   \n",
              "21          Bug     Resolved         Normal   \n",
              "22  Improvement     Resolved         Normal   \n",
              "23  Improvement     Resolved            Low   \n",
              "24  New Feature     Resolved            Low   \n",
              "25  Improvement     Resolved         Normal   \n",
              "\n",
              "                                          description  \\\n",
              "0   Same problem as with CASSANDRA-11886 - if we t...   \n",
              "1   Most reads for the auth-related tables execute...   \n",
              "2   Since we use multiple sstable rewriters in ant...   \n",
              "3   When we pause autocompaction for upgradesstabl...   \n",
              "4   This test was not run in CI because of its nam...   \n",
              "5   An in-memory index using the in-memory trie st...   \n",
              "6   The non-guardrail thresholds 'keyspace_count_w...   \n",
              "7   this replaces the periodic_with_flush approach...   \n",
              "8                             There is no description   \n",
              "9   Many people have been reporting 'repair hang' ...   \n",
              "10  If you are already running a cassandra instanc...   \n",
              "11  If a Paxos proposer proposes some value/update...   \n",
              "12  The HSHA server fails with 'Out of heap space'...   \n",
              "13  If a node crashes between the Commit log updat...   \n",
              "14  There are a few more possible use cases for {{...   \n",
              "15  When I was ran LargePartitionsTest.test_11_1G ...   \n",
              "16  From the user list:\\n\\nhttps://lists.apache.or...   \n",
              "17  We have a total number timeouts for each node....   \n",
              "18  There are too many conversion methods between ...   \n",
              "19                            There is no description   \n",
              "20  When customer wants to bump up log level of a ...   \n",
              "21  If a value is repeated within an IN clause the...   \n",
              "22  From CASSANDRA-5202 discussion:\\n\\n{quote}\\nDi...   \n",
              "23  Obviously when this condition exists the node ...   \n",
              "24  we want to support scanning from KeyX to KeyA ...   \n",
              "25  The Metrics histogram is pretty bad at non-nor...   \n",
              "\n",
              "                                             comments architectural_impact  \\\n",
              "0   ['https://github.com/krummas/cassandra/commits...                  YES   \n",
              "1   ['Linked patch allows an operator to set the r...                  YES   \n",
              "2   ['|[3.0|https://github.com/bdeggleston/cassand...                  YES   \n",
              "3   ['Patch adds a flag in `CompactionManager` whi...                  YES   \n",
              "4   ['I fixed here what I could: [https://github.c...                  YES   \n",
              "5   ['The github PR for this ticket is here:\\xa0\\r...                  YES   \n",
              "6   [\"Part of this change is to add converters tha...                  YES   \n",
              "7   [\"The approach I took was to add a shutdownBlo...                  YES   \n",
              "8   ['rebased.\\n\\n02\\n    remove sstableLock.  re-...                  YES   \n",
              "9   ['Work in progress is pushed to: https://githu...                  YES   \n",
              "10  [\"We could exit on IOException (it's {{BindExc...                   NO   \n",
              "11  ['bq. if for a given proposal at least one acc...                   NO   \n",
              "12  ['Committed.', \"Is it guaranteed to OOM?  Can'...                   NO   \n",
              "13  [\"[patch|https://github.com/tjake/cassandra/tr...                   NO   \n",
              "14  ['Patch uses recycled {{DataOutputBuffer}}s in...                   NO   \n",
              "15  ['Patch is here. Could you please review this?...                   NO   \n",
              "16  ['The issue I am seeing on C* cluster with the...                   NO   \n",
              "17  ['expose the number of timeouts per host.\\nexp...                   NO   \n",
              "18  ['Standardizes on to{Avro,Thrift} and from{Avr...                   NO   \n",
              "19  [\"shouldn't we also get rid of getReadDiskHits...                   NO   \n",
              "20  [\"Don't see any reason not to allow a symlink ...                   NO   \n",
              "21  [\"That is kind of the intended behavior. Is it...                   NO   \n",
              "22  [\"It's actually not clear to me that the benef...                   NO   \n",
              "23  ['The catch with doing this is, now everyone h...                   NO   \n",
              "24  ['add wrapped range support + test', '+1 Looks...                   NO   \n",
              "25                             ['Committed, thanks.']                   NO   \n",
              "\n",
              "                                        comments_text  label label_text  \n",
              "0   https://github.com/krummas/cassandra/commits/m...      1   positive  \n",
              "1   [Link|https://app.circleci.com/pipelines/githu...      1   positive  \n",
              "2   not sure what is going on with the dtests thou...      1   positive  \n",
              "3   [3.0|https://circleci.com/workflow-run/8882a8a...      1   positive  \n",
              "4   repaired}} rely on running regular/incremental...      1   positive  \n",
              "5   [https://app.circleci.com/pipelines/github/ade...      1   positive  \n",
              "6   \\xa0[https://github.com/apache/cassandra/pull/...      1   positive  \n",
              "7   Could not create ServerSocket on address /127....      1   positive  \n",
              "8   the cleanup does happen.  If it were the SSTR ...      1   positive  \n",
              "9   https://github.com/yukim/cassandra/commits/542...      1   positive  \n",
              "10  Address already in use\\n        at java.net.Pl...      0   negative  \n",
              "11  attaching v4 with that version (which is equiv...      0   negative  \n",
              "12  Committed.' \"Is it guaranteed to OOM?  Can't w...      0   negative  \n",
              "13  [\"[patch|https://github.com/tjake/cassandra/tr...      0   negative  \n",
              "14  Patch uses recycled {{DataOutputBuffer}}s inst...      0   negative  \n",
              "15  Patch is here. Could you please review this?\\n...      0   negative  \n",
              "16  nodetool cleanup on KS with no replicas should...      0   negative  \n",
              "17  \\n* /cassandra/branches/cassandra-0.8/src/java...      0   negative  \n",
              "18  \\n* /cassandra/trunk/test/unit/org/apache/cass...      0   negative  \n",
              "19  [\"shouldn't we also get rid of getReadDiskHits...      0   negative  \n",
              "20  [\"Don't see any reason not to allow a symlink ...      0   negative  \n",
              "21  [\"That is kind of the intended behavior. Is it...      0   negative  \n",
              "22  feels pretty error prone. What about keeping t...      0   negative  \n",
              "23  false in their seed configs.' 'Yes the right f...      0   negative  \n",
              "24  add wrapped range support + test' '+1 Looks go...      0   negative  \n",
              "25                                 Committed thanks.       0   negative  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6aefd4d9-1445-419c-8053-44ed6e7102e2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_key</th>\n",
              "      <th>summary</th>\n",
              "      <th>issue_type</th>\n",
              "      <th>issue_status</th>\n",
              "      <th>issue_priority</th>\n",
              "      <th>description</th>\n",
              "      <th>comments</th>\n",
              "      <th>architectural_impact</th>\n",
              "      <th>comments_text</th>\n",
              "      <th>label</th>\n",
              "      <th>label_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CASSANDRA-11944</td>\n",
              "      <td>sstablesInBounds might not actually give all s...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Same problem as with CASSANDRA-11886 - if we t...</td>\n",
              "      <td>['https://github.com/krummas/cassandra/commits...</td>\n",
              "      <td>YES</td>\n",
              "      <td>https://github.com/krummas/cassandra/commits/m...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CASSANDRA-12988</td>\n",
              "      <td>make the consistency level for user-level auth...</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>Most reads for the auth-related tables execute...</td>\n",
              "      <td>['Linked patch allows an operator to set the r...</td>\n",
              "      <td>YES</td>\n",
              "      <td>[Link|https://app.circleci.com/pipelines/githu...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CASSANDRA-15004</td>\n",
              "      <td>Anti-compaction briefly corrupts sstable state...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Urgent</td>\n",
              "      <td>Since we use multiple sstable rewriters in ant...</td>\n",
              "      <td>['|[3.0|https://github.com/bdeggleston/cassand...</td>\n",
              "      <td>YES</td>\n",
              "      <td>not sure what is going on with the dtests thou...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CASSANDRA-15265</td>\n",
              "      <td>Index summary redistribution can start even wh...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>When we pause autocompaction for upgradesstabl...</td>\n",
              "      <td>['Patch adds a flag in `CompactionManager` whi...</td>\n",
              "      <td>YES</td>\n",
              "      <td>[3.0|https://circleci.com/workflow-run/8882a8a...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CASSANDRA-18029</td>\n",
              "      <td>fix starting Paxos auto repair</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>This test was not run in CI because of its nam...</td>\n",
              "      <td>['I fixed here what I could: [https://github.c...</td>\n",
              "      <td>YES</td>\n",
              "      <td>repaired}} rely on running regular/incremental...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>CASSANDRA-18058</td>\n",
              "      <td>In-memory index and query path</td>\n",
              "      <td>New Feature</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>An in-memory index using the in-memory trie st...</td>\n",
              "      <td>['The github PR for this ticket is here:\\xa0\\r...</td>\n",
              "      <td>YES</td>\n",
              "      <td>[https://app.circleci.com/pipelines/github/ade...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>CASSANDRA-18617</td>\n",
              "      <td>Disable the deprecated keyspace/table threshol...</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>The non-guardrail thresholds 'keyspace_count_w...</td>\n",
              "      <td>[\"Part of this change is to add converters tha...</td>\n",
              "      <td>YES</td>\n",
              "      <td>\\xa0[https://github.com/apache/cassandra/pull/...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>CASSANDRA-1919</td>\n",
              "      <td>Add shutdownhook to flush commitlog</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>this replaces the periodic_with_flush approach...</td>\n",
              "      <td>[\"The approach I took was to add a shutdownBlo...</td>\n",
              "      <td>YES</td>\n",
              "      <td>Could not create ServerSocket on address /127....</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>CASSANDRA-414</td>\n",
              "      <td>remove sstableLock</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>There is no description</td>\n",
              "      <td>['rebased.\\n\\n02\\n    remove sstableLock.  re-...</td>\n",
              "      <td>YES</td>\n",
              "      <td>the cleanup does happen.  If it were the SSTR ...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>CASSANDRA-5426</td>\n",
              "      <td>Redesign repair messages</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>Many people have been reporting 'repair hang' ...</td>\n",
              "      <td>['Work in progress is pushed to: https://githu...</td>\n",
              "      <td>YES</td>\n",
              "      <td>https://github.com/yukim/cassandra/commits/542...</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>CASSANDRA-11540</td>\n",
              "      <td>The JVM should exit if jmx fails to bind</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>If you are already running a cassandra instanc...</td>\n",
              "      <td>[\"We could exit on IOException (it's {{BindExc...</td>\n",
              "      <td>NO</td>\n",
              "      <td>Address already in use\\n        at java.net.Pl...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>CASSANDRA-6013</td>\n",
              "      <td>CAS may return false but still commit the insert</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>If a Paxos proposer proposes some value/update...</td>\n",
              "      <td>['bq. if for a given proposal at least one acc...</td>\n",
              "      <td>NO</td>\n",
              "      <td>attaching v4 with that version (which is equiv...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>CASSANDRA-8116</td>\n",
              "      <td>HSHA fails with default rpc_max_threads setting</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>The HSHA server fails with 'Out of heap space'...</td>\n",
              "      <td>['Committed.', \"Is it guaranteed to OOM?  Can'...</td>\n",
              "      <td>NO</td>\n",
              "      <td>Committed.' \"Is it guaranteed to OOM?  Can't w...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>CASSANDRA-10164</td>\n",
              "      <td>Re-apply MV updates on commitlog replay</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>If a node crashes between the Commit log updat...</td>\n",
              "      <td>[\"[patch|https://github.com/tjake/cassandra/tr...</td>\n",
              "      <td>NO</td>\n",
              "      <td>[\"[patch|https://github.com/tjake/cassandra/tr...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>CASSANDRA-11971</td>\n",
              "      <td>More uses of DataOutputBuffer.RECYCLER</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>There are a few more possible use cases for {{...</td>\n",
              "      <td>['Patch uses recycled {{DataOutputBuffer}}s in...</td>\n",
              "      <td>NO</td>\n",
              "      <td>Patch uses recycled {{DataOutputBuffer}}s inst...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>CASSANDRA-12717</td>\n",
              "      <td>IllegalArgumentException in CompactionTask</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>When I was ran LargePartitionsTest.test_11_1G ...</td>\n",
              "      <td>['Patch is here. Could you please review this?...</td>\n",
              "      <td>NO</td>\n",
              "      <td>Patch is here. Could you please review this?\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>CASSANDRA-13526</td>\n",
              "      <td>nodetool cleanup on KS with no replicas should...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>From the user list:\\n\\nhttps://lists.apache.or...</td>\n",
              "      <td>['The issue I am seeing on C* cluster with the...</td>\n",
              "      <td>NO</td>\n",
              "      <td>nodetool cleanup on KS with no replicas should...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>CASSANDRA-2941</td>\n",
              "      <td>Expose number of rpc timeouts for individual h...</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>We have a total number timeouts for each node....</td>\n",
              "      <td>['expose the number of timeouts per host.\\nexp...</td>\n",
              "      <td>NO</td>\n",
              "      <td>\\n* /cassandra/branches/cassandra-0.8/src/java...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>CASSANDRA-3032</td>\n",
              "      <td>clean up KSMetadata, CFMetadata</td>\n",
              "      <td>Task</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>There are too many conversion methods between ...</td>\n",
              "      <td>['Standardizes on to{Avro,Thrift} and from{Avr...</td>\n",
              "      <td>NO</td>\n",
              "      <td>\\n* /cassandra/trunk/test/unit/org/apache/cass...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>CASSANDRA-359</td>\n",
              "      <td>CFS readStats_ and diskReadStats_ are missing</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>There is no description</td>\n",
              "      <td>[\"shouldn't we also get rid of getReadDiskHits...</td>\n",
              "      <td>NO</td>\n",
              "      <td>[\"shouldn't we also get rid of getReadDiskHits...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>CASSANDRA-6170</td>\n",
              "      <td>Modify AbstractCassandraDaemon.initLog4j() to ...</td>\n",
              "      <td>New Feature</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>When customer wants to bump up log level of a ...</td>\n",
              "      <td>[\"Don't see any reason not to allow a symlink ...</td>\n",
              "      <td>NO</td>\n",
              "      <td>[\"Don't see any reason not to allow a symlink ...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>CASSANDRA-6706</td>\n",
              "      <td>Duplicate rows returned when in clause has rep...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>If a value is repeated within an IN clause the...</td>\n",
              "      <td>[\"That is kind of the intended behavior. Is it...</td>\n",
              "      <td>NO</td>\n",
              "      <td>[\"That is kind of the intended behavior. Is it...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>CASSANDRA-6962</td>\n",
              "      <td>examine shortening path length post-5202</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>From CASSANDRA-5202 discussion:\\n\\n{quote}\\nDi...</td>\n",
              "      <td>[\"It's actually not clear to me that the benef...</td>\n",
              "      <td>NO</td>\n",
              "      <td>feels pretty error prone. What about keeping t...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>CASSANDRA-6972</td>\n",
              "      <td>Throw an ERROR when auto_bootstrap: true and b...</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>Obviously when this condition exists the node ...</td>\n",
              "      <td>['The catch with doing this is, now everyone h...</td>\n",
              "      <td>NO</td>\n",
              "      <td>false in their seed configs.' 'Yes the right f...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>CASSANDRA-758</td>\n",
              "      <td>support wrapped range queries</td>\n",
              "      <td>New Feature</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Low</td>\n",
              "      <td>we want to support scanning from KeyX to KeyA ...</td>\n",
              "      <td>['add wrapped range support + test', '+1 Looks...</td>\n",
              "      <td>NO</td>\n",
              "      <td>add wrapped range support + test' '+1 Looks go...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>CASSANDRA-8627</td>\n",
              "      <td>Support Total/Recent latency histogram metrics...</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>The Metrics histogram is pretty bad at non-nor...</td>\n",
              "      <td>['Committed, thanks.']</td>\n",
              "      <td>NO</td>\n",
              "      <td>Committed thanks.</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6aefd4d9-1445-419c-8053-44ed6e7102e2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6aefd4d9-1445-419c-8053-44ed6e7102e2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6aefd4d9-1445-419c-8053-44ed6e7102e2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6c8169aa-1a65-4ff8-80fa-1e5f42415b88\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6c8169aa-1a65-4ff8-80fa-1e5f42415b88')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6c8169aa-1a65-4ff8-80fa-1e5f42415b88 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d732bcd0-0e8d-4887-a3f8-77a99cc8ac52\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_teste')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d732bcd0-0e8d-4887-a3f8-77a99cc8ac52 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_teste');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_teste",
              "summary": "{\n  \"name\": \"df_teste\",\n  \"rows\": 26,\n  \"fields\": [\n    {\n      \"column\": \"issue_key\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"CASSANDRA-414\",\n          \"CASSANDRA-13526\",\n          \"CASSANDRA-11944\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"remove sstableLock\",\n          \"nodetool cleanup on KS with no replicas should remove old data, not silently complete\",\n          \"sstablesInBounds might not actually give all sstables within the bounds due to having start positions moved in sstables\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Improvement\",\n          \"Task\",\n          \"Bug\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Resolved\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_priority\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Normal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"There is no description\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"['rebased.\\\\n\\\\n02\\\\n    remove sstableLock.  re-order a few ops so that we can never \\\"lose\\\" data temporarily -- always remove old sstable _after_ adding the new ones.  so at worst a few read ops will merge data from an sstable that is obsolete -- this is ok and better than Stop The World locking\\\\n\\\\n01\\\\n    combine addToList and storeLocation; rename to addSSTable\\\\n', \\\"We need to add a comment to CFS.snapshot, indicating that it's a fuzzy one, instead of point-at-time one. Also, the following unit tests failed on me.\\\\n\\\\n[junit] Testcase: testNameSort10(org.apache.cassandra.db.NameSortTest):     Caused an ERROR\\\\n[junit] /home/junrao/local/cassandra_test/build/test/cassandra/data/Keyspace1/Super1-50-Data.db (No such file or directory)\\\\n[junit] java.io.FileNotFoundException: /home/junrao/local/cassandra_test/build/test/cassandra/data/Keyspace1/Super1-50-Data.db (No such file or directory)\\\\n[junit]     at java.io.RandomAccessFile.open(Native Method)\\\\n[junit]     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)\\\\n[junit]     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)\\\\n[junit]     at org.apache.cassandra.io.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)\\\\n[junit]     at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.<init>(SSTableSliceIterator.java:110)\\\\n[junit]     at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:56)\\\\n[junit]     at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:64)\\\\n[junit]     at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1347)\\\\n[junit]     at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1283)\\\\n[junit]     at org.apache.cassandra.db.Table.get(Table.java:564)\\\\n[junit]     at org.apache.cassandra.db.NameSortTest.validateNameSort(NameSortTest.java:113)\\\\n[junit]     at org.apache.cassandra.db.NameSortTest.testNameSort(NameSortTest.java:94)\\\\n[junit]     at org.apache.cassandra.db.NameSortTest.testNameSort10(NameSortTest.java:48)\\\\n[junit]\\\\n[junit]\\\\n[junit] Testcase: testNameSort100(org.apache.cassandra.db.NameSortTest):    Caused an ERROR\\\\n[junit] /home/junrao/local/cassandra_test/build/test/cassandra/data/Keyspace1/Super1-582-Data.db (No such file or directory)\\\\n[junit] java.io.FileNotFoundException: /home/junrao/local/cassandra_test/build/test/cassandra/data/Keyspace1/Super1-582-Data.db (No such file or directory\\\\n[junit]     at java.io.RandomAccessFile.open(Native Method)\\\\n[junit]     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)\\\\n[junit]     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)\\\\n[junit]     at org.apache.cassandra.io.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)\\\\n[junit]     at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.<init>(SSTableSliceIterator.java:110)\\\\n[junit]     at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:56)\\\\n[junit]     at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:64)\\\\n[junit]     at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1347)\\\\n[junit]     at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1283)\\\\n[junit]     at org.apache.cassandra.db.Table.get(Table.java:564)\\\\n[junit]     at org.apache.cassandra.db.NameSortTest.validateNameSort(NameSortTest.java:113)\\\\n[junit]     at org.apache.cassandra.db.NameSortTest.testNameSort(NameSortTest.java:94)\\\\n[junit]     at org.apache.cassandra.db.NameSortTest.testNameSort100(NameSortTest.java:55)\\\\n[junit]\\\", 'when compaction completes it modifies CFS.sstables_ as follows:\\\\n\\\\n    - sstables_.put(new compacted sstable)\\\\n    - remove the source sstables & delete their file on disk\\\\n\\\\nnow, NonBlockingHashMap guarantees that \\\"Iterators and Enumerations return elements reflecting the state of the hash table at some point at or since the creation of the iterator/enumeration.\\\"  BUT this has no effect on other parts of the system, particularly the delete!\\\\n\\\\nso the fundamental problem exhibeted by the unit tests is this:\\\\n\\\\n    - thread A starts iterating over sstables_\\\\n    - compaction thread finishes and does its thing.  now some of the files A needs to see a consistent view of the data are gone: the set of sstables being iterated over was in fact consistent but because we\\\\'re violating encapsulation by doing the delete and remove separately, we can get incorrect results.\\\\n\\\\nI think the simplest solution is to use pseudo-finalizers to do the actual file delete once no references to the owning SSTableReader exist anymore.  These can be done using http://java.sun.com/javase/6/docs/api/java/lang/ref/ReferenceQueue.html and http://java.sun.com/javase/6/docs/api/java/lang/ref/PhantomReference.html.\\\\n', 'Keeping references to SSTableReader may not be enough. The iterator could iterate file A. Then just before an SSTableReader is opened on A, A is removed from sstables and deleted on disk. Now, you can get the same \\\"file not found\\\" exception as above.', 'The iterator will either have a reference to A, or not.  If it does, then A will not be deleted until the iterator is done since the delete is no longer done by compaction but by the referencequeue.  If it does not, it will have a reference to the new sstable instead.', \\\"The question is when a reference is added to the referencequeue. If it's at SSTableReader open time, it may be too late. Between file A is iterated and SSTableReader is opened on A, a compaction can remove A, assuming no one else is reading A at that time.\\\", 'No, it has nothing to do with whether someone is actually reading (has a BufferedRAF open) on A, only whether any reference to the SSTR object itself exists.  Which is no earlier than when the last iterator that might open a BRAF is done.', 'Remember, SSTR objects are not transitory -- we only have one such object for each file on disk ever.  Even the compaction code uses SSTR.get to use a reference to the open object, and does not open new ones (except for the newly compacted files of course, which is in turn the only time open is called on those).', 'I checked the source for NBHM and couldn\\\\'t find any evidence that my literal reading of the iterator contract (\\\"Iterators and Enumerations return elements reflecting the state of the hash table at some point at or since the creation of the iterator/enumeration\\\") was correct.  So I asked the author for clarification here: https://sourceforge.net/forum/message.php?msg_id=7611241.\\\\n\\\\nHe replied, and sure enough, Jun\\\\'s suspicions were correct and even if the compaction thread is careful to add the new SSTR before removing the old ones from sstables_, iterator threads may see the absence of the latter but not the presence of the former.\\\\n\\\\nSo I think that this approach is not going to work.  But I think we can still cut the lock penalty dramatically from what it is now.  I should have some code for that approach Monday.', 'Integrated in Cassandra #191 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/191/])\\\\n    Revert \\\"remove sstableLock.  re-order a few ops so that we can never \\\"lose\\\" data temporarily -- always remove old sstable references _after_ adding the new ones.  so at worst a few read ops will merge data from an sstable that is obsolete -- this is ok and better than Stop The World locking\\\"\\\\nand \\\" combine addToList and storeLocation; rename to addSSTable\\\"\\\\n\\\\nThese were works in progress (and broken); accidentally committed w/ the 418 fix.\\\\n combine addToList and storeLocation; rename to addSSTable\\\\n', '03\\\\n    Replace sstableLock with SSTableTracker, which performs updates to the sstable list atomically\\\\n    without readers ever having to block.  (Readers will always either see the old list, or the new.)\\\\n    We avoid a race on the delete of the old SSTable files on-disk by using a ReferenceQueue:\\\\n    when the last reference is gone, a PhantomReference is added to the queue and can do cleanup.\\\\n    In case Cassandra is killed between compaction and this cleanup, a -Compacted empty file\\\\n    is written to disk; Cassandra removes any files thus tagged on startup.\\\\n\\\\n02\\\\n    convert ssTables_ to a Set, since the filename is encapsulated in the SSTR object now\\\\n\\\\n01\\\\n    CASSANDRA-414 combine addToList and storeLocation; rename to addSSTable\\\\n\\\\nready for review.\\\\n\\\\nthere is a good summary of how PhantomReference and ReferenceQueue work here: http://www.kdgregory.com/index.php?page=java.refobj', '+1', 'committed', \\\"Maybe I don''t understand how PhantomReference works, but the code in SSTableReader dealing with finalizerQueue doesn't look right to me. What gets enqueued in finalizerQueue is SSTR. It doesn't seem like that you can cast it directly to FileDeletingReference. It seems to me that you have to maintain a map btw SSTR and FileDeletingReference. Every time you dequeue an item from finalizerQueue, you can lookup the map to find the corresponding FileDeletingReference.\\\", \\\"Yeah, it's a little subtle.  The article I linked is a good explanation, the javadoc alone isn't sufficient or at least wasn't for me.\\\\n\\\\nA Reference of any type has a get() method that returns the actual referent.  Here that would be the SSTR.  But ReferenceQueue holds the Reference wrapper, not the actual SSTRs.  (When you pass a RQ to the Reference constructor, the Reference will be enqueued on that RQ when its referent is GC'd.  The referent itself already GC'd or in the process of being GC'd so it can't be put on the RQ or you would get back to the Bad Old Days of finalizer resurrection bugs.)\\\\n\\\\nNow, if the referent is no longer live, get() will return null.  Since the point of the RQ design is to do cleanup after the object is dead, we subclass PhantomReference and store a reference to the path, so we don't actually need the SSTR to do the delete.  (In fact for PR in particular get() _always_ returns null but that is not really essential to understanding what is going on here.)\\\", \\\"It's easy enough to check the logs with e.g. stress.py: the cleanup does happen.  If it were the SSTR being enqueued then\\\\n\\\\n                        r = (FileDeletingReference) finalizerQueue.remove();\\\\n\\\\nwould generate ClassCastException and nothing would get cleaned up (until server restart).\\\", 'Integrated in Cassandra #194 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/194/])\\\\n    Replace sstableLock with SSTableTracker, which performs updates to the sstable list atomically\\\\nwithout readers ever having to block.  (Readers will always either see the old list, or the new.)\\\\nWe avoid a race on the delete of the old SSTable files on-disk by using a ReferenceQueue:\\\\nwhen the last reference is gone, a PhantomReference is added to the queue and can do cleanup.\\\\nIn case Cassandra is killed between compaction and this cleanup, a -Compacted empty file\\\\nis written to disk; Cassandra removes any files thus tagged on startup.\\\\n\\\\npatch by jbellis; reviewed by Chris Goffinet for \\\\nconvert ssTables_ to a Set, since the filename is encapsulated in the SSTR object now\\\\npatch by jbellis; reviewed by Chris Goffinet for \\\\ncombine addToList and storeLocation; rename to addSSTable\\\\npatch by jbellis; reviewed by Chris Goffinet for \\\\n']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"architectural_impact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"NO\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"the cleanup does happen.  If it were the SSTR being enqueued then\\\\n\\\\n                        r = (FileDeletingReference) finalizerQueue.remove();\\\\n\\\\nwould generate ClassCastException and nothing would get cleaned up (until server restart).\\\" 'Integrated in Cassandra #194 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/194/])\\\\n    Replace sstableLock with SSTableTracker which performs updates to the sstable list atomically\\\\nwithout readers ever having to block.  (Readers will always either see the old list or the new.)\\\\nWe avoid a race on the delete of the old SSTable files on-disk by using a ReferenceQueue:\\\\nwhen the last reference is gone a PhantomReference is added to the queue and can do cleanup.\\\\nIn case Cassandra is killed between compaction and this cleanup a -Compacted empty file\\\\nis written to disk; Cassandra removes any files thus tagged on startup.\\\\n\\\\npatch by jbellis; reviewed by Chris Goffinet for \\\\nconvert ssTables_ to a Set since the filename is encapsulated in the SSTR object now\\\\npatch by jbellis; reviewed by Chris Goffinet for \\\\ncombine addToList and storeLocation; rename to addSSTable\\\\npatch by jbellis; reviewed by Chris Goffinet for \\\\n \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 368
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carrega os dados de treino e teste"
      ],
      "metadata": {
        "id": "p63bUK_dlLIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from armandoufpi hugging face\n",
        "train_data = load_dataset(MY_HUGGING_FACE_DATASET, split=\"train\")\n",
        "test_data = load_dataset(MY_HUGGING_FACE_DATASET, split=\"test\")"
      ],
      "metadata": {
        "id": "oGH3j90u6URW"
      },
      "execution_count": 369,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "FUUw5h_yVeAk",
        "outputId": "0adb6c6c-876f-4785-a241-2a43054f2f6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['summary', 'architectural_impact', 'comments_text', 'label_text', 'comments', 'issue_status', 'description', 'issue_priority', 'issue_type', 'issue_key', 'label'],\n",
              "    num_rows: 200\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 370
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "Ji3Z_ltlcnvf",
        "outputId": "49a3d869-f5ca-41ad-ef09-e86d4e35c901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['summary', 'architectural_impact', 'comments_text', 'label_text', 'comments', 'issue_status', 'description', 'issue_priority', 'issue_type', 'issue_key', 'label'],\n",
              "    num_rows: 26\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 371
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"len(train_data['summary']): {len(train_data['summary'])}\")\n",
        "print(f\"train_data['summary'][0]: {train_data['summary'][0]}\")\n",
        "print(f\"train_data['label'][0]: {train_data['label'][0]}\")\n",
        "print(f\"train_data['label_text'][0]: {train_data['label_text'][0]}\")\n",
        "print(f\"train_data['description'][0]: {train_data['description'][0]}\")"
      ],
      "metadata": {
        "id": "RjDQZ5npVj1N",
        "outputId": "9ddaa798-4fc0-4fd2-8a7d-15f0ef7c836f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_data['summary']): 200\n",
            "train_data['summary'][0]: EncryptionOptions should be instantiated\n",
            "train_data['label'][0]: 0\n",
            "train_data['label_text'][0]: negative\n",
            "train_data['description'][0]: As the title says, otherwise you get an NPE when the options are missing from the yaml.  It's included in my second patch on CASSANDRA-3045 and is a one line fix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processa os dados de treino e teste"
      ],
      "metadata": {
        "id": "5Fbg-kAzlWa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text data\n",
        "def preprocess_function_description(examples):\n",
        "  return tokenizer(examples[\"description\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Function to preprocess text data\n",
        "def preprocess_function(examples):\n",
        "  return tokenizer(examples[\"summary\"], padding=\"max_length\", truncation=True)"
      ],
      "metadata": {
        "id": "j2DVCRAnUjkU"
      },
      "execution_count": 373,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess train and test data\n",
        "train_data = train_data.map(preprocess_function, batched=True)\n",
        "test_data = test_data.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "kCCJSxv9xJ7O"
      },
      "execution_count": 374,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the 'input_ids' from the preprocessed data\n",
        "#train_inputs = train_data[\"input_ids\"]\n",
        "#test_inputs = test_data[\"input_ids\"]"
      ],
      "metadata": {
        "id": "8CNOUEMmUBrs"
      },
      "execution_count": 375,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treina o modelo"
      ],
      "metadata": {
        "id": "DKtvSqAdlg44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf results\n",
        "!mkdir results\n",
        "!ls -l"
      ],
      "metadata": {
        "id": "xHJzOmJnWGMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7240eac-6e02-4fc2-e1f0-f239d8e2f85e"
      },
      "execution_count": 376,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12\n",
            "drwxr-xr-x 2 root root 4096 Jul 30 20:58 logs\n",
            "drwxr-xr-x 2 root root 4096 Jul 30 20:58 results\n",
            "drwxr-xr-x 1 root root 4096 Jul 29 13:22 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"results\",  # Fixed typo (removed extra space)\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,  # Assuming you meant \"size\" here\n",
        "    learning_rate=2e-5,\n",
        ")"
      ],
      "metadata": {
        "id": "ym4R8x0ZPvLc"
      },
      "execution_count": 377,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create trainer object\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    compute_metrics=\"accuracy\",\n",
        ")"
      ],
      "metadata": {
        "id": "_yp92-DbP7zd"
      },
      "execution_count": 378,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "WcbAwInvP-Qn",
        "outputId": "a6d11c50-cd98-4612-fd88-b81d097c29c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "execution_count": 379,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [39/39 00:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=39, training_loss=0.819644047663762, metrics={'train_runtime': 32.3881, 'train_samples_per_second': 18.525, 'train_steps_per_second': 1.204, 'total_flos': 79480439193600.0, 'train_loss': 0.819644047663762, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 379
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faz as previsões baseadas no modelo treinado"
      ],
      "metadata": {
        "id": "hXtVqRQ1lj9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: fazer a analise do issue baseado em varios fields ao mesmo tempo\n",
        "def analyse_issue(issue_field):\n",
        "  inputs = tokenizer(issue_field, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  # Move the model and input to GPU if available\n",
        "  if torch.cuda.is_available():\n",
        "    model.to('cuda')\n",
        "    inputs.to('cuda')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "  # Print the predicted sentiment\n",
        "  if predictions == 1:\n",
        "    return \"Architectural Impact: Yes\"\n",
        "  else:\n",
        "    return \"Architectural Impact: No\""
      ],
      "metadata": {
        "id": "R-R5rujwXDfl"
      },
      "execution_count": 380,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dados de testes"
      ],
      "metadata": {
        "id": "LkY8cCXxe6MZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_teste[['issue_key', 'summary', 'architectural_impact']]"
      ],
      "metadata": {
        "id": "5vVK2cLyeYjl",
        "outputId": "ec49cf2e-6c2d-4206-8516-417365d85ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        }
      },
      "execution_count": 388,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          issue_key                                            summary  \\\n",
              "0   CASSANDRA-11944  sstablesInBounds might not actually give all s...   \n",
              "1   CASSANDRA-12988  make the consistency level for user-level auth...   \n",
              "2   CASSANDRA-15004  Anti-compaction briefly corrupts sstable state...   \n",
              "3   CASSANDRA-15265  Index summary redistribution can start even wh...   \n",
              "4   CASSANDRA-18029                     fix starting Paxos auto repair   \n",
              "5   CASSANDRA-18058                     In-memory index and query path   \n",
              "6   CASSANDRA-18617  Disable the deprecated keyspace/table threshol...   \n",
              "7    CASSANDRA-1919                Add shutdownhook to flush commitlog   \n",
              "8     CASSANDRA-414                                 remove sstableLock   \n",
              "9    CASSANDRA-5426                           Redesign repair messages   \n",
              "10  CASSANDRA-11540           The JVM should exit if jmx fails to bind   \n",
              "11   CASSANDRA-6013   CAS may return false but still commit the insert   \n",
              "12   CASSANDRA-8116    HSHA fails with default rpc_max_threads setting   \n",
              "13  CASSANDRA-10164            Re-apply MV updates on commitlog replay   \n",
              "14  CASSANDRA-11971             More uses of DataOutputBuffer.RECYCLER   \n",
              "15  CASSANDRA-12717         IllegalArgumentException in CompactionTask   \n",
              "16  CASSANDRA-13526  nodetool cleanup on KS with no replicas should...   \n",
              "17   CASSANDRA-2941  Expose number of rpc timeouts for individual h...   \n",
              "18   CASSANDRA-3032                    clean up KSMetadata, CFMetadata   \n",
              "19    CASSANDRA-359      CFS readStats_ and diskReadStats_ are missing   \n",
              "20   CASSANDRA-6170  Modify AbstractCassandraDaemon.initLog4j() to ...   \n",
              "21   CASSANDRA-6706  Duplicate rows returned when in clause has rep...   \n",
              "22   CASSANDRA-6962           examine shortening path length post-5202   \n",
              "23   CASSANDRA-6972  Throw an ERROR when auto_bootstrap: true and b...   \n",
              "24    CASSANDRA-758                      support wrapped range queries   \n",
              "25   CASSANDRA-8627  Support Total/Recent latency histogram metrics...   \n",
              "\n",
              "   architectural_impact  \n",
              "0                   YES  \n",
              "1                   YES  \n",
              "2                   YES  \n",
              "3                   YES  \n",
              "4                   YES  \n",
              "5                   YES  \n",
              "6                   YES  \n",
              "7                   YES  \n",
              "8                   YES  \n",
              "9                   YES  \n",
              "10                   NO  \n",
              "11                   NO  \n",
              "12                   NO  \n",
              "13                   NO  \n",
              "14                   NO  \n",
              "15                   NO  \n",
              "16                   NO  \n",
              "17                   NO  \n",
              "18                   NO  \n",
              "19                   NO  \n",
              "20                   NO  \n",
              "21                   NO  \n",
              "22                   NO  \n",
              "23                   NO  \n",
              "24                   NO  \n",
              "25                   NO  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66f81677-bf8b-41bc-a396-6f83937d324f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_key</th>\n",
              "      <th>summary</th>\n",
              "      <th>architectural_impact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CASSANDRA-11944</td>\n",
              "      <td>sstablesInBounds might not actually give all s...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CASSANDRA-12988</td>\n",
              "      <td>make the consistency level for user-level auth...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CASSANDRA-15004</td>\n",
              "      <td>Anti-compaction briefly corrupts sstable state...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CASSANDRA-15265</td>\n",
              "      <td>Index summary redistribution can start even wh...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CASSANDRA-18029</td>\n",
              "      <td>fix starting Paxos auto repair</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>CASSANDRA-18058</td>\n",
              "      <td>In-memory index and query path</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>CASSANDRA-18617</td>\n",
              "      <td>Disable the deprecated keyspace/table threshol...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>CASSANDRA-1919</td>\n",
              "      <td>Add shutdownhook to flush commitlog</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>CASSANDRA-414</td>\n",
              "      <td>remove sstableLock</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>CASSANDRA-5426</td>\n",
              "      <td>Redesign repair messages</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>CASSANDRA-11540</td>\n",
              "      <td>The JVM should exit if jmx fails to bind</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>CASSANDRA-6013</td>\n",
              "      <td>CAS may return false but still commit the insert</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>CASSANDRA-8116</td>\n",
              "      <td>HSHA fails with default rpc_max_threads setting</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>CASSANDRA-10164</td>\n",
              "      <td>Re-apply MV updates on commitlog replay</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>CASSANDRA-11971</td>\n",
              "      <td>More uses of DataOutputBuffer.RECYCLER</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>CASSANDRA-12717</td>\n",
              "      <td>IllegalArgumentException in CompactionTask</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>CASSANDRA-13526</td>\n",
              "      <td>nodetool cleanup on KS with no replicas should...</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>CASSANDRA-2941</td>\n",
              "      <td>Expose number of rpc timeouts for individual h...</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>CASSANDRA-3032</td>\n",
              "      <td>clean up KSMetadata, CFMetadata</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>CASSANDRA-359</td>\n",
              "      <td>CFS readStats_ and diskReadStats_ are missing</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>CASSANDRA-6170</td>\n",
              "      <td>Modify AbstractCassandraDaemon.initLog4j() to ...</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>CASSANDRA-6706</td>\n",
              "      <td>Duplicate rows returned when in clause has rep...</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>CASSANDRA-6962</td>\n",
              "      <td>examine shortening path length post-5202</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>CASSANDRA-6972</td>\n",
              "      <td>Throw an ERROR when auto_bootstrap: true and b...</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>CASSANDRA-758</td>\n",
              "      <td>support wrapped range queries</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>CASSANDRA-8627</td>\n",
              "      <td>Support Total/Recent latency histogram metrics...</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66f81677-bf8b-41bc-a396-6f83937d324f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-66f81677-bf8b-41bc-a396-6f83937d324f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-66f81677-bf8b-41bc-a396-6f83937d324f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-93e07c88-403f-41d0-a352-49b3383dae5c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-93e07c88-403f-41d0-a352-49b3383dae5c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-93e07c88-403f-41d0-a352-49b3383dae5c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_teste[['issue_key', 'summary', 'architectural_impact']]\",\n  \"rows\": 26,\n  \"fields\": [\n    {\n      \"column\": \"issue_key\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"CASSANDRA-414\",\n          \"CASSANDRA-13526\",\n          \"CASSANDRA-11944\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"remove sstableLock\",\n          \"nodetool cleanup on KS with no replicas should remove old data, not silently complete\",\n          \"sstablesInBounds might not actually give all sstables within the bounds due to having start positions moved in sstables\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"architectural_impact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"NO\",\n          \"YES\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 388
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Roda o modelo com os dados de testes"
      ],
      "metadata": {
        "id": "4Lt1Q69ye0w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df_teste.iterrows():\n",
        "  field = row['summary']\n",
        "  issue_key = row['issue_key']\n",
        "  summary = truncate_string(text=row['summary'], max_length=50)\n",
        "  print(f\"{issue_key}, {summary}, {analyse_issue(issue_field=field)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX-Jk_kChzbq",
        "outputId": "bee5b6b6-65c4-4b51-ff9c-36dcf036975e"
      },
      "execution_count": 381,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CASSANDRA-11944, sstablesInBounds might not actually give all sstab..., Architectural Impact: No\n",
            "CASSANDRA-12988, make the consistency level for user-level auth rea..., Architectural Impact: Yes\n",
            "CASSANDRA-15004, Anti-compaction briefly corrupts sstable state for..., Architectural Impact: No\n",
            "CASSANDRA-15265, Index summary redistribution can start even when c..., Architectural Impact: Yes\n",
            "CASSANDRA-18029, fix starting Paxos auto repair, Architectural Impact: No\n",
            "CASSANDRA-18058, In-memory index and query path, Architectural Impact: Yes\n",
            "CASSANDRA-18617, Disable the deprecated keyspace/table thresholds a..., Architectural Impact: Yes\n",
            "CASSANDRA-1919, Add shutdownhook to flush commitlog, Architectural Impact: No\n",
            "CASSANDRA-414, remove sstableLock, Architectural Impact: Yes\n",
            "CASSANDRA-5426, Redesign repair messages, Architectural Impact: Yes\n",
            "CASSANDRA-11540, The JVM should exit if jmx fails to bind, Architectural Impact: No\n",
            "CASSANDRA-6013, CAS may return false but still commit the insert, Architectural Impact: No\n",
            "CASSANDRA-8116, HSHA fails with default rpc_max_threads setting, Architectural Impact: No\n",
            "CASSANDRA-10164, Re-apply MV updates on commitlog replay, Architectural Impact: No\n",
            "CASSANDRA-11971, More uses of DataOutputBuffer.RECYCLER, Architectural Impact: No\n",
            "CASSANDRA-12717, IllegalArgumentException in CompactionTask, Architectural Impact: No\n",
            "CASSANDRA-13526, nodetool cleanup on KS with no replicas should rem..., Architectural Impact: No\n",
            "CASSANDRA-2941, Expose number of rpc timeouts for individual hosts..., Architectural Impact: No\n",
            "CASSANDRA-3032, clean up KSMetadata, CFMetadata, Architectural Impact: No\n",
            "CASSANDRA-359, CFS readStats_ and diskReadStats_ are missing, Architectural Impact: No\n",
            "CASSANDRA-6170, Modify AbstractCassandraDaemon.initLog4j() to allo..., Architectural Impact: No\n",
            "CASSANDRA-6706, Duplicate rows returned when in clause has repeate..., Architectural Impact: No\n",
            "CASSANDRA-6962, examine shortening path length post-5202, Architectural Impact: No\n",
            "CASSANDRA-6972, Throw an ERROR when auto_bootstrap: true and boots..., Architectural Impact: No\n",
            "CASSANDRA-758, support wrapped range queries, Architectural Impact: Yes\n",
            "CASSANDRA-8627, Support Total/Recent latency histogram metrics for..., Architectural Impact: No\n"
          ]
        }
      ]
    }
  ]
}